import logging
import asyncio
from core.platform.models import UnifiedContext

from core.config import MCP_MEMORY_ENABLED
from core.tool_registry import tool_registry
from services.ai_service import AiService
from core.prompts import DEFAULT_SYSTEM_PROMPT, MEMORY_MANAGEMENT_GUIDE

logger = logging.getLogger(__name__)


class AgentOrchestrator:
    """
    The Agent Brain.
    Orchestrates the interaction between:
    1. Tool Registry (Capabilities)
    2. User Context (Telegram Update)
    3. AI Service (Gemini Agent Engine)
    """

    def __init__(self):
        self.ai_service = AiService()
        self._memory_tools_cache = None  # Cache for tool definitions

    async def handle_message(self, ctx: UnifiedContext, message_history: list):
        """
        Main entry point for handling user messages via the Agent.
        Returns a generator of text chunks (streaming response).
        """
        user_id = ctx.message.user.id  # Assuming ID is int compatible for now

        # 0. Dynamic Skill Search (Context Loading)
        # Instead of giving the AI all skills or a generic search tool, we pre-search based on user input.
        # This acts as a "RAG" for tools/skills.
        from core.skill_loader import skill_loader

        # Extract user text from history (last user message)
        last_user_text = ""
        for msg in reversed(message_history):
            # Compatible handle for dict (legacy) or Content object (google.genai.types)
            if isinstance(msg, dict):
                role = msg.get("role")
                parts = msg.get("parts", [])
            else:
                role = getattr(msg, "role", None)
                parts = getattr(msg, "parts", [])

            if role == "user":
                for p in parts:
                    if isinstance(p, dict) and "text" in p:
                        last_user_text = p["text"]
                    elif hasattr(p, "text"):
                        last_user_text = p.text
                break

        # 1. Gather Tools
        # Start with base tools (e.g. skill_manager for explicitly managing skills)
        # Note: We might want a simplified skill_manager tool if we are auto-injecting.
        tools = []

        # Always include skill_manager for explicit "install", "search" etc commands unless handled purely via NLI
        # For now, let's keep the generic capability logic but prioritizing matched skills.

        matched_skills = []
        if last_user_text:
            # Use a lower threshold to catch more potential candidates
            # matched_skills = await skill_loader.find_similar_skills(
            #     last_user_text, threshold=0.4
            # )
            pass

        if matched_skills:
            logger.info(
                f"Dynamic Skill Injection: Found {len(matched_skills)} matches for '{last_user_text[:20]}...'"
            )
            # Create specific tools for these skills
            # We need to ask ToolRegistry to generate tools for these specific skills
            specific_tools = tool_registry.get_specific_skill_tools(matched_skills)
            tools.extend(specific_tools)

        # Add the generic 'call_skill' tool as a fallback (but maybe with reduced description to save tokens?)
        # Or if we trust the search, we might not need it?
        # Safety: Keep generic tool but maybe prompt emphasizes using specific ones?
        # Actually, get_all_tools() returns the generic one.
        # Let's add the generic one LAST as fallback.
        tools.extend(tool_registry.get_all_tools())

        # 2. Add Memory Tools
        if MCP_MEMORY_ENABLED:
            memory_tools = await self._get_memory_tool_definitions(user_id)
            if memory_tools:
                tools.extend(memory_tools)

        # 3. Define Tool Executor (Closure with Context)
        async def tool_executor(name: str, args: dict) -> str:
            logger.info(f"Agent invoking tool: {name} with {args}")
            try:
                # Dispatch to specific handlers
                if name == "call_skill" or name.startswith("skill_"):
                    from agents.skill_agent import (
                        skill_agent,
                        SkillDelegationRequest,
                        SkillFinalReply,
                        SkillDecision,
                    )

                    if name == "call_skill":
                        skill_name = args["skill_name"]
                        instruction = args["instruction"]
                    else:
                        # Dynamic tool: skill_rss_subscribe -> rss_subscribe
                        # Remove prefix "skill_"
                        # skill_manager -> skill_manager
                        safe_name = name[6:] if name != "skill_manager" else name

                        from core.skill_loader import skill_loader

                        # 1. Try exact match (e.g. rss_subscribe)
                        skill_name = safe_name

                        if not skill_loader.get_skill(skill_name):
                            # 2. Try hyphenated version (e.g. data_storytelling -> data-storytelling)
                            alt_name = skill_name.replace("_", "-")
                            if skill_loader.get_skill(alt_name):
                                skill_name = alt_name

                        instruction = args["instruction"]

                    # Notify user about skill invocation (ephemeral, not saved)

                    instruction_preview = (
                        instruction[:200] + "..."
                        if len(instruction) > 200
                        else instruction
                    )
                    await ctx.reply(
                        f"âš¡ å‡†å¤‡è°ƒç”¨ `{skill_name}` èƒ½åŠ›ï¼ŒæŒ‡ä»¤ï¼š{instruction_preview}"
                    )

                    full_output = ""
                    extra_context = ""

                    # Continuous Observation Loop (ReAct Pattern)
                    # åªæœ‰ REPLY æ‰é€€å‡ºï¼ŒEXECUTE å’Œ DELEGATE éƒ½ç»§ç»­å¾ªçŽ¯
                    MAX_DEPTH = 20
                    MAX_ROUND_OUTPUT_LEN = 2000  # æ¯è½®ç»“æžœæœ€å¤§é•¿åº¦
                    MAX_CONTEXT_LEN = 8000  # æ€» context æœ€å¤§é•¿åº¦

                    # å¾ªçŽ¯æ£€æµ‹å˜é‡
                    last_iteration_output = None
                    last_decision = None
                    decision_loop_counter = 0
                    loop_counter = 0

                    for depth in range(MAX_DEPTH):
                        delegation = None
                        execution_result = None
                        is_final_reply = False
                        iteration_output = ""
                        current_decision = None

                        logger.info(
                            "=============================1================================="
                        )

                        # Check for cancellation
                        from core.task_manager import task_manager

                        if task_manager.is_cancelled(user_id):
                            logger.info(
                                f"Task cancelled by user {user_id} during tool execution loop"
                            )
                            raise asyncio.CancelledError()

                        # åŒ…è£¹å¼‚å¸¸æ•èŽ·ï¼Œç¡®ä¿é”™è¯¯ä¿¡æ¯èƒ½ä¼ é€’ç»™ä¸‹ä¸€è½®
                        try:
                            # Execute Skill Agent (Think -> Act)
                            async for (
                                chunk,
                                files,
                                result_obj,
                            ) in skill_agent.execute_skill(
                                skill_name,
                                instruction,
                                extra_context=extra_context,
                                ctx=ctx,
                            ):
                                logger.info(
                                    "=============================2================================="
                                )
                                # Check for cancellation during streaming
                                if task_manager.is_cancelled(user_id):
                                    raise asyncio.CancelledError()

                                # æ£€æµ‹è¿”å›žç±»åž‹
                                if isinstance(result_obj, SkillDelegationRequest):
                                    delegation = result_obj
                                elif isinstance(result_obj, SkillDecision):
                                    current_decision = result_obj
                                elif isinstance(result_obj, SkillFinalReply):
                                    # Agent æ˜Žç¡®è¿”å›žäº†æœ€ç»ˆå›žå¤
                                    is_final_reply = True
                                elif isinstance(result_obj, dict):
                                    if "ui" in result_obj:
                                        if "pending_ui" not in ctx.user_data:
                                            ctx.user_data["pending_ui"] = []
                                        ctx.user_data["pending_ui"].append(
                                            result_obj["ui"]
                                        )
                                    # æ•èŽ·æ‰§è¡Œç»“æžœï¼ˆç”¨äºŽåé¦ˆç»™ä¸‹ä¸€è½®ï¼‰
                                    execution_result = result_obj

                                    if chunk:
                                        # åªåœ¨ result_obj ä¸ºç©ºï¼ˆæ™®é€šæ–‡æœ¬æµï¼‰æ—¶å‘é€æ¶ˆæ¯
                                        # å¦‚æžœæ˜¯ dict (structured result)ï¼Œä¼šåœ¨åŽç»­ execution_result é€»è¾‘ä¸­ç»Ÿä¸€å‘é€ï¼Œé¿å…é‡å¤
                                        # é¿å…å‘é€ Agent çš„ä¸­é—´æ€è€ƒæ¶ˆæ¯ï¼ˆå¦‚ "æ­£åœ¨æ€è€ƒ..."ï¼‰
                                        if (
                                            not isinstance(result_obj, dict)
                                            and not chunk.startswith("ðŸ§ ")
                                            and not chunk.startswith("ðŸ”‡ðŸ”‡ðŸ”‡")
                                            and not is_final_reply
                                        ):
                                            await ctx.reply(chunk)
                                            logger.info(f"[Round {depth + 1}] {chunk}")

                                    iteration_output += chunk + "\n"

                                if files:
                                    for filename, content in files.items():
                                        await ctx.reply_document(
                                            document=content, filename=filename
                                        )
                        except Exception as e:
                            # æ•èŽ·æŠ€èƒ½æ‰§è¡Œè¿‡ç¨‹ä¸­çš„å¼‚å¸¸
                            error_msg = f"âŒ æ‰§è¡Œå‡ºé”™: {str(e)}"
                            logger.error(
                                f"[Round {depth + 1}] Skill execution error: {e}",
                                exc_info=True,
                            )

                            # å°†é”™è¯¯ä¿¡æ¯å‘é€ç»™ç”¨æˆ·
                            await ctx.reply(error_msg)

                            # å°†é”™è¯¯ä¿¡æ¯åŠ å…¥ iteration_output å’Œ execution_result
                            iteration_output += error_msg + "\n"
                            execution_result = {"text": error_msg, "error": str(e)}

                        logger.info(
                            "=============================3================================="
                        )
                        full_output += iteration_output

                        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ç»ˆå›žå¤ï¼ˆAgent è¿”å›ž REPLY actionï¼‰
                        # å¦‚æžœ iteration_output ä¸åŒ…å«ç‰¹å®šçš„ä¸­é—´çŠ¶æ€æ ‡è®°ï¼Œä¸”æ²¡æœ‰ delegationï¼Œ
                        # æˆ‘ä»¬éœ€è¦æ›´ç²¾ç¡®åœ°åˆ¤æ–­æ˜¯å¦æ˜¯ REPLY
                        # å®žé™…ä¸Šï¼ŒSkillAgent åœ¨ REPLY æ—¶ä¼šç›´æŽ¥ yield content å¹¶ return
                        # è€Œ EXECUTE æ—¶ä¼š yield æ‰§è¡Œç»“æžœ

                        if delegation:
                            # === DELEGATE: æ‰§è¡Œå§”æ‰˜å¹¶ç»§ç»­å¾ªçŽ¯ ===
                            logger.info(
                                f"[Round {depth + 1}] Delegating to {delegation.target_skill}"
                            )
                            await ctx.reply(
                                f"ðŸ”„ æ­£åœ¨å§”æ‰˜ç»™ `{delegation.target_skill}`: {delegation.instruction}"
                            )

                            # Execute Delegated Skill
                            delegated_output = ""
                            try:
                                async for (
                                    d_chunk,
                                    d_files,
                                    d_result,
                                ) in skill_agent.execute_skill(
                                    delegation.target_skill,
                                    delegation.instruction,
                                    ctx=ctx,
                                ):
                                    if d_chunk:
                                        delegated_output += d_chunk + "\n"
                                    if d_files:
                                        for f_name, f_content in d_files.items():
                                            await ctx.reply_document(
                                                document=f_content, filename=f_name
                                            )
                            except Exception as e:
                                # æ•èŽ·å§”æ‰˜æ‰§è¡Œè¿‡ç¨‹ä¸­çš„å¼‚å¸¸
                                error_msg = f"âŒ å§”æ‰˜æ‰§è¡Œå‡ºé”™: {str(e)}"
                                logger.error(
                                    f"[Round {depth + 1}] Delegation error: {e}",
                                    exc_info=True,
                                )
                                await ctx.reply(error_msg)
                                delegated_output = error_msg + "\n"

                            # æ™ºèƒ½æˆªæ–­
                            if len(delegated_output) > MAX_ROUND_OUTPUT_LEN:
                                truncated = delegated_output[:MAX_ROUND_OUTPUT_LEN]
                                truncated += f"\n...[å·²æˆªæ–­ï¼ŒåŽŸé•¿åº¦ {len(delegated_output)} å­—ç¬¦]"
                            else:
                                truncated = delegated_output

                            extra_context += f"\n\nã€è½®æ¬¡ {depth + 1} ç»“æžœ - {delegation.target_skill}ã€‘:\n{truncated}"

                        elif execution_result or iteration_output:
                            # === EXECUTE: æŠŠæ‰§è¡Œç»“æžœåŠ å…¥ context å¹¶ç»§ç»­å¾ªçŽ¯ ===
                            logger.info(
                                "=============================4================================="
                            )
                            # å¦‚æžœæœ‰å…·ä½“çš„æ‰§è¡Œç»“æžœï¼ˆå¦‚ write_file è¿”å›žçš„ successï¼‰ï¼ŒåŠ å…¥ä¸Šä¸‹æ–‡
                            if execution_result:
                                result_text = str(execution_result)
                                logger.info(
                                    "=============================5================================="
                                )
                                if isinstance(execution_result, dict):
                                    result_text = execution_result.get(
                                        "text", str(execution_result)
                                    )

                                    # [æ–°å¢ž] å°†æ‰§è¡Œç»“æžœå‘é€ç»™ç”¨æˆ·ï¼ˆå¢žå¼ºå¯è§æ€§ï¼‰
                                    # é¿å…å‘é€çº¯æ•°æ®å¯¹è±¡çš„å­—ç¬¦ä¸²è¡¨ç¤ºï¼Œåªå‘é€æœ‰æ„ä¹‰çš„æ–‡æœ¬
                                    if "text" in execution_result and result_text:
                                        if not result_text.startswith("ðŸ”‡ðŸ”‡ðŸ”‡"):
                                            await ctx.reply(result_text)

                                if len(result_text) > MAX_ROUND_OUTPUT_LEN:
                                    result_text = (
                                        result_text[:MAX_ROUND_OUTPUT_LEN]
                                        + "...[å·²æˆªæ–­]"
                                    )

                                command_info = ""
                                if current_decision:
                                    cmd_content = str(current_decision.content)
                                    # Truncate large params in context to save tokens, but keep enough
                                    if len(cmd_content) > 500:
                                        cmd_content = (
                                            cmd_content[:500] + "...[truncated]"
                                        )

                                    command_info = f"ã€è½®æ¬¡ {depth + 1} æ“ä½œã€‘: {current_decision.action}"
                                    if current_decision.execute_type:
                                        command_info += (
                                            f" ({current_decision.execute_type})"
                                        )
                                    command_info += f"\nå‚æ•°: {cmd_content}\n"

                                extra_context += f"\n\n{command_info}ã€è½®æ¬¡ {depth + 1} æ‰§è¡Œç»“æžœã€‘:\n{result_text}"
                                logger.info(
                                    f"[Round {depth + 1}] EXECUTE result captured, continuing..."
                                )
                                logger.info(f"Extra context: {extra_context}")
                                continue

                            # å¦‚æžœåªæœ‰æ–‡æœ¬è¾“å‡ºä¸”ä¸æ˜¯æœ€ç»ˆå›žå¤ï¼ˆä¾‹å¦‚ Agent çš„æ€è€ƒè¿‡ç¨‹ï¼‰
                            elif not is_final_reply and iteration_output.strip():
                                # å¿½ç•¥çº¯çŠ¶æ€æ¶ˆæ¯
                                is_status_msg = any(
                                    marker in iteration_output
                                    for marker in [
                                        "æ­£åœ¨æ‰§è¡Œ",
                                        "æ­£åœ¨æ€è€ƒ",
                                        "âš™ï¸",
                                        "ðŸ§ ",
                                        "ðŸ‘‰ å§”æ‰˜ç»™",
                                    ]
                                )
                                if not is_status_msg:
                                    extra_context += f"\n\nã€è½®æ¬¡ {depth + 1} è¾“å‡ºã€‘:\n{iteration_output[:MAX_ROUND_OUTPUT_LEN]}"
                                    # æ³¨æ„ï¼šè¿™é‡Œä¸continueï¼Œä»¥ä¾¿è¿›è¡ŒåŽç»­çš„æ­»å¾ªçŽ¯æ£€æµ‹

                        # === æ­»å¾ªçŽ¯æ£€æµ‹ (Loop Circuit Breaker) ===

                        # 1. Decision-based Check (Semantic Loop)
                        if (
                            last_decision
                            and current_decision
                            and current_decision == last_decision
                        ):
                            decision_loop_counter += 1
                            logger.warning(
                                f"[Loop Detector] Detailed Decision repeated: {decision_loop_counter} times"
                            )
                            if decision_loop_counter >= 2:
                                failure_msg = f"\n\nâš ï¸ **ç³»ç»Ÿä¿æŠ¤**: æ£€æµ‹åˆ° Agent åœ¨è¿žç»­å°è¯•ç›¸åŒçš„æ“ä½œ ({decision_loop_counter + 1} æ¬¡)ï¼Œä»»åŠ¡å·²å¼ºåˆ¶ç»ˆæ­¢ã€‚"
                                await ctx.reply(failure_msg)
                                full_output += failure_msg
                                is_final_reply = True
                        else:
                            decision_loop_counter = 0

                        last_decision = current_decision

                        # 2. Text-based Check (Output Loop)
                        # æ£€æŸ¥å½“å‰è½®æ¬¡çš„è¾“å‡ºæ˜¯å¦ä¸Žä¸Šä¸€è½®å®Œå…¨ä¸€è‡´
                        current_output_signature = iteration_output.strip()

                        if (
                            last_iteration_output
                            and current_output_signature == last_iteration_output
                        ):
                            loop_counter += 1
                            logger.warning(
                                f"[Loop Detector] Detected identical output for {loop_counter} rounds."
                            )

                            # æ”¾å®½é˜ˆå€¼ï¼šå…è®¸è¿žç»­ 2 æ¬¡é‡å¤ï¼ˆå³å…è®¸é‡è¯• 1 æ¬¡ï¼‰
                            # åªæœ‰å½“è¿žç»­ç¬¬ 3 æ¬¡å‡ºçŽ°ç›¸åŒè¾“å‡ºæ—¶ï¼ˆloop_counter=2ï¼‰ï¼Œæ‰è§¦å‘ç†”æ–­
                            if loop_counter >= 2:
                                failure_msg = f"\n\nâš ï¸ **ç³»ç»Ÿä¿æŠ¤**: æ£€æµ‹åˆ° Agent åœ¨è¿žç»­é‡è¯•ç›¸åŒçš„æ“ä½œ ({loop_counter + 1} æ¬¡)ï¼Œä»»åŠ¡å·²å¼ºåˆ¶ç»ˆæ­¢ã€‚"
                                await ctx.reply(failure_msg)
                                full_output += failure_msg
                                is_final_reply = True  # è§¦å‘å¾ªçŽ¯é€€å‡º
                        else:
                            loop_counter = 0

                        last_iteration_output = current_output_signature

                        if is_final_reply:
                            logger.info(
                                f"[Round {depth + 1}] Final REPLY detected, breaking loop"
                            )
                            break

                        # ä¸Šä¸‹æ–‡é•¿åº¦ç®¡ç†
                        if len(extra_context) > MAX_CONTEXT_LEN:
                            keep_len = 6000
                            summary = f"ã€æ—©æœŸè½®æ¬¡æ‘˜è¦ã€‘: ä¹‹å‰å·²å®Œæˆ {depth} è½®æ“ä½œã€‚\n"
                            extra_context = summary + extra_context[-keep_len:]

                        logger.info(
                            f"[Round {depth + 1}] extra_context: {extra_context}"
                        )
                        logger.info(
                            f"[Round {depth + 1}] extra_context é•¿åº¦: {len(extra_context)}"
                        )

                    if not full_output.strip():
                        logger.warning(f"Skill {skill_name} returned empty output!")
                        return None

                    logger.info(
                        f"Skill {skill_name} completed after {depth + 1} rounds, output length: {len(full_output)}"
                    )
                    return f"Skill Execution Output:\n{full_output}"

                # Memory Tools (Lazy Connect)
                else:
                    # Try to see if it's a memory tool
                    if self._is_memory_tool(name):
                        logger.info(
                            f"Connecting to Memory Server for tool execution: {name}"
                        )
                        memory_server = await self._get_active_memory_server(user_id)
                        if memory_server:
                            return await memory_server.call_tool(name, args)

                    return f"Error: Unknown tool '{name}'"

            except Exception as e:
                logger.error(f"Error in tool_executor: {e}", exc_info=True)
                return f"System Error: {str(e)}"

        # 4. Generate Response
        import datetime

        current_time_str = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S %A")

        # Inject Skill Awareness - User Feedback Optimization
        # Only inject skill_manager details to save context and encourage dynamic lookup
        skill_mgr = skill_loader.get_skill("skill_manager")
        skill_instruction = ""

        if skill_mgr:
            skill_instruction = (
                f"\n\nã€ç³»ç»Ÿæ ¸å¿ƒèƒ½åŠ›ã€‘\n"
                f"ä½ ä¸ä»…ä»…æ˜¯ä¸€ä¸ªèŠå¤©æœºå™¨äººï¼Œä½ æ‹¥æœ‰å®Œæ•´çš„æŠ€èƒ½ç®¡ç†ç³»ç»Ÿã€‚\n"
                f"skill_managerï¼š{skill_mgr['description']}\n"
            )
        else:
            logger.warning("Skill Manager not found during prompt generation!")

        system_instruction = DEFAULT_SYSTEM_PROMPT
        system_instruction += skill_instruction
        system_instruction += "åœ¨ä½ ä½¿ç”¨call_skillæ—¶ï¼Œä½ ä¸éœ€è¦äº†è§£skillçš„è¯¦ç»†ä¿¡æ¯ï¼Œç›´æŽ¥ä½¿ç”¨è‡ªç„¶è¯­è¨€å‘é€æŒ‡ä»¤å³å¯ï¼ŒSkillAgentä¼šå¤„ç†åŽç»­çš„è°ƒç”¨ã€‚"
        # system_instruction += "\nâš ï¸ **æç¤º**ï¼šç³»ç»Ÿå¯èƒ½å®‰è£…äº†å…¶ä»–æ•°ç™¾ä¸ªæŠ€èƒ½ã€‚å¦‚æžœä½ éœ€è¦ç‰¹å®šçš„èƒ½åŠ›ï¼ˆå¦‚ç»˜åˆ¶å›¾è¡¨ã€Dockerç®¡ç†ç­‰ï¼‰ï¼Œè¯·åŠ¡å¿…å…ˆè°ƒç”¨ `skill_manager`æ¥æŸ¥æ‰¾ï¼Œè€Œä¸æ˜¯å‡è®¾è‡ªå·±ä¸èƒ½åšã€‚"

        if MCP_MEMORY_ENABLED:
            # Use memory guide if enabled, but we avoid eager connection
            system_instruction += "\n\n" + MEMORY_MANAGEMENT_GUIDE

        # Append dynamic time context
        system_instruction += f"\n\nã€å½“å‰ç³»ç»Ÿæ—¶é—´ã€‘: {current_time_str}"

        async for chunk in self.ai_service.generate_response_stream(
            message_history,
            tools=tools,
            tool_executor=tool_executor,
            system_instruction=system_instruction,
        ):
            yield chunk

    async def _get_memory_tool_definitions(self, user_id: int):
        """
        Get memory tool definitions (schemas).
        Uses caching to avoid connecting on every request.
        """
        if self._memory_tools_cache:
            return self._memory_tools_cache

        try:
            # First time: Need to connect and fetch
            logger.info("Fetching Memory Tool Definitions (One-time init)...")
            from mcp_client import mcp_manager
            from mcp_client.tools_bridge import convert_mcp_tools_to_gemini
            from mcp_client.memory import register_memory_server

            register_memory_server()
            # We start server just to get tools, then we can let it be (manager handles process)
            memory_server = await mcp_manager.get_server("memory", user_id=user_id)

            if memory_server and memory_server.session:
                mcp_tools_result = await memory_server.session.list_tools()
                gemini_funcs = convert_mcp_tools_to_gemini(mcp_tools_result.tools)

                self._memory_tools_cache = gemini_funcs
                return gemini_funcs
        except Exception as e:
            logger.error(f"Failed to fetch memory tools: {e}")
            pass
        return None

    async def _get_active_memory_server(self, user_id: int):
        """
        Get an active connection to the memory server for EXECUTION.
        """
        try:
            from mcp_client import mcp_manager
            from mcp_client.memory import register_memory_server

            register_memory_server()
            return await mcp_manager.get_server("memory", user_id=user_id)
        except Exception:
            return None

    def _is_memory_tool(self, name: str) -> bool:
        """Check if tool name belongs to memory tools"""
        if not self._memory_tools_cache:
            return False
        # Check against cached definitions
        for tool in self._memory_tools_cache:
            if tool.get("name") == name:
                return True
        return False


agent_orchestrator = AgentOrchestrator()
