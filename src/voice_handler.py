"""
è¯­éŸ³æ¶ˆæ¯å¤„ç†æ¨¡å— - æ™ºèƒ½è·¯ç”±ç‰ˆ

çŸ­è¯­éŸ³ï¼ˆâ‰¤60sï¼‰: è½¬æ–‡å­—åèµ°æ™ºèƒ½è·¯ç”±ï¼ˆä¸æ–‡æœ¬æ¶ˆæ¯ä¸€è‡´ï¼‰
é•¿è¯­éŸ³ï¼ˆ>60sï¼‰: ç›´æ¥è½¬å†™è¾“å‡º
"""
import logging
import base64
from telegram import Update
from telegram.ext import ContextTypes
from telegram.error import BadRequest

from config import gemini_client, GEMINI_MODEL, is_user_allowed
from user_context import add_message, get_user_context
from utils import smart_edit_text, smart_reply_text

logger = logging.getLogger(__name__)

# è¯­éŸ³æ—¶é•¿é˜ˆå€¼ï¼ˆç§’ï¼‰
SHORT_VOICE_THRESHOLD = 60


async def transcribe_voice(voice_bytes: bytes, mime_type: str) -> str | None:
    """
    ä½¿ç”¨ Gemini è½¬å†™è¯­éŸ³ä¸ºæ–‡å­—
    
    Returns:
        è½¬å†™åçš„æ–‡æœ¬ï¼Œå¤±è´¥è¿”å› None
    """
    try:
        contents = [
            {
                "parts": [
                    {"text": "è¯·å°†è¿™æ®µè¯­éŸ³è½¬å†™ä¸ºæ–‡å­—ã€‚åªè¾“å‡ºè¯­éŸ³ä¸­è¯´çš„åŸè¯ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–å›å¤ã€‚å¦‚æœæ— æ³•è¯†åˆ«ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²ã€‚"},
                    {
                        "inline_data": {
                            "mime_type": mime_type,
                            "data": base64.b64encode(bytes(voice_bytes)).decode("utf-8"),
                        }
                    },
                ]
            }
        ]
        
        response = gemini_client.models.generate_content(
            model=GEMINI_MODEL,
            contents=contents,
        )
        
        if response.text and len(response.text.strip()) > 0:
            return response.text.strip()
        return None
    except Exception as e:
        logger.error(f"Voice transcription error: {e}")
        return None


async def transcribe_and_translate_voice(voice_bytes: bytes, mime_type: str) -> dict | None:
    """
    è½¬å†™è¯­éŸ³å¹¶ç¿»è¯‘ä¸ºåŒè¯­å¯¹ç…§
    
    Returns:
        {"original": "åŸæ–‡", "original_lang": "è¯­è¨€", "translated": "è¯‘æ–‡"} æˆ– None
    """
    try:
        prompt = (
            "è¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š\n"
            "1. å°†è¯­éŸ³è½¬å†™ä¸ºæ–‡å­—\n"
            "2. è¯†åˆ«è¯­éŸ³çš„è¯­è¨€\n"
            "3. å¦‚æœæ˜¯ä¸­æ–‡ï¼Œç¿»è¯‘ä¸ºè‹±æ–‡ï¼›å¦‚æœæ˜¯å…¶ä»–è¯­è¨€ï¼Œç¿»è¯‘ä¸ºä¸­æ–‡\n\n"
            "è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼ˆä¸è¦æ·»åŠ å…¶ä»–å†…å®¹ï¼‰ï¼š\n"
            "åŸæ–‡è¯­è¨€ï¼š[è¯­è¨€åç§°]\n"
            "åŸæ–‡ï¼š[è½¬å†™çš„åŸæ–‡]\n"
            "è¯‘æ–‡ï¼š[ç¿»è¯‘åçš„æ–‡å­—]"
        )
        
        contents = [
            {
                "parts": [
                    {"text": prompt},
                    {
                        "inline_data": {
                            "mime_type": mime_type,
                            "data": base64.b64encode(bytes(voice_bytes)).decode("utf-8"),
                        }
                    },
                ]
            }
        ]
        
        response = gemini_client.models.generate_content(
            model=GEMINI_MODEL,
            contents=contents,
        )
        
        if not response.text:
            return None
        
        # è§£æç»“æœ
        text = response.text.strip()
        result = {}
        
        for line in text.split('\n'):
            if line.startswith('åŸæ–‡è¯­è¨€ï¼š'):
                result['original_lang'] = line.replace('åŸæ–‡è¯­è¨€ï¼š', '').strip()
            elif line.startswith('åŸæ–‡ï¼š'):
                result['original'] = line.replace('åŸæ–‡ï¼š', '').strip()
            elif line.startswith('è¯‘æ–‡ï¼š'):
                result['translated'] = line.replace('è¯‘æ–‡ï¼š', '').strip()
        
        if result.get('original') and result.get('translated'):
            return result
        return None
        
    except Exception as e:
        logger.error(f"Voice translation error: {e}")
        return None


async def handle_voice_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """
    å¤„ç†è¯­éŸ³æ¶ˆæ¯
    
    ç¿»è¯‘æ¨¡å¼å¼€å¯: è½¬å†™ + ç¿»è¯‘ â†’ åŒè¯­å¯¹ç…§è¾“å‡º
    æ­£å¸¸æ¨¡å¼:
        çŸ­è¯­éŸ³: è½¬æ–‡å­— â†’ æ™ºèƒ½è·¯ç”±
        é•¿è¯­éŸ³: ç›´æ¥è½¬å†™è¾“å‡º
    """
    from database import get_user_settings
    
    chat_id = update.message.chat_id
    user_id = update.message.from_user.id
    
    # æ£€æŸ¥ç”¨æˆ·æƒé™
    if not await is_user_allowed(user_id):
        await smart_reply_text(update, "â›” æŠ±æ­‰ï¼Œæ‚¨æ²¡æœ‰ä½¿ç”¨ AI åŠŸèƒ½çš„æƒé™ã€‚")
        return
    
    # è·å–è¯­éŸ³æ¶ˆæ¯
    voice = update.message.voice
    if not voice:
        return
    
    # æ£€æŸ¥æ˜¯å¦å¼€å¯ç¿»è¯‘æ¨¡å¼
    settings = await get_user_settings(user_id)
    translate_mode = settings.get("auto_translate", 0)
    
    # å‘é€å¤„ç†ä¸­æç¤º
    if translate_mode:
        thinking_msg = await smart_reply_text(update, "ğŸŒ æ­£åœ¨ç¿»è¯‘è¯­éŸ³å†…å®¹...")
    else:
        thinking_msg = await smart_reply_text(update, "ğŸ¤ æ­£åœ¨è¯†åˆ«è¯­éŸ³å†…å®¹...")
    
    # å‘é€"æ­£åœ¨è¾“å…¥"çŠ¶æ€
    await context.bot.send_chat_action(chat_id=chat_id, action="typing")
    
    try:
        # ä¸‹è½½è¯­éŸ³æ–‡ä»¶
        file = await context.bot.get_file(voice.file_id)
        voice_bytes = await file.download_as_bytearray()
        mime_type = voice.mime_type or "audio/ogg"
        
        # ç¿»è¯‘æ¨¡å¼ï¼šåŒè¯­å¯¹ç…§è¾“å‡º
        if translate_mode:
            result = await transcribe_and_translate_voice(voice_bytes, mime_type)
            
            if not result:
                await smart_edit_text(thinking_msg, "âŒ æ— æ³•è¯†åˆ«æˆ–ç¿»è¯‘è¯­éŸ³å†…å®¹ï¼Œè¯·é‡è¯•ã€‚")
                return
            
            original_lang = result.get('original_lang', 'æœªçŸ¥')
            original = result.get('original', '')
            translated = result.get('translated', '')
            
            output = (
                f"ğŸ¤ **è¯­éŸ³ç¿»è¯‘**\n\n"
                f"ğŸ“ **åŸæ–‡** ({original_lang}):\n"
                f"ã€Œ{original}ã€\n\n"
                f"ğŸŒ **è¯‘æ–‡**:\n"
                f"ã€Œ{translated}ã€"
            )
            
            await smart_edit_text(thinking_msg, output)
            
            # è®°å½•ç»Ÿè®¡
            from stats import increment_stat
            await increment_stat(user_id, "translations_count")
            return
        
        # æ­£å¸¸æ¨¡å¼ï¼šè½¬å†™è¯­éŸ³
        transcribed_text = await transcribe_voice(voice_bytes, mime_type)
        
        if not transcribed_text:
            await smart_edit_text(thinking_msg, "âŒ æ— æ³•è¯†åˆ«è¯­éŸ³å†…å®¹ï¼Œè¯·é‡è¯•æˆ–å‘é€æ–‡å­—æ¶ˆæ¯ã€‚")
            return
        
        logger.info(f"Voice transcribed: {transcribed_text[:50]}...")
        
        # æ ¹æ®è¯­éŸ³æ—¶é•¿å†³å®šå¤„ç†ç­–ç•¥
        if voice.duration <= SHORT_VOICE_THRESHOLD:
            # çŸ­è¯­éŸ³ï¼šèµ°æ™ºèƒ½è·¯ç”±ï¼ˆä¸æ–‡æœ¬æ¶ˆæ¯ä¸€è‡´ï¼‰
            await smart_edit_text(thinking_msg, f"ğŸ¤ è¯­éŸ³è½¬å†™å†…å®¹ä¸º: **\"{transcribed_text}\"**\n\nğŸ¤” æ­£åœ¨æ€è€ƒä¸­...")
            
            # è°ƒç”¨æ–‡æœ¬æ¶ˆæ¯å¤„ç†é€»è¾‘
            await process_as_text_message(update, context, transcribed_text, thinking_msg)
        else:
            # é•¿è¯­éŸ³ï¼šç›´æ¥è¾“å‡ºè½¬å†™ç»“æœ
            await smart_edit_text(thinking_msg, f"ğŸ¤ **è¯­éŸ³è½¬å†™ç»“æœï¼š**\n\n{transcribed_text}")
            
            # è®°å½•åˆ°ä¸Šä¸‹æ–‡
            add_message(context, "user", f"ã€ç”¨æˆ·å‘é€äº†ä¸€æ®µé•¿è¯­éŸ³ã€‘{transcribed_text}")
            
            # è®°å½•ç»Ÿè®¡
            from stats import increment_stat
            await increment_stat(user_id, "voice_chats")
        
    except Exception as e:
        logger.error(f"Voice processing error: {e}")
        try:
            await smart_edit_text(thinking_msg,
                "âŒ è¯­éŸ³å¤„ç†å¤±è´¥ï¼Œè¯·ç¨åå†è¯•ã€‚\n\n"
                "å¯èƒ½çš„åŸå› ï¼š\n"
                "â€¢ è¯­éŸ³æ ¼å¼ä¸æ”¯æŒ\n"
                "â€¢ è¯­éŸ³å†…å®¹æ— æ³•è¯†åˆ«\n"
                "â€¢ æœåŠ¡æš‚æ—¶ä¸å¯ç”¨"
            )
        except BadRequest:
            pass


async def process_as_text_message(
    update: Update, 
    context: ContextTypes.DEFAULT_TYPE, 
    text: str,
    thinking_msg
) -> None:
    """
    å°†è½¬å†™åçš„æ–‡æœ¬æŒ‰æ™®é€šæ–‡æœ¬æ¶ˆæ¯é€»è¾‘å¤„ç†ï¼ˆæ™ºèƒ½è·¯ç”±ï¼‰
    """
    import time
    from intent_router import analyze_intent, UserIntent
    from handlers.ai_handlers import handle_ai_chat
    from stats import increment_stat
    
    user_id = update.message.from_user.id
    chat_id = update.message.chat_id
    
    # è®°å½•ç”¨æˆ·æ¶ˆæ¯åˆ°ä¸Šä¸‹æ–‡
    add_message(context, "user", text)
    
    # åˆ†ææ„å›¾
    intent_result = await analyze_intent(text)
    intent = intent_result.get("intent")
    params = intent_result.get("params", {})
    
    logger.info(f"Voice Smart Routing: {intent} | params={params}")
    
    # å¤„ç†ç‰¹æ®Šæ„å›¾
    if intent == UserIntent.DOWNLOAD_VIDEO:
        from web_summary import extract_urls
        from handlers.media_handlers import process_video_download
        
        target_url = params.get("url")
        if not target_url:
            found_urls = extract_urls(text)
            if found_urls:
                target_url = found_urls[0]
        
        if target_url:
            await smart_edit_text(thinking_msg, f"ğŸš€ è¯†åˆ«åˆ°ä¸‹è½½æ„å›¾ï¼Œæ­£åœ¨å¤„ç†é“¾æ¥...")
            await process_video_download(update, context, target_url, audio_only=False)
            return
    
    elif intent == UserIntent.GENERATE_IMAGE:
        prompt = params.get("prompt") or text
        await smart_edit_text(thinking_msg, f"ğŸ¨ è¯†åˆ«åˆ°ç”»å›¾æ„å›¾ï¼Œæ­£åœ¨ç”Ÿæˆ...")
        from image_generator import handle_image_generation
        await handle_image_generation(update, context, prompt)
        return
    
    elif intent == UserIntent.SET_REMINDER:
        time_str = params.get("time")
        content = params.get("content")
        if time_str and content:
            from handlers.service_handlers import process_remind
            await smart_edit_text(thinking_msg, f"â° è¯†åˆ«åˆ°æé†’æ„å›¾ï¼Œæ­£åœ¨è®¾ç½®...")
            await process_remind(update, context, time_str, content)
            return
    
    elif intent == UserIntent.RSS_SUBSCRIBE:
        url = params.get("url")
        if url:
            from handlers.service_handlers import process_subscribe
            await smart_edit_text(thinking_msg, f"ğŸ“¢ è¯†åˆ«åˆ°è®¢é˜…æ„å›¾ï¼Œæ­£åœ¨å¤„ç†...")
            await process_subscribe(update, context, url)
            return
    
    elif intent == UserIntent.MONITOR_KEYWORD:
        keyword = params.get("keyword")
        if keyword:
            from handlers.service_handlers import process_monitor
            await smart_edit_text(thinking_msg, f"ğŸ” è¯†åˆ«åˆ°ç›‘æ§æ„å›¾ï¼Œæ­£åœ¨å¤„ç†...")
            await process_monitor(update, context, keyword)
            return
    
    # æ™®é€šå¯¹è¯ï¼šèµ° AI ç”Ÿæˆæµç¨‹
    await context.bot.send_chat_action(chat_id=chat_id, action="typing")
    
    # æ„å»ºä¸Šä¸‹æ–‡
    context_messages = get_user_context(context)
    context_messages.append({
        "role": "user",
        "parts": [{"text": text}]
    })
    
    # ç”Ÿæˆå›å¤
    from services.ai_service import AiService
    ai_service = AiService()
    
    enable_memory = (intent == UserIntent.MEMORY_RECALL)
    if enable_memory:
        logger.info(f"Memory tools enabled for voice intent: {intent}")
    
    final_text_response = ""
    last_update_time = 0
    
    async for chunk_text in ai_service.generate_response_stream(user_id, context_messages, enable_memory=enable_memory):
        final_text_response += chunk_text
        
        now = time.time()
        if now - last_update_time > 0.8:
            await smart_edit_text(thinking_msg, final_text_response)
            last_update_time = now
    
    # å‘é€æœ€ç»ˆå›å¤
    if final_text_response:
        await smart_edit_text(thinking_msg, final_text_response)
        add_message(context, "model", final_text_response)
        await increment_stat(user_id, "voice_chats")
    else:
        await smart_edit_text(thinking_msg, "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚è¯·ç¨åå†è¯•ã€‚")
