import time
import logging
import base64
from telegram import Update
from telegram.ext import ContextTypes
from telegram.error import BadRequest

from config import gemini_client, GEMINI_MODEL
from web_summary import extract_urls, summarize_webpage, is_video_platform, fetch_webpage_content
from user_context import get_user_context, add_message
from database import get_chat_message, get_user_settings, get_video_cache
from utils import smart_edit_text, smart_reply_text
from stats import increment_stat

logger = logging.getLogger(__name__)

# æ€è€ƒæç¤ºæ¶ˆæ¯
THINKING_MESSAGE = "ğŸ¤” æ­£åœ¨æ€è€ƒä¸­..."


async def handle_ai_chat(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """
    å¤„ç†æ™®é€šæ–‡æœ¬æ¶ˆæ¯ï¼Œä½¿ç”¨ Gemini AI ç”Ÿæˆå›å¤
    æ”¯æŒå¼•ç”¨ï¼ˆå›å¤ï¼‰åŒ…å«å›¾ç‰‡æˆ–è§†é¢‘çš„æ¶ˆæ¯
    """
    user_message = update.message.text
    chat_id = update.message.chat_id
    user_id = update.message.from_user.id

    if not user_message:
        return

    # æ£€æŸ¥ç”¨æˆ·æƒé™
    from config import is_user_allowed
    if not await is_user_allowed(user_id):
        await smart_reply_text(update,
            "â›” æŠ±æ­‰ï¼Œæ‚¨æ²¡æœ‰ä½¿ç”¨ AI å¯¹è¯åŠŸèƒ½çš„æƒé™ã€‚\n\n"
            "å¦‚éœ€ä¸‹è½½è§†é¢‘ï¼Œè¯·ä½¿ç”¨ /download å‘½ä»¤ã€‚"
        )
        return

    # æ£€æŸ¥æ¶ˆæ¯ä¸­æ˜¯å¦åŒ…å« URLï¼ˆè‡ªåŠ¨ç”Ÿæˆç½‘é¡µæ‘˜è¦ï¼‰
    urls = extract_urls(user_message)
    
    # å¦‚æœåªæ˜¯ä¸€ä¸ª URL ä¸”æ²¡æœ‰å…¶ä»–å†…å®¹
    if urls and user_message.strip() in urls:
        url = urls[0]
        
        # æ™ºèƒ½é€»è¾‘ï¼šå¦‚æœæ˜¯è§†é¢‘å¹³å°é“¾æ¥ï¼Œè¯¢é—®ç”¨æˆ·æ„å›¾
        if is_video_platform(url):
            context.user_data['pending_video_url'] = url
            
            from telegram import InlineKeyboardButton, InlineKeyboardMarkup
            keyboard = [
                [
                    InlineKeyboardButton("ğŸ“¹ ä¸‹è½½è§†é¢‘", callback_data="action_download_video"),
                    InlineKeyboardButton("ğŸ“ AI æ‘˜è¦", callback_data="action_summarize_video"),
                ]
            ]
            reply_markup = InlineKeyboardMarkup(keyboard)
            
            await smart_reply_text(update,
                "ğŸ¤” æ£€æµ‹åˆ°è§†é¢‘é“¾æ¥ï¼Œæ‚¨æƒ³è¦åšä»€ä¹ˆï¼Ÿ",
                reply_markup=reply_markup
            )
            return

        # æ™®é€šç½‘é¡µï¼Œç›´æ¥ç”Ÿæˆæ‘˜è¦
        thinking_msg = await smart_reply_text(update, "ğŸ“„ æ­£åœ¨è·å–ç½‘é¡µå†…å®¹å¹¶ç”Ÿæˆæ‘˜è¦...")
        await context.bot.send_chat_action(chat_id=chat_id, action="typing")
        
        summary = await summarize_webpage(url)
        # Use smart_edit_text which handles Markdown conversion and fallbacks
        await smart_edit_text(thinking_msg, summary)
        
        # è®°å½•ç»Ÿè®¡
        await increment_stat(user_id, "ai_chats")
        return

    # æ£€æŸ¥æ˜¯å¦å¼€å¯äº†æ²‰æµ¸å¼ç¿»è¯‘
    settings = await get_user_settings(user_id)
    if settings.get("auto_translate", 0):
        # ç¿»è¯‘æ¨¡å¼å¼€å¯
        thinking_msg = await smart_reply_text(update, "ğŸŒ ç¿»è¯‘ä¸­...")
        await context.bot.send_chat_action(chat_id=chat_id, action="typing")
        
        try:
            response = gemini_client.models.generate_content(
                model=GEMINI_MODEL,
                contents=user_message,
                config={
                    "system_instruction": (
                        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹è§„åˆ™è¿›è¡Œç¿»è¯‘ï¼š\n"
                        "1. å¦‚æœè¾“å…¥æ˜¯ä¸­æ–‡ï¼Œè¯·ç¿»è¯‘æˆè‹±æ–‡ã€‚\n"
                        "2. å¦‚æœè¾“å…¥æ˜¯å…¶ä»–è¯­è¨€ï¼Œè¯·ç¿»è¯‘æˆç®€ä½“ä¸­æ–‡ã€‚\n"
                        "3. åªè¾“å‡ºè¯‘æ–‡ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–é¢å¤–çš„æ–‡æœ¬ã€‚\n"
                        "4. ä¿æŒåŸæ–‡çš„è¯­æ°”å’Œæ ¼å¼ã€‚"
                    ),
                },
            )
            if response.text:
                await smart_edit_text(thinking_msg, f"ğŸŒ **è¯‘æ–‡**\n\n{response.text}")
                # ç»Ÿè®¡
                await increment_stat(user_id, "translations_count")
            if response.text:
                await smart_edit_text(thinking_msg, f"ğŸŒ **è¯‘æ–‡**\n\n{response.text}")
                # ç»Ÿè®¡
                await increment_stat(user_id, "translations_count")
            else:
                await smart_edit_text(thinking_msg, "âŒ æ— æ³•ç¿»è¯‘ã€‚")
        except Exception as e:
            logger.error(f"Translation error: {e}")
            await smart_edit_text(thinking_msg, "âŒ ç¿»è¯‘æœåŠ¡å‡ºé”™ã€‚")
        return

    # --- Smart Intent Routing ---
    from intent_router import analyze_intent, UserIntent
    
    # Analyze intent
    # We pass the user message. The router uses a fast model to determine intent.
    intent_result = await analyze_intent(user_message)
    intent = intent_result.get("intent")
    params = intent_result.get("params", {})
    
    logger.info(f"Smart Routing: {intent} | params={params}")

    if intent == UserIntent.DOWNLOAD_VIDEO:
        # å°è¯•ä» params è·å– URLï¼Œæˆ–è€…å›é€€åˆ° extract_urls
        target_url = params.get("url")
        if not target_url:
             # Fallback extraction
            found_urls = extract_urls(user_message)
            if found_urls:
                target_url = found_urls[0]
        
        if target_url:
            # await update.message.reply_text(f"ğŸš€ è¯†åˆ«åˆ°ä¸‹è½½æ„å›¾ï¼Œæ­£åœ¨å¤„ç†é“¾æ¥ï¼š{target_url}")
            from .media_handlers import process_video_download
            # Force non-audio-only (default) unless specified (could extend router to detect audio only)
            # For now, default to video.
            await process_video_download(update, context, target_url, audio_only=False)
            return
        else:
             # å¦‚æœæ„å›¾æ˜¯ä¸‹è½½ä½†æ²¡æ‰¾åˆ° URLï¼Œå¯èƒ½ç”¨æˆ·åªè¯´äº†"ä¸‹è½½è§†é¢‘"ä½†æ²¡ç»™è¿æ¥ã€‚
             # æ­¤æ—¶è®©å…¶è¿›å…¥å¸¸è§„å¯¹è¯ï¼Œæˆ–è€…ç”± Gemini å›å¤è¯¢é—®ã€‚
             pass

    elif intent == UserIntent.GENERATE_IMAGE:
        prompt = params.get("prompt")
        if not prompt:
            prompt = user_message # Fallback to full message
            
        # await update.message.reply_text(f"ğŸ¨ è¯†åˆ«åˆ°ç”»å›¾æ„å›¾ï¼Œæ­£åœ¨ç”Ÿæˆï¼š{prompt}")
        from image_generator import handle_image_generation
        await handle_image_generation(update, context, prompt)
        return

    elif intent == UserIntent.SET_REMINDER:
        time_str = params.get("time")
        content = params.get("content")
        
        if time_str and content:
            from .service_handlers import process_remind
            await process_remind(update, context, time_str, content)
            return
        else:
             # Missing params, fallback to Chat or ask user
             pass

    elif intent == UserIntent.RSS_SUBSCRIBE:
        url = params.get("url")
        if url:
             from .service_handlers import process_subscribe
             await process_subscribe(update, context, url)
             return

    elif intent == UserIntent.MONITOR_KEYWORD:
        keyword = params.get("keyword")
        if keyword:
             from .service_handlers import process_monitor
             await process_monitor(update, context, keyword)
             return

    elif intent == UserIntent.BROWSER_ACTION:
        from .mcp_handlers import handle_browser_action
        handled = await handle_browser_action(update, context, params)
        if handled:
            return
        # å¦‚æœæœªå¤„ç†ï¼ˆå¦‚ MCP ç¦ç”¨ï¼‰ï¼Œå›é€€åˆ°æ™®é€šå¯¹è¯

    # ----------------------------

    # æ£€æŸ¥æ˜¯å¦å¼•ç”¨äº†åŒ…å«åª’ä½“çš„æ¶ˆæ¯
    reply_to = update.message.reply_to_message
    has_media = False
    media_data = None
    mime_type = None
    extra_context = ""
    
    if reply_to:
        # 1. å°è¯•æå–å¼•ç”¨æ¶ˆæ¯ä¸­çš„ URL å¹¶è·å–å†…å®¹
        reply_urls = []
        
        # DEBUG LOG
        logger.info(f"Checking reply_to message {reply_to.message_id} for URLs")
        
        # A. ä»å®ä½“ï¼ˆè¶…é“¾æ¥/æ–‡æœ¬é“¾æ¥ï¼‰æå–
        if reply_to.entities:
            for entity in reply_to.entities:
                logger.info(f"Found text entity: {entity.type} at offset {entity.offset}")
                if entity.type == "text_link":
                    reply_urls.append(entity.url)
                elif entity.type == "url":
                    reply_urls.append(reply_to.parse_entity(entity))

        if reply_to.caption_entities:
            for entity in reply_to.caption_entities:
                logger.info(f"Found caption entity: {entity.type} at offset {entity.offset}")
                if entity.type == "text_link":
                    reply_urls.append(entity.url)
                elif entity.type == "url":
                    reply_urls.append(reply_to.parse_caption_entity(entity))
                
        # B. ä»æ–‡æœ¬æ­£åˆ™æå– (å…œåº•ï¼Œé˜²æ­¢å®ä½“æœªè§£æ)
        if not reply_urls:
            reply_text = reply_to.text or reply_to.caption or ""
            found = extract_urls(reply_text)
            logger.info(f"Regex found URLs: {found}")
            reply_urls = found
        
        # å»é‡
        reply_urls = list(set(reply_urls))
        logger.info(f"Final detected reply_urls: {reply_urls}")

        if reply_urls:
            # å‘ç° URLï¼Œå°è¯•è·å–å†…å®¹
            # å…ˆå‘é€ä¸€ä¸ªæç¤ºï¼Œé¿å…ç”¨æˆ·ä»¥ä¸ºå¡æ­»
            status_msg = await smart_reply_text(update, "ğŸ“„ æ­£åœ¨è·å–å¼•ç”¨ç½‘é¡µå†…å®¹...")
            await context.bot.send_chat_action(chat_id=chat_id, action="typing")
            
            try:
                web_content = await fetch_webpage_content(reply_urls[0])
                if web_content:
                    extra_context = f"ã€å¼•ç”¨ç½‘é¡µå†…å®¹ã€‘\n{web_content}\n\n"
                    # è·å–æˆåŠŸï¼Œåˆ é™¤æç¤ºæ¶ˆæ¯
                    await status_msg.delete()
                else:
                    # è·å–å¤±è´¥ï¼Œæç¤º AI å‘ŠçŸ¥ç”¨æˆ·
                    extra_context = (
                        "ã€ç³»ç»Ÿæç¤ºã€‘å¼•ç”¨çš„ç½‘é¡µé“¾æ¥æ— æ³•è®¿é—®ï¼ˆæ— æ³•æå–å†…å®¹ï¼Œå¯èƒ½æ˜¯åçˆ¬è™«é™åˆ¶ï¼‰ã€‚"
                        "è¯·åœ¨å›ç­”ä¸­æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·ä½ æ— æ³•è¯»å–è¯¥é“¾æ¥çš„å†…å®¹ï¼Œå¹¶ä»…æ ¹æ®ç°æœ‰çš„æ–‡æœ¬ä¿¡æ¯è¿›è¡Œå›ç­”ã€‚"
                        "\n\n"
                    )
                    await status_msg.delete()
            except Exception as e:
                logger.error(f"Error fetching reply URL: {e}")
                # å‡ºé”™ä¹Ÿæç¤º AI
                extra_context = "ã€ç³»ç»Ÿæç¤ºã€‘è¯»å–é“¾æ¥æ—¶å‘ç”Ÿé”™è¯¯ã€‚è¯·å‘ŠçŸ¥ç”¨æˆ·æ— æ³•è®¿é—®è¯¥é“¾æ¥ã€‚\n\n"
                await status_msg.delete()

        # 2. å¤„ç†åª’ä½“
        if reply_to.video:
            has_media = True
            video = reply_to.video
            file_id = video.file_id
            mime_type = video.mime_type or "video/mp4"
            
            # ä¼˜å…ˆæ£€æŸ¥æœ¬åœ°ç¼“å­˜
            cache_path = await get_video_cache(file_id)
            
            if cache_path:
                import os
                if os.path.exists(cache_path):
                    logger.info(f"Using cached video: {cache_path}")
                    thinking_msg = await smart_reply_text(update, "ğŸ¬ æ­£åœ¨åˆ†æè§†é¢‘ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰...")
                    with open(cache_path, "rb") as f:
                        media_data = bytearray(f.read())
                else:
                    # ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨
                    pass 
            
            # ç¼“å­˜æœªå‘½ä¸­ï¼Œé€šè¿‡ Telegram API ä¸‹è½½
            if media_data is None:
                # æ£€æŸ¥å¤§å°é™åˆ¶ï¼ˆTelegram API é™åˆ¶ 20MBï¼‰
                if video.file_size and video.file_size > 20 * 1024 * 1024:
                    await smart_reply_text(update,
                        "âš ï¸ å¼•ç”¨çš„è§†é¢‘æ–‡ä»¶è¿‡å¤§ï¼ˆè¶…è¿‡ 20MBï¼‰ï¼Œæ— æ³•é€šè¿‡ Telegram ä¸‹è½½åˆ†æã€‚\n\n"
                        "æç¤ºï¼šBot ä¸‹è½½çš„è§†é¢‘ä¼šè¢«ç¼“å­˜ï¼Œå¯ä»¥ç›´æ¥åˆ†æã€‚"
                    )
                    return
                thinking_msg = await smart_reply_text(update, "ğŸ¬ æ­£åœ¨ä¸‹è½½å¹¶åˆ†æè§†é¢‘...")
                file = await context.bot.get_file(video.file_id)
                media_data = await file.download_as_bytearray()
                
        elif reply_to.photo:
            has_media = True
            photo = reply_to.photo[-1]
            mime_type = "image/jpeg"
            thinking_msg = await smart_reply_text(update, "ğŸ” æ­£åœ¨åˆ†æå›¾ç‰‡...")
            file = await context.bot.get_file(photo.file_id)
            media_data = await file.download_as_bytearray()

        elif reply_to.audio or reply_to.voice:
            has_media = True
            if reply_to.audio:
                file_id = reply_to.audio.file_id
                mime_type = reply_to.audio.mime_type or "audio/mpeg"
                file_size = reply_to.audio.file_size
                label = "éŸ³é¢‘"
            else:
                file_id = reply_to.voice.file_id
                mime_type = reply_to.voice.mime_type or "audio/ogg"
                file_size = reply_to.voice.file_size
                label = "è¯­éŸ³"

            # Check size limit (20MB)
            if file_size and file_size > 20 * 1024 * 1024:
                await smart_reply_text(update,
                    f"âš ï¸ å¼•ç”¨çš„{label}æ–‡ä»¶è¿‡å¤§ï¼ˆè¶…è¿‡ 20MBï¼‰ï¼Œæ— æ³•é€šè¿‡ Telegram ä¸‹è½½åˆ†æã€‚"
                )
                return

            thinking_msg = await smart_reply_text(update, f"ğŸ§ æ­£åœ¨åˆ†æ{label}...")
            file = await context.bot.get_file(file_id)
            media_data = await file.download_as_bytearray()
    
    # 3. æ£€æŸ¥å½“å‰æ¶ˆæ¯ä¸­æ˜¯å¦æœ‰ URL (æ··åˆæ–‡æœ¬æƒ…å†µ)
    # å¦‚æœ extra_context ä¸ºç©ºï¼ˆè¯´æ˜æ²¡æœ‰ Reply URLï¼‰ï¼Œä¸” urls ä¸ä¸ºç©ºï¼ˆè¯´æ˜å½“å‰æ¶ˆæ¯æœ‰ URLï¼‰
    if not extra_context and urls:
        status_msg = await smart_reply_text(update, "ğŸ“„ æ­£åœ¨è·å–ç½‘é¡µå†…å®¹...")
        await context.bot.send_chat_action(chat_id=chat_id, action="typing")
        
        try:
            # è·å–ç¬¬ä¸€ä¸ª URL çš„å†…å®¹
            web_content = await fetch_webpage_content(urls[0])
            
            if web_content:
                extra_context = f"ã€ç½‘é¡µå†…å®¹ã€‘\n{web_content}\n\n"
            else:
                logger.warning(f"Failed to fetch content for mixed URL: {urls[0]}")
                extra_context = "ã€ç³»ç»Ÿæç¤ºã€‘æ£€æµ‹åˆ°é“¾æ¥ï¼Œä½†æ— æ³•è¯»å–å…¶å†…å®¹ï¼ˆå¯èƒ½æ˜¯åçˆ¬è™«é™åˆ¶ï¼‰ã€‚è¯·ä»…æ ¹æ® URL æ ‡é¢˜æˆ–ä» URL æœ¬èº«æ¨æµ‹ï¼ˆå¦‚æœå¯èƒ½ï¼‰ï¼Œå¹¶å‘ŠçŸ¥ç”¨æˆ·æ— æ³•è¯»å–è¯¦æƒ…ã€‚\n\n"
            
        except Exception as e:
            logger.error(f"Error fetching mixed URL: {e}")
        
        # æ— è®ºæˆåŠŸå¤±è´¥ï¼Œåˆ é™¤å› ä¸º fetch è€Œäº§ç”Ÿçš„æç¤ºæ¶ˆæ¯
        try:
            await status_msg.delete()
        except:
            pass

    if not has_media:
        # æ™®é€šæ–‡æœ¬å¯¹è¯
        thinking_msg = await smart_reply_text(update, THINKING_MESSAGE)
    
    # å°†ç½‘é¡µä¸Šä¸‹æ–‡åˆå¹¶åˆ°ç”¨æˆ·æ¶ˆæ¯ä¸­
    if extra_context:
        user_message = extra_context + "ç”¨æˆ·è¯·æ±‚ï¼š" + user_message

    # å‘é€"æ­£åœ¨è¾“å…¥"çŠ¶æ€
    await context.bot.send_chat_action(chat_id=chat_id, action="typing")

    try:
        if has_media and media_data:
            # å¸¦åª’ä½“çš„è¯·æ±‚
            contents = [
                {
                    "parts": [
                        {"text": user_message},
                        {
                            "inline_data": {
                                "mime_type": mime_type,
                                "data": base64.b64encode(bytes(media_data)).decode("utf-8"),
                            }
                        },
                    ]
                }
            ]
            response = gemini_client.models.generate_content(
                model=GEMINI_MODEL,
                contents=contents,
                config={
                    "system_instruction": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹ï¼Œå¯ä»¥åˆ†æå›¾ç‰‡å’Œè§†é¢‘å†…å®¹å¹¶å›ç­”é—®é¢˜ã€‚è¯·ç”¨ä¸­æ–‡å›å¤ã€‚",
                },
            )
            if response.text:
                await smart_edit_text(thinking_msg, response.text)
            else:
                await smart_edit_text(thinking_msg, "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•åˆ†æè¿™ä¸ªå†…å®¹ã€‚")
        else:
            # çº¯æ–‡æœ¬å¯¹è¯ï¼ˆæµå¼å“åº” + å¤šè½®ä¸Šä¸‹æ–‡ï¼‰
            
            # 1. ä¿å­˜å½“å‰ç”¨æˆ·æ¶ˆæ¯
            current_msg_id = update.message.message_id
            await add_message(user_id, "user", user_message, message_id=current_msg_id)
            
            # -----------------------------------------------------------------
            # 2. æ„å»ºä¸Šä¸‹æ–‡
            context_messages = []
            
            # A. å¦‚æœæ˜¯å›å¤æŸä¸ªæ¶ˆæ¯ --> ä»…ä½¿ç”¨è¯¥æ¶ˆæ¯ + å½“å‰æ¶ˆæ¯
            if reply_to:
                reply_id = reply_to.message_id
                logger.info(f"User replied to message {reply_id}")
                
                # ç›´æ¥ä½¿ç”¨ Telegram æ¶ˆæ¯å¯¹è±¡çš„å†…å®¹
                replied_content = reply_to.text or reply_to.caption
                
                if replied_content:
                    context_messages.append({
                        "role": "user",  # è¢«å›å¤çš„æ¶ˆæ¯ä½œä¸ºä¸Šä¸€ä¸ª user message æˆ–è€… model message
                        "parts": [{"text": f"ã€å¼•ç”¨å†…å®¹ã€‘\n{replied_content}"}] 
                    })
                else:
                    logger.info("Replied message has no text content.")
            
            # B. å¦‚æœä¸æ˜¯å›å¤ --> ä½¿ç”¨æœ€è¿‘çš„å†å²è®°å½•
            else:
                context_messages = await get_user_context(user_id)
            
            # append current user message
            context_messages.append({
                "role": "user",
                "parts": [{"text": user_message}]
            })

            # -----------------------------------------------------------------
            # 3. å‡†å¤‡å·¥å…· (MCP Memory)
            from config import MCP_MEMORY_ENABLED
            tools_config = None
            
            if MCP_MEMORY_ENABLED:
                try:
                    from mcp_client import mcp_manager
                    from mcp_client.tools_bridge import convert_mcp_tools_to_gemini
                    from mcp_client.memory import register_memory_server
                    
                    # ç¡®ä¿ Memory Server ç±»å·²æ³¨å†Œ
                    register_memory_server()
                    
                    # è·å–è¯¥ç”¨æˆ·ä¸“å±çš„ Memory Server å®ä¾‹
                    # mcp_manager.get_server ä¼šä¸ºæ¯ä¸ª user_id åˆ›å»º/å¤ç”¨ç‹¬ç«‹çš„å®ä¾‹
                    # å®ä¾‹ Key å¦‚: memory_12345
                    memory_server = await mcp_manager.get_server("memory", user_id=user_id)
                    
                    if memory_server and memory_server.session:
                        # ä¸»åŠ¨åˆ—å‡ºå·¥å…·
                        mcp_tools_result = await memory_server.session.list_tools()
                        gemini_funcs = convert_mcp_tools_to_gemini(mcp_tools_result.tools)
                        
                        # æŒ‰ Gemini æ ¼å¼åŒ…è£…
                        if gemini_funcs:
                            tools_config = [{"function_declarations": gemini_funcs}]
                            logger.info(f"Injected {len(gemini_funcs)} memory tools into Gemini for user {user_id}.")
                except Exception as e:
                    logger.error(f"Failed to setup memory tools: {e}")

            # -----------------------------------------------------------------
            # 4. ç”Ÿæˆå›å¤ (æ”¯æŒ Function Calling å¾ªç¯)
            
            # å®šä¹‰æœ€å¤§å¾ªç¯æ¬¡æ•°é˜²æ­¢æ­»å¾ªç¯
            MAX_TURNS = 5
            turn_count = 0
            final_text_response = ""
            
            while turn_count < MAX_TURNS:
                turn_count += 1
                
                # å¦‚æœæœ‰ toolsï¼Œé¦–è½®ä½¿ç”¨éæµå¼ä»¥æ”¯æŒ Function Calling
                # å¦‚æœ tools_config ä¸ºç©ºï¼Œåˆ™å›é€€åˆ°æµå¼
                if tools_config:
                    response = gemini_client.models.generate_content(
                        model=GEMINI_MODEL,
                        contents=context_messages,
                        config={
                            "system_instruction": (
                                "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹ã€‚è¯·ç”¨ä¸­æ–‡å›å¤ã€‚\n\n"
                                "ã€è®°å¿†ç®¡ç†æŒ‡å—ã€‘\n"
                                "è¯·éµå¾ªä»¥ä¸‹æ­¥éª¤è¿›è¡Œäº¤äº’ï¼š\n\n"
                                "1. **èº«ä»½è¯†åˆ«**ï¼š\n"
                                "   - å§‹ç»ˆå°†å½“å‰äº¤äº’ç”¨æˆ·è§†ä¸ºå®ä½“ 'User'ã€‚\n\n"
                                "2. **è®°å¿†æ£€ç´¢ï¼ˆMemory Retrievalï¼‰**ï¼š\n"
                                "   - åœ¨å›ç­”ä¹‹å‰ï¼Œç§¯æä½¿ç”¨ `open_nodes(names=['User'])` æ£€ç´¢å…³äº 'User' çš„æ‰€æœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚\n"
                                "   - å¦‚æœé‡åˆ°ç‰¹å®šè¯é¢˜ï¼Œä¹Ÿå¯ä»¥é€šè¿‡å…³é”®è¯æœç´¢ç›¸å…³èŠ‚ç‚¹ã€‚\n\n"
                                "3. **è®°å¿†æ›´æ–°ï¼ˆMemory Updateï¼‰**ï¼š\n"
                                "   - åœ¨å¯¹è¯ä¸­æ—¶åˆ»å…³æ³¨ä»¥ä¸‹ç±»åˆ«çš„æ–°ä¿¡æ¯ï¼š\n"
                                "     a) **åŸºæœ¬èº«ä»½**ï¼šå¹´é¾„ã€æ€§åˆ«ã€å±…ä½åœ°ï¼ˆLocationï¼‰ã€èŒä¸šç­‰ã€‚\n"
                                "     b) **è¡Œä¸ºä¹ æƒ¯**ã€**åå¥½**ã€**ç›®æ ‡**ã€**å…³ç³»**ç­‰ã€‚\n\n"
                                "   - å½“æ•è·åˆ°æ–°ä¿¡æ¯æ—¶ï¼š\n"
                                "     a) ä½¿ç”¨ `create_entities` ä¸ºé‡è¦çš„äººã€åœ°ç‚¹ã€ç»„ç»‡åˆ›å»ºå®ä½“ã€‚\n"
                                "     b) ä½¿ç”¨ `create_relations` å°†å®ƒä»¬è¿æ¥åˆ° 'User'ï¼ˆä¾‹å¦‚ï¼šRelation('User', 'lives in', 'æ— é”¡')ï¼‰ã€‚\n"
                                "     c) ä½¿ç”¨ `add_observations` å­˜å‚¨å…·ä½“çš„è§‚å¯Ÿäº‹å®ã€‚\n"
                            ),
                            "tools": tools_config
                        },
                    )
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰ function call
                    # Gemini Python SDK genai.types structure:
                    # response.candidates[0].content.parts[0].function_call
                    function_calls = []
                    
                    if response.candidates and response.candidates[0].content and response.candidates[0].content.parts:
                        for part in response.candidates[0].content.parts:
                            if part.function_call:
                                function_calls.append(part.function_call)
                    
                    if function_calls:
                        # æœ‰å·¥å…·è°ƒç”¨è¯·æ±‚
                        logger.info(f"AI requested function calls: {[fc.name for fc in function_calls]}")
                        
                        # 1. å°†æ¨¡å‹å›å¤ï¼ˆåŒ…å« function_callï¼‰åŠ å…¥å†å²
                        context_messages.append(response.candidates[0].content)
                        
                        # 2. æ‰§è¡Œæ‰€æœ‰å·¥å…·
                        for fc in function_calls:
                            tool_name = fc.name
                            tool_args = fc.args
                            
                            logger.info(f"Executing tool: {tool_name} args={tool_args}")
                            
                            tool_result_content = {}
                            try:
                                # æ‰§è¡Œ MCP å·¥å…·
                                # æ³¨æ„: memory server çš„ override å·²ç»åœ¨ call_tool å†…éƒ¨å¤„ç†å¥½äº† schema æ ¡éªŒé—®é¢˜
                                
                                # ä½¿ç”¨ mcp_manager.call_tool éœ€è¦çŸ¥é“å‡†ç¡®çš„ instance_key
                                # æˆ–è€…ç›´æ¥ä½¿ç”¨æˆ‘ä»¬ä¸Šé¢è·å–åˆ°çš„ memory_server å®ä¾‹ (å¦‚æœåœ¨ scope å†…)
                                # ä¹‹å‰æˆ‘ä»¬åœ¨ scope 435è¡Œå·¦å³è·å–äº† memory_serverã€‚
                                # ä½†æ˜¯è¯¥å˜é‡åœ¨ while å¾ªç¯ä¹‹å¤–ã€‚
                                # Python å˜é‡ä½œç”¨åŸŸåœ¨å‡½æ•°å†…æ˜¯å¯è§çš„ã€‚
                                
                                # ä½†æ˜¯ï¼Œå¦‚æœ multiple servers (e.g. playwright + memory), éœ€è¦åŒºåˆ†ã€‚
                                # Playwright å·¥å…·ä¸æ˜¯ memory å·¥å…·ã€‚
                                # ç®€å•åˆ¤æ–­ï¼šå¦‚æœ tool_name åœ¨ memory tools ä¸­ï¼Œåˆ™è°ƒ memory_serverã€‚
                                # ç›®å‰ tools_config åªæœ‰ memoryã€‚
                                
                                # ä¸ºäº†å¥å£®æ€§ï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥ tool_name æ˜¯å¦å±äº memory_server çš„ capabilities?
                                # æˆ–è€…ç®€å•åœ°ï¼šå½“å‰åœºæ™¯æˆ‘ä»¬åªæ³¨å…¥äº† memory toolsã€‚
                                
                                if memory_server:
                                     raw_result = await memory_server.call_tool(tool_name, tool_args)
                                else:
                                     # Fallback (unlikely)
                                     raw_result = await mcp_manager.call_tool("memory", tool_name, tool_args)
                                
                                tool_result_content = {"result": raw_result}
                            except Exception as e:
                                logger.error(f"Tool execution failed: {e}")
                                tool_result_content = {"error": str(e)}
                                
                            # 3. å°†å·¥å…·ç»“æœï¼ˆFunctionResponseï¼‰åŠ å…¥å†å²
                            context_messages.append({
                                "role": "tool", # Gemini SDK æœŸæœ› role="tool"
                                "parts": [{
                                    "function_response": {
                                        "name": tool_name,
                                        "response": tool_result_content
                                    }
                                }]
                            })
                            
                        # ç»§ç»­ä¸‹ä¸€è½®å¾ªç¯ï¼ŒæŠŠå·¥å…·ç»“æœå‘å›ç»™æ¨¡å‹
                        continue
                        
                    else:
                        # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿™æ˜¯æœ€ç»ˆå›å¤
                        # æå–æ–‡æœ¬
                        if response.text:
                            final_text_response = response.text
                        else:
                            final_text_response = "ï¼ˆæ— æ–‡æœ¬å›å¤ï¼‰"
                        break
                        
                else:
                    # æ²¡æœ‰å·¥å…·é…ç½®ï¼Œèµ°åŸæ¥çš„æµå¼é€»è¾‘
                    response = gemini_client.models.generate_content_stream(
                        model=GEMINI_MODEL,
                        contents=context_messages,
                        config={
                            "system_instruction": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·è§£ç­”é—®é¢˜ã€‚è¯·ç”¨ä¸­æ–‡å›å¤ã€‚",
                        },
                    )
                    
                    # æµå¼å¤„ç†
                    last_update_time = 0
                    for chunk in response:
                        if chunk.text:
                            final_text_response += chunk.text
                            # æ¯ 0.8 ç§’æ›´æ–°ä¸€æ¬¡æ¶ˆæ¯ (æµå¼æ¨¡å¼ä¸‹)
                            now = time.time()
                            if now - last_update_time > 0.8:
                                await smart_edit_text(thinking_msg, final_text_response)
                                last_update_time = now
                    break

            # -----------------------------------------------------------------
            # 5. å‘é€æœ€ç»ˆå›å¤å¹¶å…¥åº“
            if final_text_response:
                # smart_edit_text handles markdown formatting and errors
                sent_msg = await smart_edit_text(thinking_msg, final_text_response)
                
                if sent_msg:
                    await add_message(user_id, "model", final_text_response, message_id=sent_msg.message_id)
                else:
                    await add_message(user_id, "model", final_text_response)

                # è®°å½•ç»Ÿè®¡
                await increment_stat(user_id, "ai_chats")
            else:
                await smart_edit_text(thinking_msg, "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚è¯·ç¨åå†è¯•ã€‚")

    except Exception as e:
        logger.error(f"AI chat error: {e}")
    except Exception as e:
        logger.error(f"AI chat error: {e}")
        await smart_edit_text(thinking_msg,
            "âŒ AI æœåŠ¡å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚\n\n"
            "å¦‚éœ€ä¸‹è½½è§†é¢‘ï¼Œè¯·ç‚¹å‡» /download è¿›å…¥ä¸‹è½½æ¨¡å¼ã€‚"
        )


async def handle_ai_photo(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """
    å¤„ç†å›¾ç‰‡æ¶ˆæ¯ï¼Œä½¿ç”¨ Gemini AI åˆ†æå›¾ç‰‡
    """
    chat_id = update.message.chat_id
    user_id = update.message.from_user.id
    
    # æ£€æŸ¥ç”¨æˆ·æƒé™
    from config import is_user_allowed
    from config import is_user_allowed
    if not is_user_allowed(user_id):
        await smart_reply_text(update,
            "â›” æŠ±æ­‰ï¼Œæ‚¨æ²¡æœ‰ä½¿ç”¨ AI åŠŸèƒ½çš„æƒé™ã€‚"
        )
        return
    
    # è·å–å›¾ç‰‡ï¼ˆé€‰æ‹©æœ€å¤§åˆ†è¾¨ç‡ï¼‰
    photo = update.message.photo[-1]
    caption = update.message.caption or "è¯·æè¿°è¿™å¼ å›¾ç‰‡"
    
    # ç«‹å³å‘é€"æ­£åœ¨åˆ†æ"æç¤º
    thinking_msg = await smart_reply_text(update, "ğŸ” æ­£åœ¨åˆ†æå›¾ç‰‡...")
    
    # å‘é€"æ­£åœ¨è¾“å…¥"çŠ¶æ€
    await context.bot.send_chat_action(chat_id=chat_id, action="typing")
    
    try:
        # ä¸‹è½½å›¾ç‰‡
        file = await context.bot.get_file(photo.file_id)
        image_bytes = await file.download_as_bytearray()
        
        # æ„å»ºå¸¦å›¾ç‰‡çš„å†…å®¹
        contents = [
            {
                "parts": [
                    {"text": caption},
                    {
                        "inline_data": {
                            "mime_type": "image/jpeg",
                            "data": base64.b64encode(bytes(image_bytes)).decode("utf-8"),
                        }
                    },
                ]
            }
        ]
        
        # è°ƒç”¨ Gemini API
        response = gemini_client.models.generate_content(
            model=GEMINI_MODEL,
            contents=contents,
            config={
                "system_instruction": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹ï¼Œå¯ä»¥åˆ†æå›¾ç‰‡å¹¶å›ç­”é—®é¢˜ã€‚è¯·ç”¨ä¸­æ–‡å›å¤ã€‚",
            },
        )
        
        if response.text:
            await smart_edit_text(thinking_msg, response.text)
            # è®°å½•ç»Ÿè®¡
            await increment_stat(user_id, "photo_analyses")
        if response.text:
            await smart_edit_text(thinking_msg, response.text)
            # è®°å½•ç»Ÿè®¡
            await increment_stat(user_id, "photo_analyses")
        else:
            await smart_edit_text(thinking_msg, "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•åˆ†æè¿™å¼ å›¾ç‰‡ã€‚è¯·ç¨åå†è¯•ã€‚")
        
    except Exception as e:
        logger.error(f"AI photo analysis error: {e}")
    except Exception as e:
        logger.error(f"AI photo analysis error: {e}")
        await smart_edit_text(thinking_msg, "âŒ å›¾ç‰‡åˆ†æå¤±è´¥ï¼Œè¯·ç¨åå†è¯•ã€‚")


async def handle_ai_video(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """
    å¤„ç†è§†é¢‘æ¶ˆæ¯ï¼Œä½¿ç”¨ Gemini AI åˆ†æè§†é¢‘
    """
    chat_id = update.message.chat_id
    user_id = update.message.from_user.id
    
    # æ£€æŸ¥ç”¨æˆ·æƒé™
    from config import is_user_allowed
    from config import is_user_allowed
    if not await is_user_allowed(user_id):
        await smart_reply_text(update,
            "â›” æŠ±æ­‰ï¼Œæ‚¨æ²¡æœ‰ä½¿ç”¨ AI åŠŸèƒ½çš„æƒé™ã€‚"
        )
        return
    
    # è·å–è§†é¢‘
    video = update.message.video
    if not video:
        return
    
    caption = update.message.caption or "è¯·åˆ†æè¿™ä¸ªè§†é¢‘çš„å†…å®¹"
    
    # æ£€æŸ¥è§†é¢‘å¤§å°ï¼ˆGemini æœ‰é™åˆ¶ï¼‰
    # æ£€æŸ¥è§†é¢‘å¤§å°ï¼ˆGemini æœ‰é™åˆ¶ï¼‰
    if video.file_size and video.file_size > 20 * 1024 * 1024:  # 20MB é™åˆ¶
        await smart_reply_text(update,
            "âš ï¸ è§†é¢‘æ–‡ä»¶è¿‡å¤§ï¼ˆè¶…è¿‡ 20MBï¼‰ï¼Œæ— æ³•åˆ†æã€‚\n\n"
            "è¯·å°è¯•å‘é€è¾ƒçŸ­çš„è§†é¢‘ç‰‡æ®µã€‚"
        )
        return
    
    # ç«‹å³å‘é€"æ­£åœ¨åˆ†æ"æç¤º
    # ç«‹å³å‘é€"æ­£åœ¨åˆ†æ"æç¤º
    thinking_msg = await smart_reply_text(update, "ğŸ¬ æ­£åœ¨åˆ†æè§†é¢‘ï¼Œè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´...")
    
    # å‘é€"æ­£åœ¨è¾“å…¥"çŠ¶æ€
    await context.bot.send_chat_action(chat_id=chat_id, action="typing")
    
    try:
        # ä¸‹è½½è§†é¢‘
        file = await context.bot.get_file(video.file_id)
        video_bytes = await file.download_as_bytearray()
        
        # è·å– MIME ç±»å‹
        mime_type = video.mime_type or "video/mp4"
        
        # æ„å»ºå¸¦è§†é¢‘çš„å†…å®¹
        contents = [
            {
                "parts": [
                    {"text": caption},
                    {
                        "inline_data": {
                            "mime_type": mime_type,
                            "data": base64.b64encode(bytes(video_bytes)).decode("utf-8"),
                        }
                    },
                ]
            }
        ]
        
        # è°ƒç”¨ Gemini API
        response = gemini_client.models.generate_content(
            model=GEMINI_MODEL,
            contents=contents,
            config={
                "system_instruction": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹ï¼Œå¯ä»¥åˆ†æè§†é¢‘å†…å®¹å¹¶å›ç­”é—®é¢˜ã€‚è¯·ç”¨ä¸­æ–‡å›å¤ã€‚",
            },
        )
        
        if response.text:
            await smart_edit_text(thinking_msg, response.text)
            # è®°å½•ç»Ÿè®¡
            await increment_stat(user_id, "video_analyses")
        else:
            await smart_edit_text(thinking_msg, "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•åˆ†æè¿™ä¸ªè§†é¢‘ã€‚è¯·ç¨åå†è¯•ã€‚")
        
    except Exception as e:
        logger.error(f"AI video analysis error: {e}")
    except Exception as e:
        logger.error(f"AI video analysis error: {e}")
        await smart_edit_text(thinking_msg,
            "âŒ è§†é¢‘åˆ†æå¤±è´¥ï¼Œè¯·ç¨åå†è¯•ã€‚\n\n"
            "å¯èƒ½çš„åŸå› ï¼š\n"
            "â€¢ è§†é¢‘æ ¼å¼ä¸æ”¯æŒ\n"
            "â€¢ è§†é¢‘æ—¶é•¿è¿‡é•¿\n"
            "â€¢ æœåŠ¡æš‚æ—¶ä¸å¯ç”¨"
        )
