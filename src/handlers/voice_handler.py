"""
è¯­éŸ³æ¶ˆæ¯å¤„ç†æ¨¡å—

ç»Ÿä¸€å°†è¯­éŸ³è½¬å†™ä¸ºæ–‡å­—åï¼Œå†æŒ‰æ™®é€šæ–‡æœ¬æ¶ˆæ¯å¤„ç†ï¼Œæˆ–è¿›è¡Œè¯­éŸ³ç¿»è¯‘ã€‚
"""

import logging
import base64
import asyncio
import json
import re
from typing import Any, cast
from telegram.error import BadRequest

from core.config import VOICE_MODEL, is_user_allowed, openai_async_client
from core.platform.exceptions import MediaProcessingError
from services.openai_adapter import build_messages
from user_context import add_message, get_user_context
from core.platform.models import MessageType, UnifiedContext
from .media_utils import extract_media_input

logger = logging.getLogger(__name__)


def _normalize_transcribed_text(raw_text: str) -> str:
    text = str(raw_text or "").strip()
    if not text:
        return ""

    for wrapped in ("```json", "```"):
        if text.lower().startswith(wrapped):
            text = text[len(wrapped) :].strip()
    if text.endswith("```"):
        text = text[:-3].strip()

    if text.lower().startswith("json"):
        text = text[4:].strip()

    if text.startswith("{"):
        try:
            parsed = json.loads(text)
            if isinstance(parsed, dict):
                for key in ("text", "transcript", "content", "result"):
                    value = parsed.get(key)
                    if isinstance(value, str) and value.strip():
                        text = value.strip()
                        break
        except Exception:
            pass

    # Remove common wrapper labels.
    for prefix in ("è½¬å†™ï¼š", "è½¬å†™ç»“æœï¼š", "è¯†åˆ«ç»“æœï¼š", "æ–‡æœ¬ï¼š"):
        if text.startswith(prefix):
            text = text[len(prefix) :].strip()

    # Strip symmetrical quote wrappers repeatedly.
    pairs = (
        ('"', '"'),
        ("'", "'"),
        ("`", "`"),
        ("â€œ", "â€"),
        ("â€˜", "â€™"),
    )
    changed = True
    while changed and len(text) >= 2:
        changed = False
        for left, right in pairs:
            if text.startswith(left) and text.endswith(right):
                text = text[len(left) : len(text) - len(right)].strip()
                changed = True
                break

    # Quote/punctuation only output means model produced no usable transcript.
    if re.fullmatch(r'[\s"`\'â€œâ€â€˜â€™.,ï¼Œã€‚!?ï¼ï¼Ÿ:ï¼š;ï¼›\-\(\)\[\]\{\}â€¦]+', text or ""):
        return ""
    return text


def _extract_model_text(response) -> str:
    if response is None:
        return ""

    try:
        direct_text = getattr(response, "text", None)
    except Exception:
        direct_text = None
    if direct_text is not None:
        text = str(direct_text).strip()
        if text:
            return text

    choices = getattr(response, "choices", None) or []
    for choice in choices:
        message = getattr(choice, "message", None)
        content = getattr(message, "content", "")
        if isinstance(content, str) and content.strip():
            return content.strip()
        if isinstance(content, list):
            chunks = []
            for part in content:
                if isinstance(part, dict) and part.get("type") == "text":
                    chunks.append(str(part.get("text") or ""))
            merged = "\n".join([item for item in chunks if item]).strip()
            if merged:
                return merged

    candidates = getattr(response, "candidates", None) or []
    for candidate in candidates:
        content = getattr(candidate, "content", None)
        parts = getattr(content, "parts", None) or []
        chunks = []
        for part in parts:
            part_text = getattr(part, "text", None)
            if part_text:
                chunks.append(str(part_text))
        merged = "\n".join(chunks).strip()
        if merged:
            return merged
    return ""


def _audio_mime_candidates(mime_type: str) -> list[str]:
    raw = str(mime_type or "").strip()
    base = raw.split(";", 1)[0].strip().lower() if raw else ""
    candidates: list[str] = []

    def add(item: str) -> None:
        value = str(item or "").strip()
        if value and value not in candidates:
            candidates.append(value)

    add(raw)
    add(base)

    if base in {"audio/ogg", "audio/opus", "audio/x-opus", "application/ogg"}:
        add("audio/ogg")
        add("audio/ogg; codecs=opus")
        add("audio/opus")
    if base in {"audio/mp3", "audio/mpeg"}:
        add("audio/mpeg")
        add("audio/mp3")

    add("audio/ogg")
    add("audio/ogg; codecs=opus")
    add("audio/webm")
    add("audio/mpeg")
    add("audio/mp4")
    add("audio/wav")
    return candidates


def _audio_base_mime(mime_type: str) -> str:
    return str(mime_type or "").split(";", 1)[0].strip().lower()


def _sniff_audio_container(voice_bytes: bytes) -> str | None:
    if not voice_bytes:
        return None

    head16 = bytes(voice_bytes[:16])
    head32 = bytes(voice_bytes[:32])

    if head16.startswith(b"OggS"):
        return "ogg"
    if len(head16) >= 12 and head16.startswith(b"RIFF") and head16[8:12] == b"WAVE":
        return "wav"
    if head16.startswith(b"\x1aE\xdf\xa3"):
        return "webm"
    if head16.startswith(b"fLaC"):
        return "flac"
    if head16.startswith(b"ID3"):
        return "mp3"
    if len(head16) >= 2 and head16[0] == 0xFF and (head16[1] & 0xE0) == 0xE0:
        return "mp3"
    if b"ftyp" in head32:
        return "mp4"
    return None


def _ffmpeg_input_format(mime_type: str, voice_bytes: bytes) -> str | None:
    base = _audio_base_mime(mime_type)
    if base in {"audio/ogg", "application/ogg", "audio/opus", "audio/x-opus"}:
        return "ogg"
    if base in {"audio/webm"}:
        return "webm"
    if base in {"audio/mp4", "audio/x-m4a"}:
        return "mp4"
    if base in {"audio/aac", "audio/x-aac"}:
        return "aac"
    if "mpeg" in base or "mp3" in base:
        return "mp3"
    if "wav" in base:
        return "wav"
    if "flac" in base:
        return "flac"

    sniffed = _sniff_audio_container(voice_bytes)
    if sniffed in {"ogg", "webm", "mp4", "aac", "mp3", "wav", "flac"}:
        return sniffed
    return None


def _should_try_wav_transcode(mime_type: str, voice_bytes: bytes) -> bool:
    base = _audio_base_mime(mime_type)
    if "wav" in base:
        return False

    if base in {
        "audio/ogg",
        "application/ogg",
        "audio/opus",
        "audio/x-opus",
        "audio/webm",
        "audio/mp4",
        "audio/x-m4a",
        "audio/aac",
        "audio/x-aac",
        "audio/flac",
    }:
        return True

    return _sniff_audio_container(voice_bytes) in {"ogg", "webm", "mp4", "flac"}


async def _transcode_audio_to_wav(voice_bytes: bytes, mime_type: str) -> bytes | None:
    if not voice_bytes or not _should_try_wav_transcode(mime_type, voice_bytes):
        return None

    cmd = ["ffmpeg", "-nostdin", "-hide_banner", "-loglevel", "error"]
    input_fmt = _ffmpeg_input_format(mime_type, voice_bytes)
    if input_fmt:
        cmd.extend(["-f", input_fmt])
    cmd.extend(["-i", "pipe:0", "-ac", "1", "-ar", "16000", "-f", "wav", "pipe:1"])

    try:
        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdin=asyncio.subprocess.PIPE,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        stdout, stderr = await process.communicate(input=bytes(voice_bytes))
        if process.returncode != 0:
            logger.warning(
                "Voice transcode failed: mime=%s code=%s err=%s",
                mime_type,
                process.returncode,
                (stderr or b"")[:200].decode("utf-8", errors="ignore"),
            )
            return None
        if stdout:
            return stdout
    except FileNotFoundError:
        logger.warning("Voice transcode skipped: ffmpeg not found")
    except Exception as exc:
        logger.warning("Voice transcode failed: mime=%s err=%s", mime_type, exc)
    return None


def _build_audio_contents(
    prompt: str, voice_bytes: bytes, mime_type: str
) -> list[dict]:
    return [
        {
            "role": "user",
            "parts": [
                {"text": prompt},
                {
                    "inline_data": {
                        "mime_type": mime_type,
                        "data": base64.b64encode(bytes(voice_bytes)).decode("utf-8"),
                    }
                },
            ],
        }
    ]


async def _run_audio_prompt(prompt: str, voice_bytes: bytes, mime_type: str) -> str:
    last_error: Exception | None = None
    client: Any = openai_async_client
    if client is None:
        logger.error("Voice model call skipped: OpenAI async client is not initialized")
        return ""

    attempts: list[tuple[str, bytes, str]] = []
    transcoded_wav = await _transcode_audio_to_wav(voice_bytes, mime_type)
    if transcoded_wav:
        attempts.append(("audio/wav", transcoded_wav, "transcoded_wav"))
    for candidate_mime in _audio_mime_candidates(mime_type):
        attempts.append((candidate_mime, voice_bytes, "raw"))

    for candidate_mime, candidate_bytes, source in attempts:
        try:
            response = await cast(Any, client).chat.completions.create(
                model=VOICE_MODEL,
                messages=build_messages(
                    contents=_build_audio_contents(
                        prompt, candidate_bytes, candidate_mime
                    ),
                ),
            )
        except Exception as exc:
            last_error = exc
            logger.warning(
                "Voice model call failed with model=%s mime=%s source=%s err=%s",
                VOICE_MODEL,
                candidate_mime,
                source,
                exc,
            )
            continue

        text = _extract_model_text(response)
        if text and _looks_like_audio_missing_reply(text):
            logger.info(
                "Voice model could not consume audio: model=%s mime=%s",
                VOICE_MODEL,
                candidate_mime,
            )
            continue

        normalized = _normalize_transcribed_text(text)
        if text and not normalized:
            logger.info("Voice model returned only quotes, retrying...")
            continue

        if text:
            return text

    if last_error is not None:
        logger.error("Voice model call failed after mime retries: %s", last_error)
    else:
        logger.warning(
            "Voice model returned empty transcript after %s attempts", len(attempts)
        )
    return ""


async def transcribe_voice(voice_bytes: bytes, mime_type: str) -> str | None:
    """
    ä½¿ç”¨å¯¹è¯æ¨¡å‹è½¬å†™è¯­éŸ³ä¸ºæ–‡å­—

    Returns:
        è½¬å†™åçš„æ–‡æœ¬ï¼Œå¤±è´¥è¿”å› None
    """
    if not voice_bytes:
        logger.warning("Voice transcription skipped: empty audio payload.")
        return None

    try:
        prompt = (
            "è¯·å°†è¿™æ®µè¯­éŸ³è½¬å†™ä¸ºæ–‡å­—ã€‚"
            "åªè¾“å‡ºè¯­éŸ³ä¸­è¯´çš„åŸè¯ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–å›å¤ã€‚"
            "å¦‚æœæ— æ³•è¯†åˆ«ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²ã€‚"
        )
        text = _normalize_transcribed_text(
            await _run_audio_prompt(prompt, voice_bytes, mime_type)
        )
        if text:
            return text
        return None
    except Exception as e:
        logger.error(f"Voice transcription error: {e}")
        return None


async def transcribe_and_translate_voice(
    voice_bytes: bytes, mime_type: str
) -> dict | None:
    """
    è½¬å†™è¯­éŸ³å¹¶ç¿»è¯‘ä¸ºåŒè¯­å¯¹ç…§

    Returns:
        {"original": "åŸæ–‡", "original_lang": "è¯­è¨€", "translated": "è¯‘æ–‡"} æˆ– None
    """
    if not voice_bytes:
        logger.warning("Voice translation skipped: empty audio payload.")
        return None

    try:
        prompt = (
            "è¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š\n"
            "1. å°†è¯­éŸ³è½¬å†™ä¸ºæ–‡å­—\n"
            "2. è¯†åˆ«è¯­éŸ³çš„è¯­è¨€\n"
            "3. å¦‚æœæ˜¯ä¸­æ–‡ï¼Œç¿»è¯‘ä¸ºè‹±æ–‡ï¼›å¦‚æœæ˜¯å…¶ä»–è¯­è¨€ï¼Œç¿»è¯‘ä¸ºä¸­æ–‡\n\n"
            "è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼ˆä¸è¦æ·»åŠ å…¶ä»–å†…å®¹ï¼‰ï¼š\n"
            "åŸæ–‡è¯­è¨€ï¼š[è¯­è¨€åç§°]\n"
            "åŸæ–‡ï¼š[è½¬å†™çš„åŸæ–‡]\n"
            "è¯‘æ–‡ï¼š[ç¿»è¯‘åçš„æ–‡å­—]"
        )

        text = await _run_audio_prompt(prompt, voice_bytes, mime_type)
        if not text:
            return None

        # è§£æç»“æœ
        text = text.strip()
        result = {}

        for line in text.split("\n"):
            if line.startswith("åŸæ–‡è¯­è¨€ï¼š"):
                result["original_lang"] = line.replace("åŸæ–‡è¯­è¨€ï¼š", "").strip()
            elif line.startswith("åŸæ–‡ï¼š"):
                result["original"] = line.replace("åŸæ–‡ï¼š", "").strip()
            elif line.startswith("è¯‘æ–‡ï¼š"):
                result["translated"] = line.replace("è¯‘æ–‡ï¼š", "").strip()

        if result.get("original") and result.get("translated"):
            return result
        return None

    except Exception as e:
        logger.error(f"Voice translation error: {e}")
        return None


async def handle_voice_message(ctx: UnifiedContext) -> None:
    from core.state_store import get_user_settings

    user_id = ctx.message.user.id

    # æ£€æŸ¥ç”¨æˆ·æƒé™
    if not await is_user_allowed(user_id):
        await ctx.reply("â›” æŠ±æ­‰ï¼Œæ‚¨æ²¡æœ‰ä½¿ç”¨ AI åŠŸèƒ½çš„æƒé™ã€‚")
        return

    try:
        media = await extract_media_input(
            ctx,
            expected_types={MessageType.VOICE, MessageType.AUDIO},
            auto_download=True,
        )
    except MediaProcessingError as exc:
        if exc.error_code == "unsupported_media_on_platform":
            await ctx.reply("âŒ å½“å‰å¹³å°æš‚ä¸æ”¯æŒè¯¥è¯­éŸ³/éŸ³é¢‘æ ¼å¼ã€‚")
        else:
            await ctx.reply("âŒ å½“å‰å¹³å°æš‚æ—¶æ— æ³•ä¸‹è½½è¯­éŸ³/éŸ³é¢‘å†…å®¹ï¼Œè¯·ç¨åå†è¯•ã€‚")
        return

    mime_type = media.mime_type or "audio/ogg"
    duration = int(media.meta.get("duration") or 0)
    user_instruction = (
        media.caption.strip()
        if media.caption
        else (ctx.message.text or "").strip() or None
    )

    # æ£€æŸ¥æ˜¯å¦å¼€å¯ç¿»è¯‘æ¨¡å¼
    settings = await get_user_settings(user_id)
    translate_mode = settings.get("auto_translate", 0)

    # å‘é€å¤„ç†ä¸­æç¤º
    if translate_mode:
        thinking_msg = await ctx.reply("ğŸŒ æ­£åœ¨ç¿»è¯‘è¯­éŸ³å†…å®¹...")
    else:
        thinking_msg = await ctx.reply("ğŸ¤ æ­£åœ¨ç†è§£è¯­éŸ³å†…å®¹...")

    # å‘é€"æ­£åœ¨è¾“å…¥"çŠ¶æ€
    await ctx.send_chat_action(action="typing")

    try:
        logger.info("Voice payload loaded: mime=%s duration=%s", mime_type, duration)
        voice_bytes = media.content or b""
        if not voice_bytes:
            msg_id = getattr(
                thinking_msg, "message_id", getattr(thinking_msg, "id", None)
            )
            await ctx.edit_message(msg_id, "âŒ æœªèƒ½è¯»å–è¯­éŸ³æ•°æ®ï¼Œè¯·é‡è¯•ã€‚")
            return

        # ç¿»è¯‘æ¨¡å¼ï¼šåŒè¯­å¯¹ç…§è¾“å‡º
        if translate_mode:
            result = await transcribe_and_translate_voice(voice_bytes, mime_type)

            if not result:
                msg_id = getattr(
                    thinking_msg, "message_id", getattr(thinking_msg, "id", None)
                )
                await ctx.edit_message(msg_id, "âŒ æ— æ³•è¯†åˆ«æˆ–ç¿»è¯‘è¯­éŸ³å†…å®¹ï¼Œè¯·é‡è¯•ã€‚")
                return

            original_lang = result.get("original_lang", "æœªçŸ¥")
            original = result.get("original", "")
            translated = result.get("translated", "")

            output = (
                f"ğŸ¤ **è¯­éŸ³ç¿»è¯‘**\n\n"
                f"ğŸ“ **åŸæ–‡** ({original_lang}):\n"
                f"ã€Œ{original}ã€\n\n"
                f"ğŸŒ **è¯‘æ–‡**:\n"
                f"ã€Œ{translated}ã€"
            )

            msg_id = getattr(
                thinking_msg, "message_id", getattr(thinking_msg, "id", None)
            )
            await ctx.edit_message(msg_id, output)

            # è®°å½•ç»Ÿè®¡
            from stats import increment_stat

            await increment_stat(user_id, "translations_count")
            return

        await process_as_voice_message(
            ctx=ctx,
            voice_bytes=voice_bytes,
            mime_type=mime_type,
            user_instruction=user_instruction,
            thinking_msg=thinking_msg,
        )

    except BadRequest as e:
        msg_id = getattr(thinking_msg, "message_id", getattr(thinking_msg, "id", None))
        if "File is too big" in str(e):
            await ctx.edit_message(
                msg_id,
                "âš ï¸ **éŸ³é¢‘æ–‡ä»¶è¿‡å¤§**\n\n"
                "æŠ±æ­‰ï¼ŒTelegram é™åˆ¶ Bot åªèƒ½ä¸‹è½½ **20MB** ä»¥å†…çš„æ–‡ä»¶ï¼Œæˆ‘æ— æ³•è·å–è¿™æ®µéŸ³é¢‘ã€‚\n\n"
                "ğŸ’¡ **å»ºè®®æ–¹æ¡ˆ**ï¼š\n"
                "1. ä½¿ç”¨éŸ³é¢‘å‹ç¼©è½¯ä»¶å‡å°ä½“ç§¯åé‡å‘\n"
                "2. è¿™æ˜¯ä¸€ä¸ª Telegram å®˜æ–¹é™åˆ¶ï¼Œæ— æ³•åœ¨æœåŠ¡ç«¯åˆ‡å‰²ï¼ˆå› ä¸ºæ ¹æœ¬ä¸‹è½½ä¸åˆ°ï¼‰",
            )
        else:
            logger.error(f"Voice processing BadRequest: {e}")
            await ctx.edit_message(msg_id, "âŒ å¤„ç†å¤±è´¥ï¼šæ–‡ä»¶æ ¼å¼æˆ–å†…å®¹å—é™ã€‚")

    except Exception as e:
        logger.error(f"Voice processing error: {e}")
        try:
            msg_id = getattr(
                thinking_msg, "message_id", getattr(thinking_msg, "id", None)
            )
            await ctx.edit_message(
                msg_id,
                "âŒ è¯­éŸ³å¤„ç†å¤±è´¥ï¼Œè¯·ç¨åå†è¯•ã€‚\n\n"
                "å¯èƒ½çš„åŸå› ï¼š\n"
                "â€¢ è¯­éŸ³æ ¼å¼ä¸æ”¯æŒ\n"
                "â€¢ è¯­éŸ³å†…å®¹æ— æ³•è¯†åˆ«\n"
                "â€¢ æœåŠ¡æš‚æ—¶ä¸å¯ç”¨",
            )
        except BadRequest:
            pass


def _looks_like_audio_missing_reply(text: str) -> bool:
    lowered = str(text or "").strip().lower()
    if not lowered:
        return False
    if ("æ²¡æœ‰æ”¶åˆ°" in lowered or "æœªæ”¶åˆ°" in lowered) and (
        "è¯­éŸ³" in lowered or "éŸ³é¢‘" in lowered
    ):
        return True
    cues = (
        "æ²¡æœ‰é™„ä¸Šè¯­éŸ³",
        "æ²¡æœ‰é™„ä¸ŠéŸ³é¢‘",
        "è¯­éŸ³å†…å®¹/éŸ³é¢‘æ–‡ä»¶",
        "éŸ³é¢‘æ–‡ä»¶",
        "æ— æ³•å¬åˆ°",
        "è¯·ä¸Šä¼ è¯­éŸ³",
        "è¯­éŸ³æ–‡ä»¶/è¯­éŸ³é“¾æ¥",
        "no audio",
        "no voice",
        "voice file",
        "audio file",
        "upload audio",
        "attach audio",
    )
    return any(cue in lowered for cue in cues)


async def process_as_voice_message(
    ctx: UnifiedContext,
    voice_bytes: bytes,
    mime_type: str,
    user_instruction: str | None,
    thinking_msg: Any,
) -> None:
    """
    è¯­éŸ³æ¶ˆæ¯å¤„ç†ï¼šå…ˆè½¬å†™ä¸ºæ–‡å­—ï¼Œå†ä½œä¸ºæ–‡æœ¬æ¶ˆæ¯èµ° Agent å¤„ç†ã€‚

    ä¸å†å°è¯•å¤šæ¨¡æ€ç›´æ¥ç†è§£éŸ³é¢‘ï¼ˆä¸ç¨³å®šï¼Œå¸¸è¿”å›"æ²¡æ”¶åˆ°éŸ³é¢‘"å¯¼è‡´ä½“éªŒå·®ï¼‰ã€‚
    """
    msg_id = getattr(thinking_msg, "message_id", getattr(thinking_msg, "id", None))

    # â”€â”€ Step 1: è½¬å†™è¯­éŸ³ â”€â”€
    transcribed_text = await transcribe_voice(voice_bytes, mime_type)
    if not transcribed_text:
        await ctx.edit_message(msg_id, "âŒ æ— æ³•è¯†åˆ«è¯­éŸ³å†…å®¹ï¼Œè¯·é‡è¯•æˆ–æ”¹ç”¨æ–‡å­—å‘é€ã€‚")
        return

    # â”€â”€ Step 2: ç»„è£…æ–‡æœ¬ â”€â”€
    instruction = str(user_instruction or "").strip()
    if instruction:
        final_text = f"{instruction}\n\nã€è¯­éŸ³å†…å®¹ã€‘ï¼š\n{transcribed_text}"
    else:
        final_text = transcribed_text

    # â”€â”€ Step 3: èµ°æ–‡æœ¬æ¶ˆæ¯å¤„ç† â”€â”€
    await ctx.edit_message(
        msg_id,
        f"ğŸ¤ è¯­éŸ³å·²è¯†åˆ«ï¼Œæ­£åœ¨å¤„ç†...\n\n> {transcribed_text[:100]}{'...' if len(transcribed_text) > 100 else ''}",
    )
    await process_as_text_message(ctx, final_text, thinking_msg)


async def process_as_text_message(ctx: UnifiedContext, text: str, thinking_msg) -> None:
    """
    å°†è½¬å†™åçš„æ–‡æœ¬æŒ‰æ™®é€šæ–‡æœ¬æ¶ˆæ¯é€»è¾‘å¤„ç†ï¼ˆä»£ç†ç»™ Agent Orchestratorï¼‰
    """
    import time
    from core.agent_orchestrator import agent_orchestrator
    from stats import increment_stat

    user_id = ctx.message.user.id

    # è®°å½•ç”¨æˆ·æ¶ˆæ¯åˆ°ä¸Šä¸‹æ–‡
    await add_message(ctx, user_id, "user", text)

    # æ„å»ºä¸Šä¸‹æ–‡
    context_messages = await get_user_context(ctx, user_id)
    context_messages.append({"role": "user", "parts": [{"text": text}]})

    msg_id = getattr(thinking_msg, "message_id", getattr(thinking_msg, "id", None))

    # ä»£ç†ç»™ Agent Orchestrator
    try:
        final_text_response = ""
        last_update_time = 0

        async for chunk_text in agent_orchestrator.handle_message(
            ctx, context_messages
        ):
            final_text_response += chunk_text

            now = time.time()
            if now - last_update_time > 0.8:
                await ctx.edit_message(msg_id, final_text_response)
                last_update_time = now

        # å‘é€æœ€ç»ˆå›å¤
        if final_text_response:
            await ctx.edit_message(msg_id, final_text_response)
            await add_message(ctx, user_id, "model", final_text_response)
            await increment_stat(user_id, "voice_chats")
        else:
            await ctx.edit_message(msg_id, "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚")

    except Exception as e:
        logger.error(f"Voice Agent error: {e}")
        await ctx.edit_message(msg_id, f"âŒ Agent è¿è¡Œå‡ºé”™ï¼š{e}")
