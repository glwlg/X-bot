"""
ç½‘é¡µæ‘˜è¦æ¨¡å— - æå–ç½‘é¡µå†…å®¹å¹¶ä½¿ç”¨ AI ç”Ÿæˆæ‘˜è¦
"""
import re
import json
import logging
import asyncio
import os
import httpx
from bs4 import BeautifulSoup

from core.config import gemini_client, GEMINI_MODEL, COOKIES_FILE

logger = logging.getLogger(__name__)

# URL æ­£åˆ™è¡¨è¾¾å¼
URL_PATTERN = re.compile(
    r'https?://[^\s<>"{}|\\^`\[\]]+'
)


def extract_urls(text: str) -> list[str]:
    """ä»æ–‡æœ¬ä¸­æå– URL"""
    return URL_PATTERN.findall(text)


# è§†é¢‘å¹³å°åŸŸåæ£€æµ‹
VIDEO_DOMAINS = [
    "youtube.com", "youtu.be",
    "twitter.com", "x.com",
    "instagram.com",
    "tiktok.com",
    "bilibili.com"
]

def is_video_platform(url: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦ä¸ºæ”¯æŒçš„è§†é¢‘å¹³å° URL"""
    return any(domain in url for domain in VIDEO_DOMAINS)


async def fetch_video_metadata(url: str) -> str | None:
    """ä½¿ç”¨ yt-dlp è·å–è§†é¢‘/å¸–å­å…ƒæ•°æ®"""
    try:
        # æ£€æŸ¥ cookies æ–‡ä»¶
        cookies_arg = []
        if os.path.exists(COOKIES_FILE):
             cookies_arg = ["--cookies", COOKIES_FILE]

        # ä½¿ç”¨ yt-dlp è·å– JSON å…ƒæ•°æ® (ä¸ä¸‹è½½)
        command = [
            "yt-dlp",
            "--dump-json",
            "--skip-download",
            "--no-warnings",
            "--no-playlist",
        ] + cookies_arg + [
            # ä¸ºäº†é˜²æ­¢è¢« X/Twitter é™åˆ¶ï¼Œå°è¯•ä½¿ç”¨ cookies-from-browser æˆ–è€…ç®€å•çš„ UA ä¼ªè£…
            # è¿™é‡Œæš‚æ—¶åªä¾èµ– yt-dlp å†…ç½®çš„åçˆ¬èƒ½åŠ›
            url
        ]
        
        proc = await asyncio.create_subprocess_exec(
            *command,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        
        stdout, stderr = await proc.communicate()
        
        if proc.returncode != 0:
            logger.warning(f"yt-dlp metadata fetch failed for {url}: {stderr.decode()}")
            return None
            
        data = json.loads(stdout.decode())
        
        title = data.get("title", "")
        description = data.get("description", "")
        uploader = data.get("uploader", "")
        
        # é’ˆå¯¹ X/Twitter ç‰¹åˆ«å¤„ç†ï¼šdescription é€šå¸¸å°±æ˜¯æ¨æ–‡å†…å®¹
        content = f"å¹³å°ï¼š{data.get('extractor_key', 'Unknown')}\n"
        content += f"å‘å¸ƒè€…ï¼š{uploader}\n"
        content += f"æ ‡é¢˜/å†…å®¹ï¼š{title}\n"
        if description and description != title:
            content += f"è¯¦ç»†æè¿°ï¼š\n{description}\n"
            
        return content
        
    except Exception as e:
        logger.error(f"Error fetching video metadata: {e}")
        return None


async def fetch_fina_news(url: str) -> str | None:
    """Special handler for fina.ifnet.top API"""
    try:
        api_url = "https://fina.ifnet.top/api/news"
        headers = {
            "accept": "*/*",
            "accept-language": "zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7",
            "priority": "u=1, i",
            "referer": "https://fina.ifnet.top/"
        }
        
        async with httpx.AsyncClient(timeout=10.0) as client:
            # Although the user gave a curl command which usually implies GET, 
            # API endpoints like /api/news could be GET or POST. 
            # curl defaults to GET unless -d is used. The user provided `curl 'url' ...` which is GET.
            # But query params? The user didn't provide any in the curl snippet.
            # Let's assume it fetches latest news.
            
            # ä¿®æ­£ï¼šç”¨æˆ·æä¾›çš„ URL æ˜¯ https://fina.ifnet.top/api/news
            # ä½†ä¼ å…¥ fetch_webpage_content çš„ url å¯èƒ½æ˜¯ https://fina.ifnet.top/ (é¦–é¡µ)
            # æ‰€ä»¥æˆ‘ä»¬åº”è¯¥æ— è§†ä¼ å…¥çš„å…·ä½“ pathï¼Œç›´æ¥è¯·æ±‚ API è·å–æœ€æ–°èµ„è®¯ï¼Ÿ
            # æˆ–è€…å¦‚æœç”¨æˆ·ç»™çš„æ˜¯å…·ä½“æ–‡ç« é¡µï¼Œæˆ‘ä»¬éœ€è¦è§£æ IDï¼Ÿ
            # ç®€å•èµ·è§ï¼Œå¦‚æœ domain åŒ¹é…ï¼Œç›´æ¥æŠ“å– API çš„ Top News ä½œä¸º "å½“å‰ç½‘é¡µå†…å®¹"
            
            response = await client.get(api_url, headers=headers)
            response.raise_for_status()
            
            data = response.json()
            # å‡è®¾è¿”å›çš„æ˜¯åˆ—è¡¨æˆ–åŒ…å«åˆ—è¡¨çš„å­—å…¸
            # éœ€è¦æ ¹æ®å®é™…ç»“æ„è§£æã€‚è¿™é‡Œåšé€šç”¨å¤„ç†ã€‚
            
            # è¿™ç§ API é€šå¸¸è¿”å› JSON åˆ—è¡¨
            items = []
            if isinstance(data, list):
                items = data
            elif isinstance(data, dict):
                items = data.get("data", []) or data.get("list", []) or data.get("items", []) or [data]
                
            if not items:
                return f"API è¿”å›äº†ç©ºæ•°æ®: {data}"
            
            # æ„å»ºç®€åŒ–ç‰ˆ JSON åˆ—è¡¨ï¼ŒèŠ‚çœ Token
            simplified_items = []
            for item in items[:25]: # é™åˆ¶å‰ 25 æ¡
                simplified_items.append({
                    "id": item.get("id"),
                    "title": item.get("title") or "No Title",
                    "content": item.get("content", ""),
                    "time": item.get("time") or item.get("created_at"),
                    "source": item.get("source")
                })
            
            # è¿”å› JSON å­—ç¬¦ä¸²ï¼Œä¾› AI è¿›è¡Œçµæ´»å¤„ç†
            return json.dumps(simplified_items, ensure_ascii=False, indent=2)

    except Exception as e:
        logger.error(f"Error fetching fina news: {e}")
        return None


# åŸŸåç‰¹å®šå¤„ç†å™¨æ³¨å†Œè¡¨
DOMAIN_HANDLERS = {
    "fina.ifnet.top": fetch_fina_news
}


async def fetch_with_browser_snapshot(url: str) -> str | None:
    """
    ä½¿ç”¨ MCP Playwright è·å–ç½‘é¡µå¿«ç…§ï¼ˆå›é€€æœºåˆ¶ï¼‰
    
    ç”¨äºå¤„ç†é™æ€æŠ“å–å¤±è´¥æˆ–éœ€è¦ JS æ¸²æŸ“çš„é¡µé¢ã€‚
    """
    logger.info(f"Attempting to fetch {url} using MCP browser_snapshot...")
    try:
        # åŠ¨æ€å¯¼å…¥é¿å…å¾ªç¯å¼•ç”¨
        from mcp_client.manager import mcp_manager
        from mcp_client.playwright import register_playwright_server
        
        # ç¡®ä¿æœåŠ¡å·²æ³¨å†Œ
        register_playwright_server()
        
        # æ­¥éª¤1ï¼šå¯¼èˆª
        logger.info(f"MCP: Navigating to {url}...")
        await mcp_manager.call_tool("playwright", "browser_navigate", {"url": url})
        
        # æ­¥éª¤2ï¼šç­‰å¾…åŠ è½½
        logger.info("MCP: Waiting for page load...")
        try:
            await mcp_manager.call_tool("playwright", "browser_wait_for", {"time": 3})
        except Exception as e:
            logger.warning(f"MCP wait failed: {e}")
            
        # æ­¥éª¤3ï¼šè·å–å¿«ç…§
        logger.info("MCP: Taking snapshot...")
        result = await mcp_manager.call_tool("playwright", "browser_snapshot", {})
        
        # è§£æç»“æœ
        # browser_snapshot é€šå¸¸è¿”å› TextContent
        content = ""
        if isinstance(result, list):
            for item in result:
                if hasattr(item, 'text'):
                    content += item.text + "\n"
        elif hasattr(result, 'text'):
            content = result.text
            
        if content:
            logger.info(f"MCP snapshot successful, length: {len(content)}")
            return f"ã€é€šè¿‡ Playwright è·å–çš„é¡µé¢å¿«ç…§ã€‘\n\n{content}"
            
        return None
        
    except Exception as e:
        logger.error(f"MCP browser_snapshot failed: {e}")
        return None


async def fetch_webpage_content(url: str) -> str | None:
    """
    è·å–ç½‘é¡µå†…å®¹
    
    Args:
        url: ç½‘é¡µ URL
        
    Returns:
        ç½‘é¡µæ–‡æœ¬å†…å®¹ï¼Œå¦‚æœå¤±è´¥è¿”å› None
    """
    # -----------------------------------------------------------------
    # ç­–ç•¥å‡çº§ï¼šåŸŸåç‰¹å®šè·¯ç”± (Domain Specific Routers)
    # -----------------------------------------------------------------
    for domain, handler in DOMAIN_HANDLERS.items():
        if domain in url:
            logger.info(f"Using custom handler for domain: {domain}")
            return await handler(url)

    # -----------------------------------------------------------------
    # ç­–ç•¥å‡çº§ï¼šå¦‚æœæ˜¯ Google News é“¾æ¥ï¼Œå…ˆå°è¯•è§£ç è¿˜åŸçœŸå® URL
    # -----------------------------------------------------------------
    if "news.google.com" in url or "google.com/news" in url:
        try:
            logger.info(f"Detected Google News URL, decoding with googlenewsdecoder: {url}")
            from googlenewsdecoder import gnewsdecoder
            # gnewsdecoder æ˜¯åŒæ­¥å‡½æ•°ï¼ŒåŒ…è£¹åœ¨ executor ä¸­è¿è¡Œä»¥å…é˜»å¡
            def decode_func():
                return gnewsdecoder(url, interval=1)
            
            decoded_result = await asyncio.to_thread(decode_func)
            
            if decoded_result.get("status"):
                real_url = decoded_result.get("decoded_url")
                if real_url:
                    logger.info(f"Successfully decoded Google News URL: {url} -> {real_url}")
                    url = real_url
            else:
                logger.warning(f"Google News decoding failed: {decoded_result.get('message')}")
        except Exception as e:
            logger.error(f"Error decoding Google News URL: {e}")

    # -----------------------------------------------------------------
    # ç­–ç•¥å‡çº§ï¼šå¦‚æœæ˜¯è§†é¢‘å¹³å°ï¼Œä¼˜å…ˆå°è¯•ä½¿ç”¨ yt-dlp è·å–å…ƒæ•°æ®
    # è¿™èƒ½è§£å†³ X (Twitter) ç­‰å‰ç«¯æ¸²æŸ“é¡µé¢çš„æŠ“å–é—®é¢˜
    # -----------------------------------------------------------------
    if is_video_platform(url):
        logger.info(f"Detected video platform URL, trying yt-dlp extraction: {url}")
        video_content = await fetch_video_metadata(url)
        if video_content:
             return f"ã€ä»è§†é¢‘å¹³å°æå–çš„å…ƒæ•°æ®ã€‘\n{video_content}"
        logger.info("yt-dlp extraction failed, falling back to standard scraping.")

    try:
        async with httpx.AsyncClient(timeout=15.0, follow_redirects=True) as client:
            response = await client.get(url, headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            })
            response.raise_for_status()
            
            # è§£æ HTML
            soup = BeautifulSoup(response.text, "html.parser")
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            for script in soup(["script", "style", "nav", "footer", "header"]):
                script.decompose()
            
            # è·å–æ ‡é¢˜
            title = soup.title.string if soup.title else ""
            
            # è·å–æ­£æ–‡å†…å®¹
            # ä¼˜å…ˆå°è¯• article æ ‡ç­¾
            article = soup.find("article")
            if article:
                text = article.get_text(separator="\n", strip=True)
            else:
                # å¦åˆ™è·å– body å†…å®¹
                body = soup.find("body")
                if body:
                    text = body.get_text(separator="\n", strip=True)
                else:
                    text = soup.get_text(separator="\n", strip=True)
            
            # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼ˆé¿å… token è¶…é™ï¼‰
            max_length = 8000
            if len(text) > max_length:
                text = text[:max_length] + "..."

            # -----------------------------------------------------------------
            # å¢åŠ ç½‘é¡µæœ‰æ•ˆæ€§æ ¡éªŒ (é˜²æ­¢ AI æ€»ç»“é”™è¯¯é¡µé¢)
            # -----------------------------------------------------------------
            
            # 2. æ£€æŸ¥å¸¸è§é”™è¯¯å…³é”®å­— (JavaScript, Error page, etc)
            error_keywords = [
                "JavaScript is disabled",
                "enable JavaScript",
                "browser is not supported",
                "Something went wrong",
                "Please wait...",
                "Just a moment...",
                "Checking your browser",
                "403 Forbidden",
                "404 Not Found",
                "Access Denied",
                "JavaScript å·²ç»è¢«ç¦ç”¨",
                "è¯·å¯ç”¨ JavaScript",
                "Google News", # Google News interstitial page title often contains this
            ]
            
            # æ£€æŸ¥å‰ 500 ä¸ªå­—ç¬¦å³å¯ (é€šå¸¸é”™è¯¯æç¤ºåœ¨æœ€å‰é¢)
            preview_text = text[:500].lower()
            needs_fallback = False
            
            for ignored in error_keywords:
                if ignored.lower() in preview_text:
                    logger.warning(f"Detected invalid content ('{ignored}') for {url}")
                    needs_fallback = True
                    break
            
            # 1. æ£€æŸ¥æ–‡æœ¬é•¿åº¦ï¼Œå¤ªçŸ­ä¹Ÿè§†ä¸ºæ— æ•ˆ
            if len(text.strip()) < 50:
                logger.warning(f"Extracted content too short ({len(text)} chars) for {url}.")
                needs_fallback = True

            if needs_fallback:
                # ç­–ç•¥å‡çº§ï¼šä½¿ç”¨ MCP Browser Snapshot è¿›è¡Œå›é€€
                logger.info(f"Falling back to MCP browser_snapshot for {url}")
                snapshot_content = await fetch_with_browser_snapshot(url)
                if snapshot_content:
                    return snapshot_content
                
                # å¦‚æœ MCP ä¹Ÿå¤±è´¥ï¼Œå°è¯• yt-dlp å…œåº•
                logger.info(f"MCP fallback failed, trying yt-dlp for {url}")
                video_content = await fetch_video_metadata(url)
                if video_content:
                    return f"ã€é€šè¿‡å·¥å…·æå–çš„å…ƒæ•°æ®ã€‘\n{video_content}"
                return None
            
            return f"æ ‡é¢˜ï¼š{title}\n\nå†…å®¹ï¼š\n{text}"
            
    except Exception as e:
        logger.error(f"Failed to fetch webpage: {e}")
        
        # å‡ºé”™æ—¶ä¼˜å…ˆå°è¯• MCP Browser Snapshot
        try:
             logger.info(f"Exception occurred, trying MCP browser_snapshot for {url}")
             snapshot_content = await fetch_with_browser_snapshot(url)
             if snapshot_content:
                 return snapshot_content
        except Exception as mcp_e:
             logger.error(f"MCP fallback also failed: {mcp_e}")

        # æœ€åå°è¯• yt-dlp
        try:
             video_content = await fetch_video_metadata(url)
             if video_content:
                 return f"ã€é€šè¿‡å·¥å…·æå–çš„å…ƒæ•°æ®ã€‘\n{video_content}"
        except:
             pass
        return None


async def summarize_webpage(url: str) -> str:
    """
    è·å–ç½‘é¡µå¹¶ç”Ÿæˆæ‘˜è¦
    
    Args:
        url: ç½‘é¡µ URL
        
    Returns:
        æ‘˜è¦æ–‡æœ¬
    """
    # è·å–ç½‘é¡µå†…å®¹
    content = await fetch_webpage_content(url)
    if not content:
        return f"âŒ æ— æ³•è·å–ç½‘é¡µå†…å®¹ï¼š{url}"
    
    try:
        # å®šåˆ¶åŒ– Promptï¼šé’ˆå¯¹ Fina è´¢ç»å¿«è®¯ï¼Œä½¿ç”¨åˆ—è¡¨æ ¼å¼è€Œéæ‘˜è¦
        if "fina.ifnet.top" in url:
             prompt = f"ä»¥ä¸‹æ˜¯è·å–åˆ°çš„å®æ—¶è´¢ç»å¿«è®¯æ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰ã€‚è¯·å°†å…¶æ•´ç†ä¸ºä¸€ä»½æ¸…æ™°çš„**æ–°é—»åˆ—è¡¨**å‘é€ç»™ç”¨æˆ·ã€‚\n\næ•°æ®å†…å®¹ï¼š\n{content}"
             system_instruction = (
                 "ä½ æ˜¯ä¸€ä¸ªè´¢ç»èµ„è®¯åŠ©æ‰‹ã€‚\n"
                 "ç”¨æˆ·æä¾›äº†ä¸€ç»„ JSON æ ¼å¼çš„æ–°é—»æ•°æ®ã€‚\n"
                 "è¯·ç›´æ¥å°†å…¶æ•´ç†ä¸ºç¼–å·åˆ—è¡¨ï¼Œæ¯æ¡æ–°é—»åŒ…å«ï¼š\n"
                 "1. æ ‡é¢˜/å†…å®¹æ ¸å¿ƒ (å»é™¤ HTML æ ‡ç­¾)\n"
                 "2. æ—¶é—´ (å¦‚æœæœ‰)\n"
                 "3. æ¥æº (å¦‚æœæœ‰)\n"
                 "ä¸éœ€è¦è¿›è¡Œé€šè¿‡æ€§çš„æ€»ç»“ï¼Œåªéœ€è¦æ¸…æ™°å±•ç¤ºåˆ—è¡¨ã€‚ä¿ç•™å‰ 10-15 æ¡æœ€é‡è¦çš„å³å¯ã€‚"
             )
        else:
             prompt = f"è¯·ä¸ºä»¥ä¸‹ç½‘é¡µå†…å®¹ç”Ÿæˆç®€æ´çš„ä¸­æ–‡æ‘˜è¦ï¼š\n\n{content}"
             system_instruction = (
                    "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹æ‘˜è¦åŠ©æ‰‹ã€‚"
                    "è¯·ç”Ÿæˆç®€æ´ã€å‡†ç¡®çš„ä¸­æ–‡æ‘˜è¦ï¼ŒåŒ…å«ä»¥ä¸‹è¦ç‚¹ï¼š\n"
                    "1. ä¸»é¢˜æ˜¯ä»€ä¹ˆ\n"
                    "2. ä¸»è¦è§‚ç‚¹æˆ–å†…å®¹\n"
                    "3. å…³é”®ä¿¡æ¯\n"
                    "æ‘˜è¦åº”è¯¥ç®€æ´æ˜äº†ï¼Œä¸€èˆ¬ä¸è¶…è¿‡ 200 å­—ã€‚"
                )

        # ä½¿ç”¨ Gemini ç”Ÿæˆæ‘˜è¦
        response = gemini_client.models.generate_content(
            model=GEMINI_MODEL,
            contents=prompt,
            config={
                "system_instruction": system_instruction,
            },
        )
        
        if response.text:
            return f"ğŸ“„ **ç½‘é¡µæ‘˜è¦**\n\nğŸ”— {url}\n\n{response.text}"
        else:
            return f"âŒ æ— æ³•ç”Ÿæˆæ‘˜è¦ï¼š{url}"
            
    except Exception as e:
        logger.error(f"Failed to summarize webpage: {e}")
        return f"âŒ æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼š{url}"
