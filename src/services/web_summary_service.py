"""
ç½‘é¡µæ‘˜è¦æ¨¡å— - æå–ç½‘é¡µå†…å®¹å¹¶ä½¿ç”¨ AI ç”Ÿæˆæ‘˜è¦
"""

import re
import json
import logging
import asyncio
import os
import httpx
from bs4 import BeautifulSoup

from core.config import gemini_client, GEMINI_MODEL, COOKIES_FILE

logger = logging.getLogger(__name__)

# URL æ­£åˆ™è¡¨è¾¾å¼
URL_PATTERN = re.compile(r'https?://[^\s<>"{}|\\^`\[\]]+')


def extract_urls(text: str) -> list[str]:
    """ä»æ–‡æœ¬ä¸­æå– URL"""
    return URL_PATTERN.findall(text)


# è§†é¢‘å¹³å°åŸŸåæ£€æµ‹
VIDEO_DOMAINS = [
    "youtube.com",
    "youtu.be",
    "twitter.com",
    "x.com",
    "instagram.com",
    "tiktok.com",
    "bilibili.com",
]


def is_video_platform(url: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦ä¸ºæ”¯æŒçš„è§†é¢‘å¹³å° URL"""
    return any(domain in url for domain in VIDEO_DOMAINS)


async def fetch_video_metadata(url: str) -> str | None:
    """ä½¿ç”¨ yt-dlp è·å–è§†é¢‘/å¸–å­å…ƒæ•°æ®"""
    try:
        # æ£€æŸ¥ cookies æ–‡ä»¶
        cookies_arg = []
        if os.path.exists(COOKIES_FILE):
            cookies_arg = ["--cookies", COOKIES_FILE]

        # ä½¿ç”¨ yt-dlp è·å– JSON å…ƒæ•°æ® (ä¸ä¸‹è½½)
        command = (
            [
                "yt-dlp",
                "--dump-json",
                "--skip-download",
                "--no-warnings",
                "--no-playlist",
            ]
            + cookies_arg
            + [
                # ä¸ºäº†é˜²æ­¢è¢« X/Twitter é™åˆ¶ï¼Œå°è¯•ä½¿ç”¨ cookies-from-browser æˆ–è€…ç®€å•çš„ UA ä¼ªè£…
                # è¿™é‡Œæš‚æ—¶åªä¾èµ– yt-dlp å†…ç½®çš„åçˆ¬èƒ½åŠ›
                url
            ]
        )

        proc = await asyncio.create_subprocess_exec(
            *command,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )

        stdout, stderr = await proc.communicate()

        if proc.returncode != 0:
            logger.warning(f"yt-dlp metadata fetch failed for {url}: {stderr.decode()}")
            return None

        data = json.loads(stdout.decode())

        title = data.get("title", "")
        description = data.get("description", "")
        uploader = data.get("uploader", "")

        # é’ˆå¯¹ X/Twitter ç‰¹åˆ«å¤„ç†ï¼šdescription é€šå¸¸å°±æ˜¯æ¨æ–‡å†…å®¹
        content = f"å¹³å°ï¼š{data.get('extractor_key', 'Unknown')}\n"
        content += f"å‘å¸ƒè€…ï¼š{uploader}\n"
        content += f"æ ‡é¢˜/å†…å®¹ï¼š{title}\n"
        if description and description != title:
            content += f"è¯¦ç»†æè¿°ï¼š\n{description}\n"

        return content

    except Exception as e:
        logger.error(f"Error fetching video metadata: {e}")
        return None


async def fetch_with_browser_snapshot(url: str) -> str | None:
    """
    ä½¿ç”¨ MCP Playwright è·å–ç½‘é¡µå¿«ç…§ï¼ˆå›é€€æœºåˆ¶ï¼‰

    ç”¨äºå¤„ç†é™æ€æŠ“å–å¤±è´¥æˆ–éœ€è¦ JS æ¸²æŸ“çš„é¡µé¢ã€‚
    """
    logger.info(f"Attempting to fetch {url} using MCP browser_snapshot...")
    try:
        # åŠ¨æ€å¯¼å…¥é¿å…å¾ªç¯å¼•ç”¨
        from mcp_client.manager import mcp_manager
        from mcp_client.playwright import register_playwright_server

        # ç¡®ä¿æœåŠ¡å·²æ³¨å†Œ
        register_playwright_server()

        # æ­¥éª¤1ï¼šå¯¼èˆª
        logger.info(f"MCP: Navigating to {url}...")
        await mcp_manager.call_tool("playwright", "browser_navigate", {"url": url})

        # æ­¥éª¤2ï¼šç­‰å¾…åŠ è½½
        logger.info("MCP: Waiting for page load...")
        try:
            await mcp_manager.call_tool("playwright", "browser_wait_for", {"time": 3})
        except Exception as e:
            logger.warning(f"MCP wait failed: {e}")

        # æ­¥éª¤3ï¼šè·å–å¿«ç…§
        logger.info("MCP: Taking snapshot...")
        result = await mcp_manager.call_tool("playwright", "browser_snapshot", {})

        # è§£æç»“æœ
        # browser_snapshot é€šå¸¸è¿”å› TextContent
        content = ""
        if isinstance(result, list):
            for item in result:
                if hasattr(item, "text"):
                    content += item.text + "\n"
        elif hasattr(result, "text"):
            content = result.text

        if content:
            logger.info(f"MCP snapshot successful, length: {len(content)}")
            return f"ã€é€šè¿‡ Playwright è·å–çš„é¡µé¢å¿«ç…§ã€‘\n\n{content}"

        return None

    except Exception as e:
        logger.error(f"MCP browser_snapshot failed: {e}")
        return None


async def fetch_webpage_content(url: str) -> str | None:
    """
    è·å–ç½‘é¡µå†…å®¹

    Args:
        url: ç½‘é¡µ URL

    Returns:
        ç½‘é¡µæ–‡æœ¬å†…å®¹ï¼Œå¦‚æœå¤±è´¥è¿”å› None
    """

    # -----------------------------------------------------------------
    # ç­–ç•¥å‡çº§ï¼šå¦‚æœæ˜¯ Google News é“¾æ¥ï¼Œå…ˆå°è¯•è§£ç è¿˜åŸçœŸå® URL
    # -----------------------------------------------------------------
    if "news.google.com" in url or "google.com/news" in url:
        try:
            logger.info(
                f"Detected Google News URL, decoding with googlenewsdecoder: {url}"
            )
            from googlenewsdecoder import gnewsdecoder

            # gnewsdecoder æ˜¯åŒæ­¥å‡½æ•°ï¼ŒåŒ…è£¹åœ¨ executor ä¸­è¿è¡Œä»¥å…é˜»å¡
            def decode_func():
                return gnewsdecoder(url, interval=1)

            decoded_result = await asyncio.to_thread(decode_func)

            if decoded_result.get("status"):
                real_url = decoded_result.get("decoded_url")
                if real_url:
                    logger.info(
                        f"Successfully decoded Google News URL: {url} -> {real_url}"
                    )
                    url = real_url
            else:
                logger.warning(
                    f"Google News decoding failed: {decoded_result.get('message')}"
                )
        except Exception as e:
            logger.error(f"Error decoding Google News URL: {e}")

    # -----------------------------------------------------------------
    # ç­–ç•¥å‡çº§ï¼šå¦‚æœæ˜¯è§†é¢‘å¹³å°ï¼Œä¼˜å…ˆå°è¯•ä½¿ç”¨ yt-dlp è·å–å…ƒæ•°æ®
    # è¿™èƒ½è§£å†³ X (Twitter) ç­‰å‰ç«¯æ¸²æŸ“é¡µé¢çš„æŠ“å–é—®é¢˜
    # -----------------------------------------------------------------
    if is_video_platform(url):
        logger.info(f"Detected video platform URL, trying yt-dlp extraction: {url}")
        video_content = await fetch_video_metadata(url)
        if video_content:
            return f"ã€ä»è§†é¢‘å¹³å°æå–çš„å…ƒæ•°æ®ã€‘\n{video_content}"
        logger.info("yt-dlp extraction failed, falling back to standard scraping.")

    try:
        async with httpx.AsyncClient(timeout=15.0, follow_redirects=True) as client:
            response = await client.get(
                url,
                headers={
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                },
            )
            response.raise_for_status()

            # è§£æ HTML (CPU bound operation, offload to thread)
            def parse_html(html_content):
                soup = BeautifulSoup(html_content, "html.parser")

                # ç§»é™¤è„šæœ¬å’Œæ ·å¼
                for script in soup(["script", "style", "nav", "footer", "header"]):
                    script.decompose()

                # è·å–æ ‡é¢˜
                title = soup.title.string if soup.title else ""

                # è·å–æ­£æ–‡å†…å®¹
                # ä¼˜å…ˆå°è¯• article æ ‡ç­¾
                article = soup.find("article")
                if article:
                    text = article.get_text(separator="\n", strip=True)
                else:
                    # å¦åˆ™è·å– body å†…å®¹
                    body = soup.find("body")
                    if body:
                        text = body.get_text(separator="\n", strip=True)
                    else:
                        text = soup.get_text(separator="\n", strip=True)
                return title, text

            title, text = await asyncio.to_thread(parse_html, response.text)

            # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼ˆé¿å… token è¶…é™ï¼‰
            max_length = 8000
            if len(text) > max_length:
                text = text[:max_length] + "..."

            # -----------------------------------------------------------------
            # å¢åŠ ç½‘é¡µæœ‰æ•ˆæ€§æ ¡éªŒ (é˜²æ­¢ AI æ€»ç»“é”™è¯¯é¡µé¢)
            # -----------------------------------------------------------------

            # 2. æ£€æŸ¥å¸¸è§é”™è¯¯å…³é”®å­— (JavaScript, Error page, etc)
            error_keywords = [
                "JavaScript is disabled",
                "enable JavaScript",
                "browser is not supported",
                "Something went wrong",
                "Please wait...",
                "Just a moment...",
                "Checking your browser",
                "403 Forbidden",
                "404 Not Found",
                "Access Denied",
                "JavaScript å·²ç»è¢«ç¦ç”¨",
                "è¯·å¯ç”¨ JavaScript",
                "Google News",  # Google News interstitial page title often contains this
            ]

            # æ£€æŸ¥å‰ 500 ä¸ªå­—ç¬¦å³å¯ (é€šå¸¸é”™è¯¯æç¤ºåœ¨æœ€å‰é¢)
            preview_text = text[:500].lower()
            needs_fallback = False

            for ignored in error_keywords:
                if ignored.lower() in preview_text:
                    logger.warning(f"Detected invalid content ('{ignored}') for {url}")
                    needs_fallback = True
                    break

            # 1. æ£€æŸ¥æ–‡æœ¬é•¿åº¦ï¼Œå¤ªçŸ­ä¹Ÿè§†ä¸ºæ— æ•ˆ
            if len(text.strip()) < 50:
                logger.warning(
                    f"Extracted content too short ({len(text)} chars) for {url}."
                )
                needs_fallback = True

            if needs_fallback:
                # ç­–ç•¥å‡çº§ï¼šä½¿ç”¨ MCP Browser Snapshot è¿›è¡Œå›é€€
                logger.info(f"Falling back to MCP browser_snapshot for {url}")
                snapshot_content = await fetch_with_browser_snapshot(url)
                if snapshot_content:
                    return snapshot_content

                # å¦‚æœ MCP ä¹Ÿå¤±è´¥ï¼Œå°è¯• yt-dlp å…œåº•
                logger.info(f"MCP fallback failed, trying yt-dlp for {url}")
                video_content = await fetch_video_metadata(url)
                if video_content:
                    return f"ã€é€šè¿‡å·¥å…·æå–çš„å…ƒæ•°æ®ã€‘\n{video_content}"
                return None

            return f"æ ‡é¢˜ï¼š{title}\n\nå†…å®¹ï¼š\n{text}"

    except Exception as e:
        logger.error(f"Failed to fetch webpage: {e}")

        # å‡ºé”™æ—¶ä¼˜å…ˆå°è¯• MCP Browser Snapshot
        try:
            logger.info(f"Exception occurred, trying MCP browser_snapshot for {url}")
            snapshot_content = await fetch_with_browser_snapshot(url)
            if snapshot_content:
                return snapshot_content
        except Exception as mcp_e:
            logger.error(f"MCP fallback also failed: {mcp_e}")

        # æœ€åå°è¯• yt-dlp
        try:
            video_content = await fetch_video_metadata(url)
            if video_content:
                return f"ã€é€šè¿‡å·¥å…·æå–çš„å…ƒæ•°æ®ã€‘\n{video_content}"
        except:
            pass
        return None


async def summarize_webpage(url: str) -> str:
    """
    è·å–ç½‘é¡µå¹¶ç”Ÿæˆæ‘˜è¦

    Args:
        url: ç½‘é¡µ URL

    Returns:
        æ‘˜è¦æ–‡æœ¬
    """
    # è·å–ç½‘é¡µå†…å®¹
    content = await fetch_webpage_content(url)
    if not content:
        return f"âŒ æ— æ³•è·å–ç½‘é¡µå†…å®¹ï¼š{url}"

    try:
        prompt = f"è¯·ä¸ºä»¥ä¸‹ç½‘é¡µå†…å®¹ç”Ÿæˆç®€æ´çš„ä¸­æ–‡æ‘˜è¦ï¼š\n\n{content}"
        system_instruction = (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹æ‘˜è¦åŠ©æ‰‹ã€‚"
            "è¯·ç”Ÿæˆç®€æ´ã€å‡†ç¡®çš„ä¸­æ–‡æ‘˜è¦ï¼ŒåŒ…å«ä»¥ä¸‹è¦ç‚¹ï¼š\n"
            "1. ä¸»é¢˜æ˜¯ä»€ä¹ˆ\n"
            "2. ä¸»è¦è§‚ç‚¹æˆ–å†…å®¹\n"
            "3. å…³é”®ä¿¡æ¯\n"
            "æ‘˜è¦åº”è¯¥ç®€æ´æ˜äº†ï¼Œä¸€èˆ¬ä¸è¶…è¿‡ 200 å­—ã€‚"
        )

        # ä½¿ç”¨ Gemini ç”Ÿæˆæ‘˜è¦
        response = gemini_client.models.generate_content(
            model=GEMINI_MODEL,
            contents=prompt,
            config={
                "system_instruction": system_instruction,
            },
        )

        if response.text:
            return f"ğŸ“„ **ç½‘é¡µæ‘˜è¦**\n\nğŸ”— {url}\n\n{response.text}"
        else:
            return f"âŒ æ— æ³•ç”Ÿæˆæ‘˜è¦ï¼š{url}"

    except Exception as e:
        logger.error(f"Failed to summarize webpage: {e}")
        return f"âŒ æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼š{url}"
