"""
ç½‘é¡µæ‘˜è¦æ¨¡å— - æå–ç½‘é¡µå†…å®¹å¹¶ä½¿ç”¨ AI ç”Ÿæˆæ‘˜è¦
"""
import re
import json
import logging
import asyncio
import os
import httpx
from bs4 import BeautifulSoup

from config import gemini_client, GEMINI_MODEL, COOKIES_FILE

logger = logging.getLogger(__name__)

# URL æ­£åˆ™è¡¨è¾¾å¼
URL_PATTERN = re.compile(
    r'https?://[^\s<>"{}|\\^`\[\]]+'
)


def extract_urls(text: str) -> list[str]:
    """ä»æ–‡æœ¬ä¸­æå– URL"""
    return URL_PATTERN.findall(text)


# è§†é¢‘å¹³å°åŸŸåæ£€æµ‹
VIDEO_DOMAINS = [
    "youtube.com", "youtu.be",
    "twitter.com", "x.com",
    "instagram.com",
    "tiktok.com",
    "bilibili.com"
]

def is_video_platform(url: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦ä¸ºæ”¯æŒçš„è§†é¢‘å¹³å° URL"""
    return any(domain in url for domain in VIDEO_DOMAINS)


async def fetch_video_metadata(url: str) -> str | None:
    """ä½¿ç”¨ yt-dlp è·å–è§†é¢‘/å¸–å­å…ƒæ•°æ®"""
    try:
        # æ£€æŸ¥ cookies æ–‡ä»¶
        cookies_arg = []
        if os.path.exists(COOKIES_FILE):
             cookies_arg = ["--cookies", COOKIES_FILE]

        # ä½¿ç”¨ yt-dlp è·å– JSON å…ƒæ•°æ® (ä¸ä¸‹è½½)
        command = [
            "yt-dlp",
            "--dump-json",
            "--skip-download",
            "--no-warnings",
            "--no-playlist",
        ] + cookies_arg + [
            # ä¸ºäº†é˜²æ­¢è¢« X/Twitter é™åˆ¶ï¼Œå°è¯•ä½¿ç”¨ cookies-from-browser æˆ–è€…ç®€å•çš„ UA ä¼ªè£…
            # è¿™é‡Œæš‚æ—¶åªä¾èµ– yt-dlp å†…ç½®çš„åçˆ¬èƒ½åŠ›
            url
        ]
        
        proc = await asyncio.create_subprocess_exec(
            *command,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        
        stdout, stderr = await proc.communicate()
        
        if proc.returncode != 0:
            logger.warning(f"yt-dlp metadata fetch failed for {url}: {stderr.decode()}")
            return None
            
        data = json.loads(stdout.decode())
        
        title = data.get("title", "")
        description = data.get("description", "")
        uploader = data.get("uploader", "")
        
        # é’ˆå¯¹ X/Twitter ç‰¹åˆ«å¤„ç†ï¼šdescription é€šå¸¸å°±æ˜¯æ¨æ–‡å†…å®¹
        content = f"å¹³å°ï¼š{data.get('extractor_key', 'Unknown')}\n"
        content += f"å‘å¸ƒè€…ï¼š{uploader}\n"
        content += f"æ ‡é¢˜/å†…å®¹ï¼š{title}\n"
        if description and description != title:
            content += f"è¯¦ç»†æè¿°ï¼š\n{description}\n"
            
        return content
        
    except Exception as e:
        logger.error(f"Error fetching video metadata: {e}")
        return None


async def fetch_webpage_content(url: str) -> str | None:
    """
    è·å–ç½‘é¡µå†…å®¹
    
    Args:
        url: ç½‘é¡µ URL
        
    Returns:
        ç½‘é¡µæ–‡æœ¬å†…å®¹ï¼Œå¦‚æœå¤±è´¥è¿”å› None
    """
    # -----------------------------------------------------------------
    # ç­–ç•¥å‡çº§ï¼šå¦‚æœæ˜¯è§†é¢‘å¹³å°ï¼Œä¼˜å…ˆå°è¯•ä½¿ç”¨ yt-dlp è·å–å…ƒæ•°æ®
    # è¿™èƒ½è§£å†³ X (Twitter) ç­‰å‰ç«¯æ¸²æŸ“é¡µé¢çš„æŠ“å–é—®é¢˜
    # -----------------------------------------------------------------
    if is_video_platform(url):
        logger.info(f"Detected video platform URL, trying yt-dlp extraction: {url}")
        video_content = await fetch_video_metadata(url)
        if video_content:
             return f"ã€ä»è§†é¢‘å¹³å°æå–çš„å…ƒæ•°æ®ã€‘\n{video_content}"
        logger.info("yt-dlp extraction failed, falling back to standard scraping.")

    try:
        async with httpx.AsyncClient(timeout=15.0, follow_redirects=True) as client:
            response = await client.get(url, headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            })
            response.raise_for_status()
            
            # è§£æ HTML
            soup = BeautifulSoup(response.text, "html.parser")
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            for script in soup(["script", "style", "nav", "footer", "header"]):
                script.decompose()
            
            # è·å–æ ‡é¢˜
            title = soup.title.string if soup.title else ""
            
            # è·å–æ­£æ–‡å†…å®¹
            # ä¼˜å…ˆå°è¯• article æ ‡ç­¾
            article = soup.find("article")
            if article:
                text = article.get_text(separator="\n", strip=True)
            else:
                # å¦åˆ™è·å– body å†…å®¹
                body = soup.find("body")
                if body:
                    text = body.get_text(separator="\n", strip=True)
                else:
                    text = soup.get_text(separator="\n", strip=True)
            
            # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼ˆé¿å… token è¶…é™ï¼‰
            max_length = 8000
            if len(text) > max_length:
                text = text[:max_length] + "..."

            # -----------------------------------------------------------------
            # å¢åŠ ç½‘é¡µæœ‰æ•ˆæ€§æ ¡éªŒ (é˜²æ­¢ AI æ€»ç»“é”™è¯¯é¡µé¢)
            # -----------------------------------------------------------------
            
            # 1. æ£€æŸ¥æ–‡æœ¬é•¿åº¦ï¼Œå¤ªçŸ­é€šå¸¸è§†ä¸ºæ— æ•ˆ
            if len(text.strip()) < 50:
                logger.warning(f"Extracted content too short ({len(text)} chars) for {url}")
                return None

            # 2. æ£€æŸ¥å¸¸è§é”™è¯¯å…³é”®å­— (JavaScript, Error page, etc)
            error_keywords = [
                "JavaScript is disabled",
                "enable JavaScript",
                "browser is not supported",
                "Something went wrong",
                "Please wait...",
                "Just a moment...",
                "Checking your browser",
                "403 Forbidden",
                "404 Not Found",
                "Access Denied",
                "JavaScript å·²ç»è¢«ç¦ç”¨",
                "è¯·å¯ç”¨ JavaScript",
            ]
            
            # æ£€æŸ¥å‰ 500 ä¸ªå­—ç¬¦å³å¯ (é€šå¸¸é”™è¯¯æç¤ºåœ¨æœ€å‰é¢)
            preview_text = text[:500].lower()
            for ignored in error_keywords:
                if ignored.lower() in preview_text:
                    logger.warning(f"Detected invalid content ('{ignored}') for {url}")
                    return None
            
            return f"æ ‡é¢˜ï¼š{title}\n\nå†…å®¹ï¼š\n{text}"
            
    except Exception as e:
        logger.error(f"Failed to fetch webpage: {e}")
        return None


async def summarize_webpage(url: str) -> str:
    """
    è·å–ç½‘é¡µå¹¶ç”Ÿæˆæ‘˜è¦
    
    Args:
        url: ç½‘é¡µ URL
        
    Returns:
        æ‘˜è¦æ–‡æœ¬
    """
    # è·å–ç½‘é¡µå†…å®¹
    content = await fetch_webpage_content(url)
    if not content:
        return f"âŒ æ— æ³•è·å–ç½‘é¡µå†…å®¹ï¼š{url}"
    
    try:
        # ä½¿ç”¨ Gemini ç”Ÿæˆæ‘˜è¦
        response = gemini_client.models.generate_content(
            model=GEMINI_MODEL,
            contents=f"è¯·ä¸ºä»¥ä¸‹ç½‘é¡µå†…å®¹ç”Ÿæˆç®€æ´çš„ä¸­æ–‡æ‘˜è¦ï¼š\n\n{content}",
            config={
                "system_instruction": (
                    "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹æ‘˜è¦åŠ©æ‰‹ã€‚"
                    "è¯·ç”Ÿæˆç®€æ´ã€å‡†ç¡®çš„ä¸­æ–‡æ‘˜è¦ï¼ŒåŒ…å«ä»¥ä¸‹è¦ç‚¹ï¼š\n"
                    "1. ä¸»é¢˜æ˜¯ä»€ä¹ˆ\n"
                    "2. ä¸»è¦è§‚ç‚¹æˆ–å†…å®¹\n"
                    "3. å…³é”®ä¿¡æ¯\n"
                    "æ‘˜è¦åº”è¯¥ç®€æ´æ˜äº†ï¼Œä¸€èˆ¬ä¸è¶…è¿‡ 200 å­—ã€‚"
                ),
            },
        )
        
        if response.text:
            return f"ğŸ“„ **ç½‘é¡µæ‘˜è¦**\n\nğŸ”— {url}\n\n{response.text}"
        else:
            return f"âŒ æ— æ³•ç”Ÿæˆæ‘˜è¦ï¼š{url}"
            
    except Exception as e:
        logger.error(f"Failed to summarize webpage: {e}")
        return f"âŒ æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼š{url}"
