"""
ç½‘é¡µæ‘˜è¦æ¨¡å— - æå–ç½‘é¡µå†…å®¹å¹¶ä½¿ç”¨ AI ç”Ÿæˆæ‘˜è¦
"""
import re
import logging
import httpx
from bs4 import BeautifulSoup

from config import gemini_client, GEMINI_MODEL

logger = logging.getLogger(__name__)

# URL æ­£åˆ™è¡¨è¾¾å¼
URL_PATTERN = re.compile(
    r'https?://[^\s<>"{}|\\^`\[\]]+'
)


def extract_urls(text: str) -> list[str]:
    """ä»æ–‡æœ¬ä¸­æå– URL"""
    return URL_PATTERN.findall(text)


async def fetch_webpage_content(url: str) -> str | None:
    """
    è·å–ç½‘é¡µå†…å®¹
    
    Args:
        url: ç½‘é¡µ URL
        
    Returns:
        ç½‘é¡µæ–‡æœ¬å†…å®¹ï¼Œå¦‚æœå¤±è´¥è¿”å› None
    """
    try:
        async with httpx.AsyncClient(timeout=15.0, follow_redirects=True) as client:
            response = await client.get(url, headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            })
            response.raise_for_status()
            
            # è§£æ HTML
            soup = BeautifulSoup(response.text, "html.parser")
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            for script in soup(["script", "style", "nav", "footer", "header"]):
                script.decompose()
            
            # è·å–æ ‡é¢˜
            title = soup.title.string if soup.title else ""
            
            # è·å–æ­£æ–‡å†…å®¹
            # ä¼˜å…ˆå°è¯• article æ ‡ç­¾
            article = soup.find("article")
            if article:
                text = article.get_text(separator="\n", strip=True)
            else:
                # å¦åˆ™è·å– body å†…å®¹
                body = soup.find("body")
                if body:
                    text = body.get_text(separator="\n", strip=True)
                else:
                    text = soup.get_text(separator="\n", strip=True)
            
            # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼ˆé¿å… token è¶…é™ï¼‰
            max_length = 8000
            if len(text) > max_length:
                text = text[:max_length] + "..."
            
            return f"æ ‡é¢˜ï¼š{title}\n\nå†…å®¹ï¼š\n{text}"
            
    except Exception as e:
        logger.error(f"Failed to fetch webpage: {e}")
        return None


async def summarize_webpage(url: str) -> str:
    """
    è·å–ç½‘é¡µå¹¶ç”Ÿæˆæ‘˜è¦
    
    Args:
        url: ç½‘é¡µ URL
        
    Returns:
        æ‘˜è¦æ–‡æœ¬
    """
    # è·å–ç½‘é¡µå†…å®¹
    content = await fetch_webpage_content(url)
    if not content:
        return f"âŒ æ— æ³•è·å–ç½‘é¡µå†…å®¹ï¼š{url}"
    
    try:
        # ä½¿ç”¨ Gemini ç”Ÿæˆæ‘˜è¦
        response = gemini_client.models.generate_content(
            model=GEMINI_MODEL,
            contents=f"è¯·ä¸ºä»¥ä¸‹ç½‘é¡µå†…å®¹ç”Ÿæˆç®€æ´çš„ä¸­æ–‡æ‘˜è¦ï¼š\n\n{content}",
            config={
                "system_instruction": (
                    "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹æ‘˜è¦åŠ©æ‰‹ã€‚"
                    "è¯·ç”Ÿæˆç®€æ´ã€å‡†ç¡®çš„ä¸­æ–‡æ‘˜è¦ï¼ŒåŒ…å«ä»¥ä¸‹è¦ç‚¹ï¼š\n"
                    "1. ä¸»é¢˜æ˜¯ä»€ä¹ˆ\n"
                    "2. ä¸»è¦è§‚ç‚¹æˆ–å†…å®¹\n"
                    "3. å…³é”®ä¿¡æ¯\n"
                    "æ‘˜è¦åº”è¯¥ç®€æ´æ˜äº†ï¼Œä¸€èˆ¬ä¸è¶…è¿‡ 200 å­—ã€‚"
                ),
            },
        )
        
        if response.text:
            return f"ğŸ“„ **ç½‘é¡µæ‘˜è¦**\n\nğŸ”— {url}\n\n{response.text}"
        else:
            return f"âŒ æ— æ³•ç”Ÿæˆæ‘˜è¦ï¼š{url}"
            
    except Exception as e:
        logger.error(f"Failed to summarize webpage: {e}")
        return f"âŒ æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼š{url}"
