"""
Deployment Manager Skill - åŸºç¡€æ“ä½œæ¨¡å—

æä¾›éƒ¨ç½²ç›¸å…³çš„åŸºç¡€æ–‡ä»¶æ“ä½œï¼Œä¾› Skill Agent è°ƒåº¦ä½¿ç”¨ã€‚
Agent é€šè¿‡ SKILL.md ä¸­å®šä¹‰çš„ SOP ç¼–æ’ searxng_searchã€web_browserã€docker_ops å®Œæˆéƒ¨ç½²ã€‚
"""

import asyncio
import logging
import os
import re
import shutil
from pathlib import Path
from urllib.parse import quote, urlparse

import httpx

from core.config import (
    X_DEPLOYMENT_STAGING_PATH,
    is_user_allowed,
    SERVER_IP,
)
from core.platform.models import UnifiedContext

logger = logging.getLogger(__name__)
DEFAULT_HOST_PORT = 20080
COMPOSE_FILENAMES = (
    "docker-compose.yml",
    "docker-compose.yaml",
    "compose.yml",
    "compose.yaml",
)

def _is_valid_ipv4(value: str) -> bool:
    parts = value.split(".")
    if len(parts) != 4:
        return False
    try:
        return all(0 <= int(part) <= 255 for part in parts)
    except Exception:
        return False


def _sanitize_display_host(raw_value: str) -> str:
    raw = str(raw_value or "").strip().strip("\"'")
    if not raw:
        return ""

    parsed = raw
    if raw.startswith(("http://", "https://")):
        parsed = (urlparse(raw).hostname or "").strip()
    if not parsed:
        return ""

    lowered = parsed.lower()
    if lowered == "localhost":
        return "localhost"
    if _is_valid_ipv4(parsed):
        return parsed
    # Allow domain-like hostnames (must contain dot to avoid accidental '1' etc.)
    if "." in parsed and re.fullmatch(r"[a-zA-Z0-9.-]+", parsed):
        return parsed
    return ""


def _resolve_display_host() -> str:
    for candidate in (
        SERVER_IP,
        os.getenv("PUBLIC_HOST", ""),
        os.getenv("PUBLIC_IP", ""),
        os.getenv("HOST_IP", ""),
    ):
        host = _sanitize_display_host(candidate)
        if host:
            return host
    return "localhost"


# å·¥ä½œç›®å½• - å¿…é¡»æ˜¯å®¿ä¸»æœºç»å¯¹è·¯å¾„
if not X_DEPLOYMENT_STAGING_PATH:
    logger.warning(
        "âš ï¸ X_DEPLOYMENT_STAGING_PATH æœªé…ç½®ï¼éƒ¨ç½²åŠŸèƒ½å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œã€‚"
        "è¯·åœ¨ .env ä¸­è®¾ç½®ä¸ºå®¿ä¸»æœºç»å¯¹è·¯å¾„ã€‚"
    )
    WORK_BASE = Path("/tmp/deployment_staging")  # Fallback, ä¸æ¨è
else:
    WORK_BASE = Path(X_DEPLOYMENT_STAGING_PATH)

WORK_BASE.mkdir(parents=True, exist_ok=True)


async def execute(ctx: UnifiedContext, params: dict, runtime=None):
    """
    æ‰§è¡Œéƒ¨ç½²ç®¡ç†å™¨çš„åŸºç¡€æ“ä½œã€‚

    å¯ç”¨ action:
    - auto_deploy: è‡ªåŠ¨éƒ¨ç½²å¸¸è§æœåŠ¡/ä»“åº“
    - status: æŸ¥çœ‹å·²éƒ¨ç½²é¡¹ç›®
    - delete_project: åˆ é™¤é¡¹ç›®ç›®å½•ï¼ˆè°¨æ…ï¼‰
    - get_access_info: è·å–é¡¹ç›®è®¿é—®ä¿¡æ¯
    - verify_access: æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯è®¿é—®
    """
    action = params.get("action", "status")

    if action == "auto_deploy":
        return await _auto_deploy(params)

    elif action == "status":
        return await _get_status()

    elif action == "delete_project":
        return await _delete_project(params)

    elif action == "get_access_info":
        return await _get_access_info(params)

    elif action == "verify_access":
        return await _verify_access(params)

    else:
        return {
            "text": (
                f"âŒ æœªçŸ¥æ“ä½œ: {action}ã€‚"
                "æ”¯æŒ: auto_deploy, status, delete_project, get_access_info, verify_access"
            ),
            "ui": {},
        }


def _extract_repo_name(repo_url: str) -> str:
    return repo_url.rstrip("/").split("/")[-1].replace(".git", "")


def _resolve_project_path(target_dir: str | None, repo_name: str) -> Path:
    raw = (target_dir or repo_name or "").strip() or repo_name
    base = WORK_BASE.resolve()
    target_path = Path(raw)
    if target_path.is_absolute():
        resolved = target_path.resolve()
    else:
        resolved = (base / target_path).resolve()

    # Enforce path standard: deployment workspace only.
    if not str(resolved).startswith(str(base)):
        return (base / repo_name).resolve()
    return resolved


def _extract_repo_url_from_text(text: str) -> str:
    if not text:
        return ""
    match = re.search(r"https?://[^\s)]+", text)
    if not match:
        return ""
    return match.group(0).rstrip(".,);")


def _find_compose_file(project_path: Path) -> Path | None:
    for filename in COMPOSE_FILENAMES:
        candidate = project_path / filename
        if candidate.exists():
            return candidate
    return None


def _normalize_service_name(value: str) -> str:
    cleaned = (value or "").strip(" ,ï¼Œã€‚.!ï¼?ï¼Ÿ")
    cleaned = re.sub(r"(æœåŠ¡|ç³»ç»Ÿ|å¹³å°)$", "", cleaned)
    cleaned = re.sub(r"\s+", "-", cleaned.strip())
    return cleaned.lower()


def _canonical_service_key(value: str) -> str:
    return _normalize_service_name(value)


def _extract_service_from_request(request_text: str, explicit_service: str = "") -> str:
    if explicit_service:
        normalized = _canonical_service_key(explicit_service)
        if normalized:
            return normalized

    text = request_text.strip()
    patterns = [
        r"(?:éƒ¨ç½²|å®‰è£…|æ­å»º|å¯åŠ¨)\s*(?:ä¸€å¥—|ä¸€ä¸ª|ä¸ª|å¥—)?\s*([a-zA-Z0-9._\-\u4e00-\u9fff]+(?:\s+[a-zA-Z0-9._\-\u4e00-\u9fff]+)?)",
        r"(?:deploy|install|setup)\s+([a-zA-Z0-9._\-]+)",
    ]
    for pattern in patterns:
        match = re.search(pattern, text, flags=re.IGNORECASE)
        if match:
            candidate = _canonical_service_key(match.group(1))
            if candidate:
                return candidate

    return _canonical_service_key(explicit_service)


def _normalize_host_port(raw_value: object, default: int) -> int:
    try:
        port = int(raw_value)
    except Exception:
        return default
    if 20000 <= port <= 60000:
        return port
    return default


def _extract_published_host_ports(ps_output: str) -> list[int]:
    ports: list[int] = []
    raw = str(ps_output or "")
    for match in re.finditer(
        r"(?:0\.0\.0\.0|\[::\]|::|localhost|127\.0\.0\.1):(\d{2,5})\s*->",
        raw,
        flags=re.IGNORECASE,
    ):
        try:
            port = int(match.group(1))
        except Exception:
            continue
        if 1 <= port <= 65535 and port not in ports:
            ports.append(port)
    return ports


def _build_access_urls(host_port: int) -> tuple[str, str]:
    display_host = _resolve_display_host()
    local_url = f"http://127.0.0.1:{host_port}"
    public_url = f"http://{display_host}:{host_port}"
    if display_host in {"localhost", "127.0.0.1"}:
        return local_url, local_url
    return local_url, public_url


def _normalize_github_repo_url(url: str) -> str:
    if not url:
        return ""
    match = re.match(r"^https?://github\.com/([^/\s]+)/([^/\s#?]+)", url.strip())
    if not match:
        return ""
    owner = match.group(1)
    repo = match.group(2).replace(".git", "")
    if repo.lower() in {"issues", "pull", "pulls", "releases", "tags", "wiki"}:
        return ""
    return f"https://github.com/{owner}/{repo}.git"


def _split_github_repo(repo_url: str) -> tuple[str, str]:
    match = re.match(r"^https?://github\.com/([^/\s]+)/([^/\s#?]+)", repo_url.strip())
    if not match:
        return "", ""
    owner = match.group(1).strip().lower()
    repo = match.group(2).replace(".git", "").strip().lower()
    return owner, repo


def _compact_name(value: str) -> str:
    return re.sub(r"[^a-z0-9]+", "", str(value or "").lower())


def _safe_suffix(value: str, default: str = "repo") -> str:
    suffix = re.sub(r"[^a-z0-9-]+", "-", str(value or "").strip().lower()).strip("-")
    return suffix or default


def _build_conflict_clone_path(target_path: Path, repo_url: str) -> Path:
    owner, _repo = _split_github_repo(repo_url)
    suffix = _safe_suffix(owner, default="fresh")
    base_name = f"{target_path.name}-{suffix}"
    candidate = target_path.parent / base_name
    index = 1
    while candidate.exists():
        candidate = target_path.parent / f"{base_name}-{index}"
        index += 1
    return candidate


def _has_redeploy_confirmation(text: str) -> bool:
    """
    Check whether user explicitly confirms redeploy.
    Avoid matching generic "éƒ¨ç½²" wording.
    """
    raw = (text or "").strip().lower()
    if not raw:
        return False
    confirmations = (
        "ç»§ç»­é‡éƒ¨ç½²",
        "ç¡®è®¤é‡éƒ¨ç½²",
        "é‡æ–°éƒ¨ç½²",
        "é‡éƒ¨ç½²",
        "redeploy",
        "force redeploy",
    )
    return any(token in raw for token in confirmations)


def _classify_failure_mode(output: str) -> str:
    lowered = str(output or "").lower()
    recoverable_tokens = (
        "env file",
        ".env not found",
        "no such file or directory",
        "address already in use",
        "port is already allocated",
        "bind for",
        "name is already in use",
        "conflict",
    )
    if any(token in lowered for token in recoverable_tokens):
        return "recoverable"
    return "fatal"


async def _search_searxng(query: str, language: str = "zh-CN", num_results: int = 8) -> list[dict]:
    base_url = os.getenv("SEARXNG_URL", "").strip()
    if not base_url:
        return []

    if not base_url.endswith("/search"):
        if not base_url.endswith("/"):
            base_url += "/"
        base_url += "search"

    search_url = (
        f"{base_url}?q={quote(query)}&format=json"
        f"&categories=general,it,science&language={language}&time_range=year"
    )

    try:
        async with httpx.AsyncClient(timeout=20.0) as client:
            response = await client.get(search_url)
            response.raise_for_status()
            data = response.json()
            return (data.get("results") or [])[: max(1, min(20, num_results))]
    except Exception as exc:
        logger.warning("SearXNG search failed for query=%s: %s", query, exc)
        return []


async def _search_repo_and_guides(request_text: str, service_hint: str) -> dict:
    base = service_hint or request_text or ""
    queries = [
        f"{base} github docker compose",
        f"{base} official deployment docker compose",
    ]
    all_results: list[dict] = []
    seen_url: set[str] = set()
    for query in queries:
        results = await _search_searxng(query)
        for item in results:
            url = str(item.get("url", "")).strip()
            if not url or url in seen_url:
                continue
            seen_url.add(url)
            all_results.append(item)

    repo_candidates: list[tuple[int, str, str]] = []
    guides: list[str] = []
    service_lower = service_hint.lower()
    service_compact = _compact_name(service_lower)
    for item in all_results:
        url = str(item.get("url", "")).strip()
        title = str(item.get("title", "")).lower()
        content = str(item.get("content", "")).lower()
        github_repo = _normalize_github_repo_url(url)
        if github_repo:
            score = 5
            if service_lower and service_lower in f"{url} {title} {content}":
                score += 3
            owner, repo_name = _split_github_repo(github_repo)
            repo_compact = _compact_name(repo_name)
            if service_compact and repo_compact == service_compact:
                score += 8
            elif service_compact and repo_compact.startswith(service_compact):
                score += 5
            elif service_compact and service_compact in repo_compact:
                score += 2
            if service_compact and service_compact in _compact_name(owner):
                score += 2
            if "official" in f"{title} {content}" or "å®˜æ–¹" in f"{title} {content}":
                score += 1
            if (
                repo_name
                and repo_name != service_lower
                and any(tag in repo_name for tag in ("i18n", "chinese", "mirror", "fork"))
            ):
                score -= 2
            repo_candidates.append((score, github_repo, url))

        # Keep top deployment references.
        if len(guides) < 4:
            lowered_url = url.lower()
            if any(tag in lowered_url for tag in ("docs", "docker", "compose", "github")):
                guides.append(url)

    repo_candidates.sort(key=lambda item: item[0], reverse=True)
    dedup_candidates: list[dict] = []
    seen_repo: set[str] = set()
    for score, repo, source in repo_candidates:
        if repo in seen_repo:
            continue
        seen_repo.add(repo)
        dedup_candidates.append(
            {
                "repo_url": repo,
                "source_url": source,
                "score": score,
            }
        )

    repo_url = dedup_candidates[0]["repo_url"] if dedup_candidates else ""
    repo_source = dedup_candidates[0]["source_url"] if dedup_candidates else ""

    return {
        "queries": queries,
        "repo_url": repo_url,
        "repo_source": repo_source,
        "repo_candidates": dedup_candidates,
        "guides": guides,
    }


async def _search_repo_candidates_via_github(
    request_text: str,
    service_hint: str,
    per_query: int = 6,
) -> list[dict]:
    """
    Generic fallback repository search using GitHub Search API.
    Used only when searxng results are unavailable or weak.
    """
    base = (service_hint or request_text or "").strip()
    if not base:
        return []

    queries = [
        f"{base} docker compose",
        f"{base} docker",
    ]
    seen_repo: set[str] = set()
    candidates: list[dict] = []
    service_compact = _compact_name(service_hint or base)

    headers = {
        "Accept": "application/vnd.github+json",
        "User-Agent": "x-bot-deployment-manager",
    }
    token = os.getenv("GITHUB_TOKEN", "").strip()
    if token:
        headers["Authorization"] = f"Bearer {token}"

    async with httpx.AsyncClient(timeout=20.0, headers=headers) as client:
        for query in queries:
            try:
                response = await client.get(
                    "https://api.github.com/search/repositories",
                    params={
                        "q": query,
                        "sort": "stars",
                        "order": "desc",
                        "per_page": max(1, min(10, int(per_query))),
                    },
                )
                if response.status_code >= 400:
                    continue
                payload = response.json()
            except Exception as exc:
                logger.warning("GitHub fallback search failed for query=%s: %s", query, exc)
                continue

            for item in payload.get("items", []) or []:
                html_url = str(item.get("html_url", "")).strip()
                repo_url = _normalize_github_repo_url(html_url)
                if not repo_url or repo_url in seen_repo:
                    continue
                seen_repo.add(repo_url)

                full_name = str(item.get("full_name", "")).lower()
                repo_desc = str(item.get("description", "") or "").lower()
                stars = int(item.get("stargazers_count") or 0)
                archived = bool(item.get("archived"))
                fork = bool(item.get("fork"))

                score = 5 + min(20, stars // 1000)
                repo_compact = _compact_name(full_name)
                desc_compact = _compact_name(repo_desc)
                if service_compact and (
                    service_compact in repo_compact or service_compact in desc_compact
                ):
                    score += 6
                if not fork:
                    score += 2
                if archived:
                    score -= 6

                candidates.append(
                    {
                        "repo_url": repo_url,
                        "source_url": html_url,
                        "score": score,
                    }
                )

    candidates.sort(key=lambda item: int(item.get("score", 0)), reverse=True)
    return candidates


def _rewrite_compose_host_port(
    compose_file: Path,
    container_port: int,
    host_port: int,
) -> tuple[bool, str]:
    """
    Force host port mapping to `host_port:container_port` in compose file.
    Returns (changed, error_message).
    """
    try:
        original = compose_file.read_text(encoding="utf-8")
    except Exception as exc:
        return False, f"è¯»å– compose æ–‡ä»¶å¤±è´¥: {exc}"

    updated = original

    # Handles forms like "8080:8080", '8080:8080', 8080:8080
    updated, count = re.subn(
        rf'(?P<prefix>["\']?)(?P<host>\d{{2,5}})\s*:\s*{container_port}(?P<suffix>["\']?)',
        rf"\g<prefix>{host_port}:{container_port}\g<suffix>",
        updated,
        count=1,
    )

    if count == 0:
        return False, f"æœªæ‰¾åˆ°å¯æ›¿æ¢çš„ `*:{container_port}` ç«¯å£æ˜ å°„ï¼Œä¿æŒåŸé…ç½®ã€‚"

    if updated == original:
        return False, ""

    try:
        compose_file.write_text(updated, encoding="utf-8")
        return True, ""
    except Exception as exc:
        return False, f"å†™å…¥ compose æ–‡ä»¶å¤±è´¥: {exc}"


async def _run_shell(command: str, cwd: Path, timeout: int = 120) -> tuple[int, str]:
    process = await asyncio.create_subprocess_shell(
        command,
        cwd=str(cwd),
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
    )
    try:
        stdout, stderr = await asyncio.wait_for(process.communicate(), timeout=timeout)
    except asyncio.TimeoutError:
        try:
            process.kill()
        except Exception:
            pass
        return -1, f"å‘½ä»¤è¶…æ—¶ ({timeout}s): {command}"

    out_text = (stdout or b"").decode("utf-8", errors="replace")
    err_text = (stderr or b"").decode("utf-8", errors="replace")
    combined = out_text
    if err_text:
        combined = f"{combined}\n[stderr]\n{err_text}" if combined else f"[stderr]\n{err_text}"
    return process.returncode, combined.strip()


def _patch_compose_known_issues(
    compose_file: Path,
    deploy_output: str,
) -> list[str]:
    """
    Apply deterministic fixes for common compose issues.
    Returns a list of applied fix notes.
    """
    notes: list[str] = []
    try:
        original = compose_file.read_text(encoding="utf-8")
    except Exception:
        return notes

    updated = original
    lowered_output = (deploy_output or "").lower()

    # Normalize unresolved version placeholders in generic compose templates.
    if "invalid reference format" in lowered_output:
        updated = re.sub(r"\$\{version\}", "latest", updated, flags=re.IGNORECASE)
        updated = re.sub(r"\{version\}", "latest", updated, flags=re.IGNORECASE)
        if updated != original:
            notes.append("æ£€æµ‹åˆ°é•œåƒæ ‡ç­¾å ä½ç¬¦å¼‚å¸¸ï¼Œå·²æ›¿æ¢ä¸º `latest`ã€‚")

    if updated == original:
        return notes

    try:
        compose_file.write_text(updated, encoding="utf-8")
    except Exception:
        return []

    if not notes:
        notes.append("å·²è‡ªåŠ¨ä¿®å¤ compose ä¸­çš„å·²çŸ¥é—®é¢˜åé‡è¯•ã€‚")
    return notes


async def _auto_deploy(params: dict) -> dict:
    """
    Deterministic one-shot deployment path.
    Search-first behavior:
    1) Resolve repo URL from user input / search.
    2) Clone repo into deployment workspace.
    3) Resolve compose method (repo compose file or service template fallback).
    4) Bring up container and report URL.
    """
    request_text = str(params.get("request", "") or "").strip()
    service_input = str(params.get("service", "") or params.get("name", "")).strip()
    repo_url = str(params.get("repo_url", "") or "").strip()
    repo_from_user = bool(repo_url)
    search_summary_lines: list[str] = []
    force_redeploy = bool(params.get("force_redeploy")) or _has_redeploy_confirmation(
        request_text
    )

    # 0) Infer service name.
    service_key = _extract_service_from_request(request_text, service_input)
    if service_key:
        search_summary_lines.append(f"- æœåŠ¡è¯†åˆ«: `{service_key}`")

    # 1) Try explicit URL in text first.
    if not repo_url:
        repo_url = _extract_repo_url_from_text(request_text)
        repo_url = _normalize_github_repo_url(repo_url) or repo_url
        if repo_url:
            repo_from_user = True
            search_summary_lines.append(f"- ä»ç”¨æˆ·è¾“å…¥æå–ä»“åº“: `{repo_url}`")

    repo_candidates: list[dict] = []
    if repo_url:
        repo_candidates.append(
            {
                "repo_url": repo_url,
                "source_url": "user_input",
                "score": 9999 if repo_from_user else 1000,
            }
        )

    # 2) Search repository and deployment references when URL missing.
    search_result = {"queries": [], "repo_url": "", "repo_source": "", "guides": [], "repo_candidates": []}
    if not repo_candidates:
        search_result = await _search_repo_and_guides(request_text, service_key)
        for candidate in search_result.get("repo_candidates", []):
            if not isinstance(candidate, dict):
                continue
            repo_candidates.append(
                {
                    "repo_url": str(candidate.get("repo_url", "")).strip(),
                    "source_url": str(candidate.get("source_url", "")).strip(),
                    "score": int(candidate.get("score", 0)),
                }
            )

        if repo_candidates:
            best = repo_candidates[0]
            search_summary_lines.append(f"- è‡ªåŠ¨æœç´¢å‘½ä¸­ä»“åº“: `{best.get('repo_url', '')}`")
            if best.get("source_url"):
                search_summary_lines.append(f"- å‘½ä¸­æ¥æº: {best.get('source_url')}")

        for guide in search_result.get("guides", [])[:3]:
            search_summary_lines.append(f"- éƒ¨ç½²å‚è€ƒ: {guide}")

    # 3) Generic fallback search via GitHub API.
    if not repo_candidates:
        github_candidates = await _search_repo_candidates_via_github(
            request_text=request_text,
            service_hint=service_key,
        )
        if github_candidates:
            repo_candidates.extend(github_candidates)
            best = github_candidates[0]
            search_summary_lines.append("- å·²å¯ç”¨ GitHub API é€šç”¨å…œåº•æœç´¢ã€‚")
            search_summary_lines.append(f"- GitHub å‘½ä¸­ä»“åº“: `{best.get('repo_url', '')}`")
            if best.get("source_url"):
                search_summary_lines.append(f"- å‘½ä¸­æ¥æº: {best.get('source_url')}")

    # 4) Deduplicate candidate list while keeping order.
    deduped_candidates: list[dict] = []
    seen_repos: set[str] = set()
    for candidate in repo_candidates:
        candidate_repo = _normalize_github_repo_url(candidate.get("repo_url", "")) or str(
            candidate.get("repo_url", "")
        ).strip()
        if not candidate_repo or candidate_repo in seen_repos:
            continue
        seen_repos.add(candidate_repo)
        deduped_candidates.append(
            {
                "repo_url": candidate_repo,
                "source_url": str(candidate.get("source_url", "")).strip(),
                "score": int(candidate.get("score", 0)),
            }
        )
    repo_candidates = deduped_candidates

    if not repo_candidates:
        query_text = "\n".join(search_result.get("queries", []))
        return {
            "text": (
                "âŒ æ— æ³•è‡ªåŠ¨éƒ¨ç½²ï¼šæœªè¯†åˆ«åˆ°å¯éƒ¨ç½²ä»“åº“ã€‚\n\n"
                "å·²å°è¯•æœç´¢ä½†æœªæ‰¾åˆ°å¯é ä»“åº“ã€‚\n"
                f"æœç´¢è¯:\n```\n{query_text or request_text}\n```\n\n"
                "è¯·æä¾› GitHub ä»“åº“é“¾æ¥ï¼Œæˆ–æ˜ç¡®è¯´æ˜ç›®æ ‡æœåŠ¡åç§°ã€‚"
            ),
            "ui": {},
            "success": False,
            "terminal": True,
            "task_outcome": "failed",
            "failure_mode": "fatal",
        }

    # Use the top candidate as initial target; if it is not deployable, we will
    # automatically try the next candidates below.
    repo_url = repo_candidates[0]["repo_url"]

    repo_name = _extract_repo_name(repo_url) or "project"
    target_dir = str(params.get("target_dir", "") or params.get("project_name", "")).strip()
    target_path = _resolve_project_path(target_dir, repo_name)
    existing_compose = _find_compose_file(target_path) if target_path.exists() else None

    # If an existing deployment is already running, report first and ask for confirmation.
    if existing_compose:
        has_unresolved_version_placeholder = False
        try:
            compose_text = existing_compose.read_text(encoding="utf-8")
            has_unresolved_version_placeholder = bool(
                re.search(
                    r"\$\{version\}|\{version\}",
                    compose_text,
                    flags=re.IGNORECASE,
                )
            )
        except Exception:
            has_unresolved_version_placeholder = False

        ps_cmd = f"docker compose -f {existing_compose.name} ps"
        ps_code, ps_output = await _run_shell(ps_cmd, target_path, timeout=60)
        already_running = ps_code == 0 and (
            "Up" in ps_output or "running" in ps_output.lower()
        )
        if already_running and not force_redeploy and not has_unresolved_version_placeholder:
            existing_port = _normalize_host_port(
                params.get("host_port", DEFAULT_HOST_PORT), DEFAULT_HOST_PORT
            )
            local_url, public_url = _build_access_urls(existing_port)
            return {
                "text": (
                    "â„¹ï¸ æ£€æµ‹åˆ°ç›®æ ‡ç›®å½•å·²æœ‰è¿è¡Œä¸­çš„éƒ¨ç½²ã€‚\n\n"
                    f"ç›®å½•: `{target_path}`\n"
                    f"compose: `{existing_compose.name}`\n"
                    f"è®¿é—®åœ°å€(æœ¬æœº): {local_url}\n"
                    f"è®¿é—®åœ°å€(å±€åŸŸç½‘/å…¬ç½‘): {public_url}\n\n"
                    "å¦‚éœ€é‡éƒ¨ç½²ï¼Œè¯·æ˜ç¡®å›å¤ï¼š`ç»§ç»­é‡éƒ¨ç½²`ã€‚"
                ),
                "ui": {},
                "success": True,
                "terminal": True,
                "task_outcome": "partial",
                "needs_confirmation": True,
                "project_name": repo_name,
                "project_path": str(target_path),
                "url": public_url,
            }

    compose_file = None
    compose_notes: list[str] = []
    candidate_attempt_notes: list[str] = []

    # Try top candidates in order; pick the first that is cloneable and contains compose.
    for candidate in repo_candidates[:3]:
        candidate_repo = str(candidate.get("repo_url", "")).strip()
        if not candidate_repo:
            continue
        candidate_repo_name = _extract_repo_name(candidate_repo) or "project"
        candidate_target_path = _resolve_project_path(target_dir, candidate_repo_name)

        clone_result = await _clone_repo(
            {
                "repo_url": candidate_repo,
                "target_dir": str(candidate_target_path),
            }
        )
        if clone_result.get("text", "").startswith("âŒ"):
            candidate_attempt_notes.append(
                f"- `{candidate_repo}`: å…‹éš†å¤±è´¥ ({clone_result.get('text', '')[:160]})"
            )
            continue

        resolved_project_path = str(clone_result.get("project_path") or "").strip()
        if resolved_project_path:
            candidate_target_path = Path(resolved_project_path).resolve()
        resolved_project_name = str(clone_result.get("project_name") or "").strip()
        if resolved_project_name:
            candidate_repo_name = resolved_project_name

        candidate_compose = _find_compose_file(candidate_target_path)
        if not candidate_compose:
            candidate_attempt_notes.append(
                f"- `{candidate_repo}`: æœªæ‰¾åˆ° compose æ–‡ä»¶"
            )
            continue

        repo_url = candidate_repo
        repo_name = candidate_repo_name
        target_path = candidate_target_path
        compose_file = candidate_compose
        if candidate_repo != repo_candidates[0]["repo_url"]:
            search_summary_lines.append(
                f"- åˆå§‹å€™é€‰ä¸å¯éƒ¨ç½²ï¼Œå·²è‡ªåŠ¨åˆ‡æ¢åˆ°: `{candidate_repo}`"
            )
        break

    if not compose_file:
        attempts = "\n".join(candidate_attempt_notes) or "- æ— å¯ç”¨å€™é€‰ä»“åº“"
        return {
            "text": (
                "âŒ æœªæ‰¾åˆ°å¯ç›´æ¥éƒ¨ç½²çš„ä»“åº“ï¼ˆç¼ºå°‘ compose æˆ–å…‹éš†å¤±è´¥ï¼‰ã€‚\n\n"
                f"å°è¯•è®°å½•:\n{attempts}\n\n"
                "è¯·æä¾›æ›´æ˜ç¡®çš„ä»“åº“é“¾æ¥ï¼Œæˆ–è¡¥å……éƒ¨ç½²æ–‡æ¡£ã€‚"
            ),
            "ui": {},
            "success": False,
            "terminal": True,
            "task_outcome": "failed",
            "failure_mode": "fatal",
            "project_path": str(target_path),
        }

    if repo_url:
        search_summary_lines.append(f"- æœ€ç»ˆéƒ¨ç½²ä»“åº“: `{repo_url}`")

    default_port = DEFAULT_HOST_PORT
    host_port = _normalize_host_port(params.get("host_port", default_port), default_port)
    rewrite_note = ""
    raw_container_port = params.get("container_port")
    container_port: int | None = None
    try:
        parsed = int(raw_container_port) if raw_container_port is not None else 0
        if 1 <= parsed <= 65535:
            container_port = parsed
    except Exception:
        container_port = None

    if container_port is not None:
        changed, rewrite_error = _rewrite_compose_host_port(
            compose_file=compose_file,
            container_port=container_port,
            host_port=host_port,
        )
        if rewrite_error:
            rewrite_note = f"\nâš ï¸ {rewrite_error}"
        elif changed:
            rewrite_note = f"\nâœ… å·²å°†æœåŠ¡ç«¯å£æ˜ å°„åˆ° `{host_port}:{container_port}`ã€‚"

    up_cmd = f"docker compose -f {compose_file.name} up -d"
    up_code, up_output = await _run_shell(up_cmd, target_path, timeout=300)
    if up_code != 0:
        patch_notes = _patch_compose_known_issues(compose_file, up_output)
        if patch_notes:
            compose_notes.extend(patch_notes)
            retry_code, retry_output = await _run_shell(up_cmd, target_path, timeout=300)
            if retry_code == 0:
                up_code, up_output = retry_code, retry_output
            else:
                combined_output = (
                    f"{up_output}\n\n[è‡ªåŠ¨ä¿®å¤åé‡è¯•è¾“å‡º]\n{retry_output}"
                ).strip()
                notes_text = "\n".join([f"- {item}" for item in patch_notes])
                return {
                    "text": (
                        f"âŒ éƒ¨ç½²å¯åŠ¨å¤±è´¥ (exit={retry_code})\n"
                        f"ç›®å½•: `{target_path}`\n"
                        f"å‘½ä»¤: `{up_cmd}`\n\n"
                        f"è‡ªåŠ¨ä¿®å¤:\n{notes_text}\n\n"
                        f"è¾“å‡º:\n```\n{combined_output[:3000]}\n```"
                    ),
                    "ui": {},
                    "success": False,
                    "terminal": True,
                    "task_outcome": "failed",
                    "failure_mode": _classify_failure_mode(combined_output),
                    "project_path": str(target_path),
                }
        else:
            return {
                "text": (
                    f"âŒ éƒ¨ç½²å¯åŠ¨å¤±è´¥ (exit={up_code})\n"
                    f"ç›®å½•: `{target_path}`\n"
                    f"å‘½ä»¤: `{up_cmd}`\n\n"
                    f"è¾“å‡º:\n```\n{up_output[:3000]}\n```"
                ),
                "ui": {},
                "success": False,
                "terminal": True,
                "task_outcome": "failed",
                "failure_mode": _classify_failure_mode(up_output),
                "project_path": str(target_path),
            }

    if up_code != 0:
        return {
            "text": (
                f"âŒ éƒ¨ç½²å¯åŠ¨å¤±è´¥ (exit={up_code})\n"
                f"ç›®å½•: `{target_path}`\n"
                f"å‘½ä»¤: `{up_cmd}`\n\n"
                f"è¾“å‡º:\n```\n{up_output[:3000]}\n```"
            ),
            "ui": {},
            "success": False,
            "terminal": True,
            "task_outcome": "failed",
            "failure_mode": _classify_failure_mode(up_output),
            "project_path": str(target_path),
        }

    ps_cmd = f"docker compose -f {compose_file.name} ps"
    ps_code, ps_output = await _run_shell(ps_cmd, target_path, timeout=60)
    running = ps_code == 0 and ("Up" in ps_output or "running" in ps_output.lower())

    published_ports = _extract_published_host_ports(ps_output)
    effective_host_port = published_ports[0] if published_ports else host_port
    local_url, public_url = _build_access_urls(effective_host_port)
    status_line = "ğŸŸ¢ å®¹å™¨å·²å¯åŠ¨" if running else "ğŸŸ¡ å®¹å™¨å·²åˆ›å»ºï¼Œè¯·æ£€æŸ¥çŠ¶æ€"
    search_summary = "\n".join(search_summary_lines)
    search_block = f"\nè‡ªåŠ¨æ£€ç´¢ç»“æœ:\n{search_summary}\n" if search_summary else ""
    compose_block = ""
    if compose_notes:
        compose_block = "\nè‡ªåŠ¨ä¿®å¤:\n" + "\n".join([f"- {item}" for item in compose_notes]) + "\n"
    access_block = (
        f"- æœ¬æœº: {local_url}\n- å±€åŸŸç½‘/å…¬ç½‘: {public_url}"
        if public_url != local_url
        else f"- æœ¬æœº: {local_url}"
    )

    return {
        "text": (
            f"âœ… è‡ªåŠ¨éƒ¨ç½²æµç¨‹å®Œæˆ\n\n"
            f"{search_block}"
            f"é¡¹ç›®: `{repo_name}`\n"
            f"ç›®å½•: `{target_path}`\n"
            f"compose: `{compose_file.name}`\n"
            f"çŠ¶æ€: {status_line}\n"
            f"è®¿é—®åœ°å€:\n{access_block}"
            f"{compose_block}"
            f"{rewrite_note}\n\n"
            f"`docker compose ps` è¾“å‡º:\n```\n{ps_output[:2500]}\n```"
        ),
        "ui": {},
        "success": running,
        "terminal": True,
        "task_outcome": "done" if running else "partial",
        "project_name": repo_name,
        "project_path": str(target_path),
        "url": public_url,
    }


async def _clone_repo(params: dict) -> dict:
    """å…‹éš† GitHub ä»“åº“"""
    repo_url = params.get("repo_url", "")
    if not repo_url:
        return {"text": "âŒ ç¼ºå°‘å‚æ•°: repo_url", "ui": {}}

    # è§£æé¡¹ç›®å
    repo_name = _extract_repo_name(repo_url)
    target_dir = params.get("target_dir")
    target_path = _resolve_project_path(target_dir, repo_name)

    try:
        if target_path.exists():
            # éç ´åæ€§æ›´æ–°å·²æœ‰ä»“åº“ï¼šä»…å¿«è¿›æ›´æ–°ï¼Œä¸è¦†ç›–æœ¬åœ°æ”¹åŠ¨ã€‚
            logger.info(f"Updating existing repository (non-destructive): {target_path}")
            if not (target_path / ".git").exists():
                return {
                    "text": (
                        f"âš ï¸ ç›®å½•å·²å­˜åœ¨ä½†ä¸æ˜¯ Git ä»“åº“ï¼š`{target_path}`ã€‚\n"
                        "è¯·ç¡®è®¤ç›®å½•å†…å®¹ï¼Œæˆ–æ›´æ¢ target_dir åé‡è¯•ã€‚"
                    ),
                    "ui": {},
                    "success": False,
                    "project_path": str(target_path),
                }

            requested_repo = _normalize_github_repo_url(str(repo_url)) or str(repo_url).strip()
            existing_remote = await _read_git_remote_url(target_path)
            existing_repo = _normalize_github_repo_url(existing_remote) or existing_remote
            if requested_repo and existing_repo and requested_repo != existing_repo:
                new_target = _build_conflict_clone_path(target_path, requested_repo)
                logger.info(
                    "Repository mismatch at %s: existing=%s requested=%s -> clone to %s",
                    target_path,
                    existing_repo,
                    requested_repo,
                    new_target,
                )
                clone_result = await _do_clone(requested_repo, new_target)
                if clone_result.get("success"):
                    clone_result["text"] = (
                        "â„¹ï¸ æ£€æµ‹åˆ°ç›®æ ‡ç›®å½•å·²å­˜åœ¨å…¶ä»–ä»“åº“ï¼Œå·²è‡ªåŠ¨åˆ‡æ¢åˆ°æ–°ç›®å½•å…‹éš†ã€‚\n\n"
                        f"åŸç›®å½•: `{target_path}`\n"
                        f"åŸä»“åº“: `{existing_repo}`\n"
                        f"ç›®æ ‡ä»“åº“: `{requested_repo}`\n\n"
                        f"{clone_result.get('text', '')}"
                    )
                return clone_result

            process = await asyncio.create_subprocess_exec(
                "git",
                "pull",
                "--ff-only",
                cwd=str(target_path),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )
            stdout, stderr = await process.communicate()
            output = (stdout or b"").decode("utf-8", errors="replace")
            error = (stderr or b"").decode("utf-8", errors="replace")

            if process.returncode != 0:
                return {
                    "text": (
                        f"âš ï¸ ä»“åº“å·²å­˜åœ¨ï¼Œä½†æ— æ³•è¿›è¡Œéç ´åæ›´æ–°ã€‚\n"
                        f"è·¯å¾„: `{target_path}`\n\n"
                        f"è¾“å‡º:\n```\n{(output + error)[:2000]}\n```\n"
                        "è¯·å…ˆå¤„ç†æœ¬åœ°åˆ†æ”¯/æ”¹åŠ¨åå†é‡è¯•ï¼Œæˆ–æŒ‡å®šæ–°ç›®å½•éƒ¨ç½²ã€‚"
                    ),
                    "ui": {},
                    "success": False,
                    "project_path": str(target_path),
                }

            return {
                "text": f"âœ… ä»“åº“å·²æ›´æ–°ï¼ˆéç ´åï¼‰: {repo_name}\n\nè·¯å¾„: `{target_path}`",
                "ui": {},
                "success": True,
                "project_path": str(target_path),
                "project_name": repo_name,
            }
        else:
            return await _do_clone(repo_url, target_path)

    except Exception as e:
        logger.error(f"Clone error: {e}")
        return {"text": f"âŒ å…‹éš†å¤±è´¥: {e}", "ui": {}}


async def _read_git_remote_url(target_path: Path) -> str:
    process = await asyncio.create_subprocess_exec(
        "git",
        "config",
        "--get",
        "remote.origin.url",
        cwd=str(target_path),
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
    )
    stdout, _stderr = await process.communicate()
    if process.returncode != 0:
        return ""
    return (stdout or b"").decode("utf-8", errors="replace").strip()


async def _do_clone(repo_url: str, target_path: Path) -> dict:
    """æ‰§è¡Œ git clone"""
    repo_name = target_path.name

    process = await asyncio.create_subprocess_exec(
        "git",
        "clone",
        "--depth",
        "1",
        repo_url,
        str(target_path),
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
    )
    stdout, stderr = await process.communicate()

    if process.returncode == 0:
        return {
            "text": f"âœ… ä»“åº“å…‹éš†æˆåŠŸ: {repo_name}\n\nè·¯å¾„: `{target_path}`",
            "ui": {},
            "success": True,
            "project_path": str(target_path),
            "project_name": repo_name,
        }
    else:
        error_msg = stderr.decode("utf-8", errors="replace")
        return {
            "text": f"âŒ å…‹éš†å¤±è´¥:\n```\n{error_msg}\n```",
            "ui": {},
            "success": False,
        }


async def _get_status() -> dict:
    """è·å–å·²éƒ¨ç½²é¡¹ç›®çŠ¶æ€"""
    projects = []

    try:
        # åˆ—å‡ºå·¥ä½œç›®å½•ä¸‹çš„æ‰€æœ‰é¡¹ç›®
        for item in WORK_BASE.iterdir():
            if item.is_dir():
                # æ£€æŸ¥æ˜¯å¦æœ‰ docker-compose.yml
                compose_file = item / "docker-compose.yml"
                if not compose_file.exists():
                    compose_file = item / "docker-compose.yaml"

                has_compose = compose_file.exists()
                projects.append(
                    {
                        "name": item.name,
                        "path": str(item),
                        "has_compose": has_compose,
                    }
                )

        if not projects:
            return {
                "text": "ğŸ“­ æš‚æ— éƒ¨ç½²é¡¹ç›®ã€‚\n\nå·¥ä½œç›®å½•: `" + str(WORK_BASE) + "`",
                "ui": {},
            }

        # è·å–è¿è¡Œä¸­çš„å®¹å™¨åŠå…¶ç«¯å£
        container_ports = {}  # {container_name: [ports]}
        try:
            process = await asyncio.create_subprocess_shell(
                "docker ps --format '{{.Names}}|{{.Ports}}'",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )
            stdout, _ = await process.communicate()
            for line in stdout.decode().strip().split("\n"):
                if "|" in line:
                    name, ports_str = line.split("|", 1)
                    # è§£æç«¯å£ï¼Œå¦‚ "0.0.0.0:21000->8080/tcp"
                    ports = []
                    import re

                    for match in re.findall(r"0\.0\.0\.0:(\d+)->", ports_str):
                        ports.append(int(match))
                    container_ports[name] = ports
        except Exception:
            pass

        # æ„å»ºè¾“å‡º
        lines = ["ğŸ“‹ **å·²éƒ¨ç½²é¡¹ç›®**:\n"]
        for proj in projects:
            name = proj["name"]

            # æŸ¥æ‰¾åŒ¹é…çš„å®¹å™¨
            matching_ports = []
            for container_name, ports in container_ports.items():
                if name in container_name:
                    matching_ports.extend(ports)

            if matching_ports:
                status = "ğŸŸ¢ è¿è¡Œä¸­"
                urls = []
                for port in sorted(set(matching_ports)):
                    local_url, public_url = _build_access_urls(port)
                    for item in (local_url, public_url):
                        if item not in urls:
                            urls.append(item)
                access_info = " | ".join(urls)
                lines.append(f"â€¢ **{name}**: {status}")
                lines.append(f"  ğŸ“ è®¿é—®: {access_info}")
            else:
                status = "âšª æœªè¿è¡Œ"
                compose_status = (
                    "âœ“ docker-compose" if proj["has_compose"] else "âœ— æ— é…ç½®"
                )
                lines.append(f"â€¢ **{name}**: {status} ({compose_status})")

        lines.append(f"\nå·¥ä½œç›®å½•: `{WORK_BASE}`")

        return {"text": "\n".join(lines), "ui": {}}

    except Exception as e:
        logger.error(f"Get status error: {e}")
        return {"text": f"âŒ è·å–çŠ¶æ€å¤±è´¥: {e}", "ui": {}}


async def _get_access_info(params: dict) -> dict:
    """è·å–ç‰¹å®šé¡¹ç›®çš„è®¿é—®ä¿¡æ¯"""
    import re

    name = params.get("name", "")
    if not name:
        return {"text": "âŒ ç¼ºå°‘å‚æ•°: name", "ui": {}}

    try:
        # æŸ¥è¯¢ docker ps è·å–ç«¯å£ä¿¡æ¯
        process = await asyncio.create_subprocess_shell(
            "docker ps --format '{{.Names}}|{{.Ports}}'",
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        stdout, _ = await process.communicate()

        ports = []
        for line in stdout.decode().strip().split("\n"):
            if "|" in line:
                container_name, ports_str = line.split("|", 1)
                if name in container_name:
                    # è§£æç«¯å£
                    for match in re.findall(r"0\.0\.0\.0:(\d+)->", ports_str):
                        ports.append(int(match))

        if not ports:
            return {
                "text": f"âš ï¸ æœªæ‰¾åˆ°è¿è¡Œä¸­çš„å®¹å™¨: {name}\n\nè¯·å…ˆç¡®ä¿æœåŠ¡å·²å¯åŠ¨ã€‚",
                "ui": {},
            }

        urls = []
        for port in sorted(set(ports)):
            local_url, public_url = _build_access_urls(port)
            for item in (local_url, public_url):
                if item not in urls:
                    urls.append(item)

        result = f"âœ… **{name}** è®¿é—®ä¿¡æ¯:\n\n"
        for url in urls:
            result += f"ğŸ“ {url}\n"

        return {"text": result, "ui": {}, "urls": urls}

    except Exception as e:
        logger.error(f"Get access info error: {e}")
        return {"text": f"âŒ è·å–è®¿é—®ä¿¡æ¯å¤±è´¥: {e}", "ui": {}}


async def _verify_access(params: dict) -> dict:
    """
    éªŒè¯éƒ¨ç½²çš„æœåŠ¡æ˜¯å¦å¯è®¿é—®ã€‚

    ä½¿ç”¨ httpx æ£€æŸ¥ URL æ˜¯å¦å¯è¾¾ã€‚
    å¦‚æœä¸å¯è¾¾ï¼Œè¿”å›è¯Šæ–­ä¿¡æ¯ä¾› AI ç»§ç»­å¤„ç†ã€‚
    """
    import httpx

    name = params.get("name", "")
    url = params.get("url", "")
    timeout = params.get("timeout", 10)  # é»˜è®¤ 10 ç§’è¶…æ—¶

    # å¦‚æœæ²¡æœ‰æä¾› URLï¼Œå°è¯•ä»å®¹å™¨è·å–
    if not url and name:
        access_result = await _get_access_info({"name": name})
        urls = access_result.get("urls", [])
        if urls:
            url = urls[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªç«¯å£

    if not url:
        return {
            "text": "âŒ ç¼ºå°‘å‚æ•°: éœ€è¦ `url` æˆ– `name` æ¥ç¡®å®šæ£€æŸ¥ç›®æ ‡ã€‚",
            "ui": {},
            "success": False,
        }

    # ç¡®ä¿ URL æœ‰åè®®å‰ç¼€
    if not url.startswith("http"):
        url = f"http://{url}"

    try:
        async with httpx.AsyncClient(timeout=timeout, follow_redirects=True) as client:
            response = await client.get(url)

            if response.status_code < 400:
                return {
                    "text": f"âœ… **æœåŠ¡éªŒè¯æˆåŠŸ!**\n\n"
                    f"ğŸ“ è®¿é—®åœ°å€: {url}\n"
                    f"ğŸ“Š çŠ¶æ€ç : {response.status_code}\n"
                    f"ğŸ“„ å“åº”é•¿åº¦: {len(response.content)} bytes",
                    "ui": {},
                    "success": True,
                    "url": url,
                    "status_code": response.status_code,
                }
            else:
                return {
                    "text": f"âš ï¸ **æœåŠ¡å“åº”å¼‚å¸¸**\n\n"
                    f"ğŸ“ URL: {url}\n"
                    f"ğŸ“Š çŠ¶æ€ç : {response.status_code}\n\n"
                    f"æœåŠ¡å¯èƒ½éœ€è¦æ›´å¤šæ—¶é—´åˆå§‹åŒ–ï¼Œæˆ–é…ç½®æœ‰è¯¯ã€‚",
                    "ui": {},
                    "success": False,
                    "url": url,
                    "status_code": response.status_code,
                }

    except httpx.ConnectError:
        # è¿æ¥å¤±è´¥ - å¯èƒ½æœåŠ¡æœªå¯åŠ¨
        diag = await _get_container_diagnostics(name) if name else ""
        return {
            "text": f"âŒ **è¿æ¥å¤±è´¥**: æ— æ³•è¿æ¥åˆ° {url}\n\n"
            f"**å¯èƒ½åŸå› **:\n"
            f"â€¢ æœåŠ¡å°šæœªå®Œå…¨å¯åŠ¨ï¼ˆéœ€è¦ç­‰å¾…ï¼‰\n"
            f"â€¢ ç«¯å£æ˜ å°„é…ç½®é”™è¯¯\n"
            f"â€¢ å®¹å™¨å†…æœåŠ¡å´©æºƒ\n\n"
            f"{diag}",
            "ui": {},
            "success": False,
            "error": "connect_error",
            "url": url,
        }

    except httpx.TimeoutException:
        return {
            "text": f"â° **è¿æ¥è¶…æ—¶**: {url} åœ¨ {timeout} ç§’å†…æ— å“åº”\n\n"
            f"**å»ºè®®**:\n"
            f"â€¢ ç­‰å¾…å‡ ç§’åé‡è¯•\n"
            f"â€¢ æ£€æŸ¥å®¹å™¨æ—¥å¿—",
            "ui": {},
            "success": False,
            "error": "timeout",
            "url": url,
        }

    except Exception as e:
        logger.error(f"Verify access error: {e}")
        return {
            "text": f"âŒ **éªŒè¯å¤±è´¥**: {e}",
            "ui": {},
            "success": False,
            "error": str(e),
        }


async def _get_container_diagnostics(name: str) -> str:
    """è·å–å®¹å™¨è¯Šæ–­ä¿¡æ¯"""
    try:
        # æ£€æŸ¥å®¹å™¨æ˜¯å¦åœ¨è¿è¡Œ
        process = await asyncio.create_subprocess_shell(
            f"docker ps -a --filter 'name={name}' --format '{{{{.Names}}}}|{{{{.Status}}}}|{{{{.Ports}}}}'",
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        stdout, _ = await process.communicate()
        output = stdout.decode().strip()

        if not output:
            return f"**è¯Šæ–­**: æœªæ‰¾åˆ°åç§°åŒ…å« `{name}` çš„å®¹å™¨ã€‚\nè¯·æ£€æŸ¥æ˜¯å¦å·²æ‰§è¡Œ `docker compose up`ã€‚"

        lines = []
        for line in output.split("\n"):
            if "|" in line:
                parts = line.split("|")
                container_name = parts[0]
                status = parts[1] if len(parts) > 1 else "Unknown"
                ports = parts[2] if len(parts) > 2 else "None"
                lines.append(f"â€¢ `{container_name}`: {status}")
                if ports:
                    lines.append(f"  ç«¯å£: {ports}")

        # è·å–æœ€è¿‘æ—¥å¿—
        log_process = await asyncio.create_subprocess_shell(
            f"docker logs --tail 5 $(docker ps -q --filter 'name={name}' | head -1) 2>&1",
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        log_stdout, _ = await log_process.communicate()
        recent_logs = log_stdout.decode().strip()

        result = "**å®¹å™¨çŠ¶æ€**:\n" + "\n".join(lines)
        if recent_logs:
            result += f"\n\n**æœ€è¿‘æ—¥å¿—**:\n```\n{recent_logs[:500]}\n```"

        return result

    except Exception as e:
        return f"**è¯Šæ–­å¤±è´¥**: {e}"


async def _delete_project(params: dict) -> dict:
    """åˆ é™¤éƒ¨ç½²é¡¹ç›®"""
    name = params.get("name", "")
    if not name:
        return {"text": "âŒ ç¼ºå°‘å‚æ•°: name", "ui": {}}

    project_path = WORK_BASE / name

    if not project_path.exists():
        return {"text": f"âŒ é¡¹ç›®ä¸å­˜åœ¨: {name}", "ui": {}}

    try:
        shutil.rmtree(project_path)
        return {"text": f"âœ… é¡¹ç›®å·²åˆ é™¤: {name}", "ui": {}}
    except Exception as e:
        logger.error(f"Delete project error: {e}")
        return {"text": f"âŒ åˆ é™¤å¤±è´¥: {e}", "ui": {}}


# =============================================================================
# Handler Registration (for /deploy command)
# =============================================================================
def register_handlers(adapter_manager):
    """æ³¨å†Œ /deploy å‘½ä»¤"""

    async def deploy_command(ctx: UnifiedContext):
        """
        Handle /deploy <æè¿°æˆ–URL>
        è¿™æ˜¯å…¥å£å‘½ä»¤ï¼Œå®é™…éƒ¨ç½²é€»è¾‘ç”± Skill Agent é€šè¿‡ SKILL.md SOP ç¼–æ’
        """
        if not await is_user_allowed(ctx.message.user.id):
            return

        args = ctx.platform_ctx.args if ctx.platform_ctx else []
        if not args:
            await ctx.reply(
                "âš ï¸ è¯·æä¾›éƒ¨ç½²ç›®æ ‡ã€‚\n\n"
                "ç”¨æ³•:\n"
                "â€¢ `/deploy https://github.com/user/repo` - éƒ¨ç½² GitHub é¡¹ç›®\n"
                "â€¢ `/deploy Uptime Kuma` - æ™ºèƒ½æœç´¢å¹¶éƒ¨ç½²"
            )
            return

        # å°†è¯·æ±‚è½¬å‘ç»™ Agent å¤„ç†
        from core.agent_orchestrator import agent_orchestrator

        user_input = " ".join(args)
        full_request = f"éƒ¨ç½²: {user_input}"

        await ctx.reply(f"ğŸš€ æ”¶åˆ°éƒ¨ç½²è¯·æ±‚: {user_input}\n\næ­£åœ¨åˆ†æ...")

        # è°ƒç”¨ Agent å¤„ç†
        message_history = [{"role": "user", "parts": [{"text": full_request}]}]
        async for response in agent_orchestrator.handle_message(
            ctx=ctx,
            message_history=message_history,
        ):
            if response and response.strip():
                await ctx.reply(response)

    adapter_manager.on_command("deploy", deploy_command, description="æ™ºèƒ½éƒ¨ç½²æœåŠ¡")
