import asyncio
import logging
from urllib.parse import quote
import httpx
from core.platform.models import UnifiedContext
from utils import smart_reply_text
from services.web_summary_service import fetch_webpage_content
from core.config import gemini_client, GEMINI_MODEL

logger = logging.getLogger(__name__)

async def execute(ctx: UnifiedContext, params: dict) -> str:
    topic = params.get("topic", "").strip()
    depth = params.get("depth", 3)
    language = params.get("language", "zh-CN")
    
    if not topic:
        await ctx.reply("âŒ è¯·æä¾›ç ”ç©¶ä¸»é¢˜ (topic)")
        return "Failed: No topic provided."
        
    depth = min(max(1, int(depth)), 5) # é™åˆ¶ 1-5
    
    await ctx.reply(f"ğŸ§ æ­£åœ¨å¯¹ ã€Œ{topic}ã€ è¿›è¡Œæ·±åº¦ç ”ç©¶ (æ·±åº¦: {depth})...\næ­¤è¿‡ç¨‹åŒ…å«ï¼šæœç´¢ -> çˆ¬å–ç½‘é¡µ -> æ·±åº¦é˜…è¯» -> ç»¼åˆæŠ¥å‘Šï¼Œå¯èƒ½éœ€è¦ 30-60 ç§’ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚")
    
    # 1. Search Phase
    search_results = []
    try:
        encoded_query = quote(topic)
        # Always use general + news categories for research
        search_url = f"http://192.168.1.100:28080/search?q={encoded_query}&format=json&categories=general,news,it,science&time_range=year&language={language}"
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(search_url)
            if response.status_code == 200:
                data = response.json()
                search_results = data.get("results", [])[:depth]
            else:
                await ctx.reply(f"âš ï¸ æœç´¢é˜¶æ®µå¤±è´¥ (Status: {response.status_code})ï¼Œå°è¯•ç»§ç»­...")
    except Exception as e:
        logger.error(f"Search failed: {e}")
        await ctx.reply(f"âš ï¸ æœç´¢é˜¶æ®µå‡ºé”™: {e}")
        
    if not search_results:
        await ctx.reply("âŒ æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœï¼Œç ”ç©¶ç»ˆæ­¢ã€‚")
        return f"Failed: No search results found for topic '{topic}'."

    # 2. Crawl Phase
    await ctx.reply(f"ğŸ•·ï¸ æ­£åœ¨çˆ¬å–å¹¶é˜…è¯» {len(search_results)} ä¸ªç½‘é¡µ...")
    
    crawled_data = []
    
    async def process_url(item):
        url = item.get("url")
        title = item.get("title", "No Title")
        try:
            content = await fetch_webpage_content(url)
            if content:
                return {
                    "title": title,
                    "url": url,
                    "content": content
                }
        except Exception as e:
            logger.error(f"Crawl failed for {url}: {e}")
        return None

    crawled_results = await asyncio.gather(*(process_url(item) for item in search_results))
    valid_data = [item for item in crawled_results if item]
    
    if not valid_data:
        await ctx.reply("âŒ æ— æ³•è¯»å–ä»»ä½•ç½‘é¡µå†…å®¹ï¼ˆå¯èƒ½æ˜¯å› ä¸ºåçˆ¬è™«æˆ–ç½‘ç»œé—®é¢˜ï¼‰ï¼Œç ”ç©¶ç»ˆæ­¢ã€‚")
        return f"Failed: Unable to crawl any content for topic '{topic}'."

    # 3. Synthesis Phase
    await ctx.reply(f"ğŸ§  å·²è·å– {len(valid_data)} ä»½èµ„æ–™ï¼Œæ­£åœ¨ç»¼åˆåˆ†æå¹¶æ’°å†™æŠ¥å‘Š...")
    
    # Construct Context
    context_text = f"Research Topic: {topic}\n\nSources Data:\n"
    for i, data in enumerate(valid_data, 1):
        context_text += f"\n--- Source {i}: {data['title']} ---\nURL: {data['url']}\nContent:\n{data['content'][:15000]}\n" # Limit per page to avoid insanity
        
    prompt = f"""
    You are a Deep Research Analyst. Your task is to write a comprehensive Deep Dive Report on the topic: "{topic}".
    
    Based ONLY on the provided source materials below, write a detailed, structured, and professional report.
    
    Report Structure:
    1. **Executive Summary**: High-level overview of key findings.
    2. **Detailed Analysis**: Break down the topic into key aspects (e.g., Architecture, Performance, Pros/Cons, History).
    3. **Key Insights**: What are the most important takeaways?
    4. **Source Discrepancies** (if any): Did sources disagree?
    5. **Reference List**: List the titles and URLs of sources used.
    
    Format output as HTML (for a standalone report file). Use modern, clean CSS.
    Title the HTML page "Deep Research: {topic}".
    Ensure the HTML is self-contained.
    
    Source Material:
    {context_text}
    """
    
    try:
        response = gemini_client.models.generate_content(
            model=GEMINI_MODEL,
            contents=prompt,
        )
        
        report_html = response.text
        
        # Strip markdown code blocks if AI added them
        import re
        report_html = re.sub(r"^```html\s*", "", report_html)
        report_html = re.sub(r"^```\s*", "", report_html)
        report_html = re.sub(r"\s*```$", "", report_html)
        
        # Output
        import io
        file_obj = io.BytesIO(report_html.encode('utf-8'))
        file_obj.name = "deep_research_report.html"
        
        await ctx.reply_document(
            document=file_obj,
            caption=f"ğŸ“š æ·±åº¦ç ”ç©¶æŠ¥å‘Šï¼š{topic}\n\nåŸºäº {len(valid_data)} ä¸ªæ¥æºçš„æ·±åº¦ç»¼åˆåˆ†æã€‚"
        )
        
        return f"Success: Deep research report generated for '{topic}' based on {len(valid_data)} sources."
        
    except Exception as e:
        logger.error(f"Synthesis failed: {e}")
        await ctx.reply(f"âŒ æŠ¥å‘Šç”Ÿæˆé˜¶æ®µå¤±è´¥: {e}")
        return f"Failed: Synthesis error: {e}"
