import asyncio
import logging
from urllib.parse import quote
import httpx
from core.platform.models import UnifiedContext
from services.web_summary_service import fetch_webpage_content
from core.config import gemini_client, GEMINI_MODEL

logger = logging.getLogger(__name__)


async def execute(ctx: UnifiedContext, params: dict):
    topic = params.get("topic", "").strip()
    depth = params.get("depth", 3)
    language = params.get("language", "zh-CN")

    if not topic:
        yield {"text": "âŒ è¯·æä¾›ç ”ç©¶ä¸»é¢˜ (topic)", "ui": {}}
        return

    depth = min(max(1, int(depth)), 5)  # é™åˆ¶ 1-5

    yield f"ğŸ§ æ­£åœ¨å¯¹ ã€Œ{topic}ã€ è¿›è¡Œæ·±åº¦ç ”ç©¶ (æ·±åº¦: {depth})...\næ­¤è¿‡ç¨‹åŒ…å«ï¼šæœç´¢ -> çˆ¬å–ç½‘é¡µ -> æ·±åº¦é˜…è¯» -> ç»¼åˆæŠ¥å‘Šï¼Œå¯èƒ½éœ€è¦ 30-60 ç§’ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚"

    # 1. Search Phase
    search_results = []
    try:
        import os

        base_url = os.getenv("SEARXNG_URL")
        if not base_url:
            yield {
                "text": "âŒ æœç´¢æœåŠ¡æœªé…ç½® (SEARXNG_URL missing)ï¼Œæ— æ³•è¿›è¡Œæ·±åº¦ç ”ç©¶ã€‚",
                "ui": {},
            }
            return

        if not base_url.endswith("/search"):
            if not base_url.endswith("/"):
                base_url += "/"
            base_url += "search"

        encoded_query = quote(topic)
        # Always use general + news categories for research
        search_url = f"{base_url}?q={encoded_query}&format=json&categories=general,news,it,science&time_range=year&language={language}"

        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(search_url)
            if response.status_code == 200:
                data = response.json()
                search_results = data.get("results", [])[:depth]
            else:
                yield f"âš ï¸ æœç´¢é˜¶æ®µå¤±è´¥ (Status: {response.status_code})ï¼Œå°è¯•ç»§ç»­..."
    except Exception as e:
        logger.error(f"Search failed: {e}")
        yield f"âš ï¸ æœç´¢é˜¶æ®µå‡ºé”™: {e}"

    if not search_results:
        yield {"text": "âŒ æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœï¼Œç ”ç©¶ç»ˆæ­¢ã€‚", "ui": {}}
        return

    # 2. Crawl Phase
    yield f"ğŸ•·ï¸ æ­£åœ¨çˆ¬å– {len(search_results)} ä¸ªç½‘é¡µ..."

    async def process_url(item):
        url = item.get("url")
        title = item.get("title", "No Title")
        try:
            content = await fetch_webpage_content(url)
            if content:
                return {"title": title, "url": url, "content": content}
        except Exception as e:
            logger.error(f"Crawl failed for {url}: {e}")
        return None

    crawled_results = await asyncio.gather(
        *(process_url(item) for item in search_results)
    )
    valid_data = [item for item in crawled_results if item]

    if not valid_data:
        yield {
            "text": "âŒ æ— æ³•è¯»å–ä»»ä½•ç½‘é¡µå†…å®¹ï¼ˆå¯èƒ½æ˜¯å› ä¸ºåçˆ¬è™«æˆ–ç½‘ç»œé—®é¢˜ï¼‰ï¼Œç ”ç©¶ç»ˆæ­¢ã€‚",
            "ui": {},
        }
        return

    # 3. Synthesis Phase
    yield f"ğŸ§  å·²è·å– {len(valid_data)} ä»½èµ„æ–™ï¼Œæ­£åœ¨ç»¼åˆåˆ†æå¹¶æ’°å†™æŠ¥å‘Š..."

    # Construct Context
    context_text = f"Research Topic: {topic}\n\nSources Data:\n"
    for i, data in enumerate(valid_data, 1):
        context_text += f"\n--- Source {i}: {data['title']} ---\nURL: {data['url']}\nContent:\n{data['content'][:15000]}\n"  # Limit per page to avoid insanity

    prompt = f"""
    You are a Deep Research Analyst. Your task is to write a comprehensive Deep Dive Report on the topic: "{topic}".
    
    Based ONLY on the provided source materials below, write a detailed, structured, and professional report.
    
    Report Structure:
    1. **Executive Summary**: High-level overview of key findings.
    2. **Detailed Analysis**: Break down the topic into key aspects (e.g., Architecture, Performance, Pros/Cons, History).
    3. **Key Insights**: What are the most important takeaways?
    4. **Source Discrepancies** (if any): Did sources disagree?
    5. **Reference List**: List the titles and URLs of sources used.
    
    Format output as HTML (for a standalone report file). Use modern, clean CSS.
    Title the HTML page "Deep Research: {topic}".
    Ensure the HTML is self-contained.
    
    Source Material:
    {context_text}
    """

    try:
        response = await gemini_client.aio.models.generate_content(
            model=GEMINI_MODEL,
            contents=prompt,
        )

        report_html = response.text

        # Strip markdown code blocks if AI added them
        import re

        report_html = re.sub(r"^```html\s*", "", report_html)
        report_html = re.sub(r"^```\s*", "", report_html)
        report_html = re.sub(r"\s*```$", "", report_html)

        # Output
        import io

        file_obj = io.BytesIO(report_html.encode("utf-8"))
        file_obj.name = "deep_research_report.html"

        yield {
            "text": f"ğŸ”‡ğŸ”‡ğŸ”‡ã€æ·±åº¦ç ”ç©¶æŠ¥å‘Šã€‘\n\nSuccess: Deep research report generated for '{topic}' based on {len(valid_data)} sources.",
            "files": {"deep_research_report.html": report_html.encode("utf-8")},
            "ui": {},
        }
        return

    except Exception as e:
        logger.error(f"Synthesis failed: {e}")
        yield {"text": f"âŒ æŠ¥å‘Šç”Ÿæˆé˜¶æ®µå¤±è´¥: {e}", "ui": {}}
        return
